import sys
import os

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Path
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "../../")))

import torch
import hydra
import logging
import numpy as np
import pandas as pd
from tqdm import tqdm
from omegaconf import DictConfig, OmegaConf

# å¼•å…¥é¡¹ç›®æ¨¡å—
from src.models.flow_bert_multiview.models.flow_bert_multiview import FlowBertMultiview
from src.models.flow_bert_multiview.data.flow_bert_multiview_dataset import MultiviewFlowDataModule
from src.concept_drift_detect.detectors import BNDMDetector, ADWINDetector
from src.concept_drift_detect.adapter import IncrementalAdapter

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("DriftExp")


class DriftExperimentRunner:
    def __init__(self, model, cfg, detector_type="bndm"):
        self.model = model
        self.cfg = cfg
        self.detector_type = detector_type

        # æ£€æµ‹å™¨å‚æ•°é…ç½®
        det_config = {
            'seed': 2026,
            'threshold': 0.05,
            'max_level': 6,
            'window_size': 2000,
            'delta': 0.002
        }

        if detector_type == "bndm":
            self.detector = BNDMDetector(det_config)
        elif detector_type == "adwin":
            self.detector = ADWINDetector(det_config)

        # é€‚åº”å™¨å‚æ•°é…ç½®
        adapt_config = {'lr': 1e-4, 'epochs': 5, 'buffer_size': 5000}
        self.adapter = IncrementalAdapter(model, adapt_config)

        self.metrics = {
            "processed": 0,
            "drifts": 0,
            "accuracy_history": [],
            "adaptation_points": []
        }

        self.buffer_features = []
        self.buffer_labels = []

    def run_stream(self, dataloader):
        self.model.eval()
        self.model.cuda()

        progress = tqdm(dataloader, desc=f"Running {self.detector_type.upper()}")

        for batch in progress:
            batch = {k: v.to(self.model.device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}

            # æ ‡ç­¾å…¼å®¹æ€§å¤„ç†
            if 'attack_family_label' in batch:
                labels = batch['attack_family_label']
            elif 'label' in batch:
                labels = batch['label']
            else:
                labels = batch['is_malicious_label']

                # ç‰¹å¾æå–
            with torch.no_grad():
                outputs = self.model(batch)
                # ä½¿ç”¨å¤šè§†å›¾èåˆåçš„ç‰¹å¾è¿›è¡Œæ¼‚ç§»æ£€æµ‹
                features = outputs['multiview_embeddings']

                # è·å–é¢„æµ‹ç»“æœ
                if 'attack_family_cls_logits' in outputs and outputs['attack_family_cls_logits'] is not None:
                    preds = torch.argmax(outputs['attack_family_cls_logits'], dim=1)
                else:
                    preds = (torch.sigmoid(outputs['is_malicious_cls_logits']) > 0.5).long().squeeze()

            # é€æ ·æœ¬å¤„ç†
            batch_size = features.shape[0]
            for i in range(batch_size):
                self.metrics["processed"] += 1
                feat = features[i]

                # å¤„ç†å¤šæ ‡ç­¾/ç»´åº¦ä¸åŒ¹é…æƒ…å†µ
                if labels.dim() > 1:
                    lbl = torch.argmax(labels[i])
                else:
                    lbl = labels[i]

                if preds.dim() > 0:
                    pred = preds[i]
                else:
                    pred = preds

                is_correct = (pred == lbl).item()
                self.metrics["accuracy_history"].append(is_correct)

                # æ¼‚ç§»æ£€æµ‹
                feat_input = feat.unsqueeze(0)
                drift_detected = self.detector.update(self.detector.preprocess(feat_input))

                self.buffer_features.append(feat)
                self.buffer_labels.append(lbl)

                # è§¦å‘é€‚åº”
                if drift_detected:
                    self.metrics["drifts"] += 1
                    self.metrics["adaptation_points"].append(self.metrics["processed"])

                    recent_acc = 0.0
                    if len(self.metrics['accuracy_history']) > 200:
                        recent_acc = np.mean(self.metrics['accuracy_history'][-200:])

                    logger.info(f"ğŸš¨ Drift at idx {self.metrics['processed']} (Recent Acc: {recent_acc:.4f})")

                    # åªæœ‰å½“ buffer è¶³å¤Ÿå¤§æ—¶æ‰è¿›è¡Œé€‚åº”
                    if len(self.buffer_features) > 100:
                        adapt_feats = torch.stack(self.buffer_features[-1000:])
                        adapt_lbls = torch.stack(self.buffer_labels[-1000:])

                        self.adapter.adapt(adapt_feats, adapt_lbls)

                    self.detector.reset()

                    if len(self.buffer_features) > 5000:
                        self.buffer_features = self.buffer_features[-2000:]
                        self.buffer_labels = self.buffer_labels[-2000:]

    def get_results(self):
        acc = np.mean(self.metrics["accuracy_history"]) if self.metrics["accuracy_history"] else 0.0
        return {
            "Method": self.detector_type.upper(),
            "Total Samples": self.metrics["processed"],
            "Drifts Detected": self.metrics["drifts"],
            "Avg Accuracy": acc
        }


def resolve_path(raw_path, dataset_name):
    """è¾…åŠ©å‡½æ•°ï¼šå¤„ç†è·¯å¾„æ’å€¼å’Œç›¸å¯¹è·¯å¾„"""
    if raw_path is None: return ""
    if "${dataset.name}" in raw_path:
        raw_path = raw_path.replace("${dataset.name}", dataset_name)
    if raw_path.startswith("."):
        project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), "../../"))
        raw_path = os.path.normpath(os.path.join(project_root, raw_path))
    return raw_path


# ==========================================================
# ğŸŒŸ [å…³é”®ä¿®å¤] å®Œæ•´çš„ Dataset åŒ…è£…å™¨
# å¿…é¡»åŒ…å« categorical_val2idx_mappings å’Œ categorical_columns_effective
# ==========================================================
class MappingWrapper:
    def __init__(self, mappings, effective_columns):
        # 1. ç±»åˆ«æ˜ å°„å­—å…¸
        self.categorical_val2idx_mappings = mappings
        # 2. æœ‰æ•ˆç±»åˆ«åˆ—åˆ—è¡¨ (ä¿®å¤ AttributeError çš„å…³é”®)
        self.categorical_columns_effective = effective_columns

        # è°ƒè¯•æ—¥å¿—ï¼šç¡®è®¤å±æ€§å·²è®¾ç½®
        # print(f"DEBUG: MappingWrapper initialized with {len(effective_columns)} columns")


@hydra.main(config_path="../models/flow_bert_multiview/config", config_name="flow_bert_multiview_config",
            version_base="1.2")
def main(cfg: DictConfig):
    # =================================================================
    # ğŸ”¥ é…ç½®æ‰‹åŠ¨ä¿®è¡¥
    # =================================================================
    OmegaConf.set_struct(cfg, False)

    current_dir = os.path.dirname(os.path.abspath(__file__))
    dataset_yaml_path = os.path.join(
        current_dir,
        "../models/flow_bert_multiview/config/datasets/cic_ids_2017.yaml"
    )

    if os.path.exists(dataset_yaml_path):
        logger.info(f"âœ… Loading dataset config: {dataset_yaml_path}")
        dataset_cfg = OmegaConf.load(dataset_yaml_path)

        cfg.data.dataset = dataset_cfg.name

        if 'flow_data_path' in dataset_cfg:
            fixed_path = resolve_path(dataset_cfg.flow_data_path, dataset_cfg.name)
            cfg.data.flow_data_path = fixed_path
            logger.info(f"ğŸ”§ Patched flow_data_path: {fixed_path}")

        if 'session_split_path' in dataset_cfg:
            fixed_path = resolve_path(dataset_cfg.session_split_path, dataset_cfg.name)
            cfg.data.session_split.session_split_path = fixed_path
            logger.info(f"ğŸ”§ Patched session_split_path: {fixed_path}")

        if 'class_weights' in dataset_cfg:
            cfg.loss.class_weights = dataset_cfg.class_weights

    else:
        raise FileNotFoundError(f"Critical: Dataset config not found at {dataset_yaml_path}")

    # å¼ºåˆ¶è®¾ç½® Flow æ¨¡å¼
    cfg.data.split_mode = "flow"
    if hasattr(cfg.data, 'sampling'):
        cfg.data.sampling.random = False

    logger.info("ğŸ”§ Config patching complete. Initializing DataModule...")

    # å‡†å¤‡æ•°æ®
    try:
        dm = MultiviewFlowDataModule(cfg)
        # ä½¿ç”¨ "fit" é˜¶æ®µåˆå§‹åŒ–ï¼Œç¡®ä¿ train_dataset è¢«åˆ›å»º
        dm.setup(stage="fit")
        test_loader = dm.test_dataloader()

        # =================================================================
        # ğŸŒŸ [å…³é”®ä¿®å¤] æ•°æ®å…ƒæå–é€»è¾‘
        # =================================================================
        mappings = None
        effective_columns = None

        # 1. å°è¯•ä» train_dataset è·å–
        if hasattr(dm, 'train_dataset'):
            mappings = getattr(dm.train_dataset, 'categorical_val2idx_mappings', None)
            effective_columns = getattr(dm.train_dataset, 'categorical_columns_effective', None)

        # 2. å¦‚æœå¤±è´¥ï¼Œå°è¯• lazy init
        if mappings is None or effective_columns is None:
            logger.info("Triggering lazy initialization for metadata...")
            _ = dm.train_dataloader()
            if hasattr(dm, 'train_dataset'):
                mappings = getattr(dm.train_dataset, 'categorical_val2idx_mappings', None)
                effective_columns = getattr(dm.train_dataset, 'categorical_columns_effective', None)

        # 3. æœ€ç»ˆæ£€æŸ¥
        if mappings is None or effective_columns is None:
            raise AttributeError(
                "âŒ FAILED to extract 'categorical_val2idx_mappings' or 'categorical_columns_effective' from DataModule/Dataset. Please check Dataset implementation.")

        # 4. åˆ›å»ºå¢å¼ºç‰ˆ Wrapper
        dataset_wrapper = MappingWrapper(mappings, effective_columns)
        logger.info(f"âœ… Metadata extracted. Effective Columns: {len(effective_columns)}, Mappings: {len(mappings)}")

    except Exception as e:
        logger.error(f"âŒ DataModule init failed: {e}")
        raise e

    # åŠ è½½æ¨¡å‹
    ckpt_path = "checkpoints/best_model.ckpt"
    if not os.path.exists(ckpt_path):
        logger.warning(f"âš ï¸ Checkpoint {ckpt_path} not found. Using random weights!")
        # âœ… ä¿®å¤ç‚¹: ä¼ å…¥å¢å¼ºç‰ˆ wrapper
        model = FlowBertMultiview(cfg, dataset=dataset_wrapper)
    else:
        # âœ… ä¿®å¤ç‚¹: ä¼ å…¥å¢å¼ºç‰ˆ wrapper
        model = FlowBertMultiview.load_from_checkpoint(ckpt_path, cfg=cfg, dataset=dataset_wrapper)

    results = []

    # å®éªŒ 1: BNDM
    logger.info("ğŸ§ª Starting Experiment 1: BNDM Detector")
    runner_bndm = DriftExperimentRunner(model, cfg, "bndm")
    runner_bndm.run_stream(test_loader)
    results.append(runner_bndm.get_results())

    # å®éªŒ 2: ADWIN
    logger.info("ğŸ§ª Starting Experiment 2: ADWIN Detector")
    # é‡ç½®æ¨¡å‹
    if os.path.exists(ckpt_path):
        model = FlowBertMultiview.load_from_checkpoint(ckpt_path, cfg=cfg, dataset=dataset_wrapper)
    else:
        model = FlowBertMultiview(cfg, dataset=dataset_wrapper)

    runner_adwin = DriftExperimentRunner(model, cfg, "adwin")
    runner_adwin.run_stream(test_loader)
    results.append(runner_adwin.get_results())

    df = pd.DataFrame(results)
    print("\n" + "=" * 60)
    print("ğŸ“Š æ¦‚å¿µæ¼‚ç§»ä¸é€‚åº” - æ¶ˆèå®éªŒæŠ¥å‘Š")
    print("=" * 60)
    print(df.to_markdown(index=False))
    print("=" * 60)


if __name__ == "__main__":
    main()