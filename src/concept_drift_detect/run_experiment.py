import sys
import os

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Path
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "../../")))

import torch
import hydra
import logging
import numpy as np
import pandas as pd
from tqdm import tqdm
from omegaconf import DictConfig, OmegaConf

# å¼•å…¥é¡¹ç›®æ¨¡å—
from src.models.flow_bert_multiview.models.flow_bert_multiview import FlowBertMultiview
from src.models.flow_bert_multiview.data.flow_bert_multiview_dataset import MultiviewFlowDataModule
from src.concept_drift_detect.detectors import BNDMDetector, ADWINDetector
from src.concept_drift_detect.adapter import IncrementalAdapter

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("DriftExp")


class DriftExperimentRunner:
    def __init__(self, model, cfg, detector_type="bndm"):
        self.model = model
        self.cfg = cfg
        self.detector_type = detector_type

        # æ£€æµ‹å™¨å‚æ•°é…ç½®
        det_config = {
            'seed': 2026,
            'threshold': 0.05,
            'max_level': 6,
            'window_size': 2000,
            'delta': 0.002
        }

        if detector_type == "bndm":
            self.detector = BNDMDetector(det_config)
        elif detector_type == "adwin":
            self.detector = ADWINDetector(det_config)

        # é€‚åº”å™¨å‚æ•°é…ç½®
        adapt_config = {'lr': 1e-4, 'epochs': 5, 'buffer_size': 5000}
        self.adapter = IncrementalAdapter(model, adapt_config)

        self.metrics = {
            "processed": 0,
            "drifts": 0,
            "accuracy_history": [],
            "adaptation_points": []
        }

        self.buffer_features = []
        self.buffer_labels = []

    def run_stream(self, dataloader):
        self.model.eval()
        self.model.cuda()

        progress = tqdm(dataloader, desc=f"Running {self.detector_type.upper()}")

        for batch in progress:
            batch = {k: v.to(self.model.device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}

            # æ ‡ç­¾å…¼å®¹æ€§å¤„ç†
            if 'attack_family' in batch:
                labels = batch['attack_family']
            elif 'label' in batch:
                labels = batch['label']
            else:
                labels = batch['is_malicious']

                # ç‰¹å¾æå–
            with torch.no_grad():
                outputs = self.model(batch)
                features = outputs['logits']
                preds = torch.argmax(features, dim=1)

            # é€æ ·æœ¬å¤„ç†
            batch_size = features.shape[0]
            for i in range(batch_size):
                self.metrics["processed"] += 1
                feat = features[i]
                lbl = labels[i]
                pred = preds[i]

                is_correct = (pred == lbl).item()
                self.metrics["accuracy_history"].append(is_correct)

                # æ¼‚ç§»æ£€æµ‹
                feat_input = feat.unsqueeze(0)
                drift_detected = self.detector.update(self.detector.preprocess(feat_input))

                self.buffer_features.append(feat)
                self.buffer_labels.append(lbl)

                # è§¦å‘é€‚åº”
                if drift_detected:
                    self.metrics["drifts"] += 1
                    self.metrics["adaptation_points"].append(self.metrics["processed"])

                    # ä»…åœ¨æœ‰è¶³å¤Ÿå†å²æ•°æ®æ—¶æ‰“å°å‡†ç¡®ç‡
                    recent_acc = 0.0
                    if len(self.metrics['accuracy_history']) > 200:
                        recent_acc = np.mean(self.metrics['accuracy_history'][-200:])

                    logger.info(f"ğŸš¨ Drift at idx {self.metrics['processed']} (Recent Acc: {recent_acc:.4f})")

                    # é€‚åº”
                    adapt_feats = torch.stack(self.buffer_features[-1000:])
                    adapt_lbls = torch.stack(self.buffer_labels[-1000:])

                    self.adapter.adapt(adapt_feats, adapt_lbls)

                    self.detector.reset()

                    if len(self.buffer_features) > 5000:
                        self.buffer_features = self.buffer_features[-2000:]
                        self.buffer_labels = self.buffer_labels[-2000:]

    def get_results(self):
        acc = np.mean(self.metrics["accuracy_history"]) if self.metrics["accuracy_history"] else 0.0
        return {
            "Method": self.detector_type.upper(),
            "Total Samples": self.metrics["processed"],
            "Drifts Detected": self.metrics["drifts"],
            "Avg Accuracy": acc
        }


@hydra.main(config_path="../models/flow_bert_multiview/config", config_name="flow_bert_multiview_config",
            version_base="1.2")
def main(cfg: DictConfig):
    # =================================================================
    # ğŸ”¥ æ ¸å¿ƒä¿®å¤ï¼šæ‰‹åŠ¨åŠ è½½ Dataset é…ç½®
    # =================================================================
    OmegaConf.set_struct(cfg, False)  # è§£é”é…ç½®

    # æ£€æŸ¥ dataset æ˜¯å¦ç¼ºå¤±
    if not hasattr(cfg, 'dataset') or cfg.dataset is None or 'flow_data_path' not in cfg.dataset:
        logger.warning("âš ï¸ Config 'dataset' incomplete! Attempting manual load of CIC-IDS-2017.")

        # 1. å®šä½ yaml æ–‡ä»¶è·¯å¾„
        current_dir = os.path.dirname(os.path.abspath(__file__))
        # è·¯å¾„: src/concept_drift_detect/../models/flow_bert_multiview/config/datasets/cic_ids_2017.yaml
        dataset_yaml_path = os.path.join(
            current_dir,
            "../models/flow_bert_multiview/config/datasets/cic_ids_2017.yaml"
        )

        if os.path.exists(dataset_yaml_path):
            # 2. åŠ è½½å¹¶åˆå¹¶
            dataset_cfg = OmegaConf.load(dataset_yaml_path)
            cfg.dataset = dataset_cfg
            logger.info(f"âœ… Manually loaded dataset config from: {dataset_yaml_path}")

            # 3. ä¿®å¤æ’å€¼è·¯å¾„ (æ‰‹åŠ¨è¦†ç›– data.flow_data_path)
            # å› ä¸ºåŸå§‹çš„ ${dataset.flow_data_path} å¯èƒ½å› ä¸ºä¸Šä¸‹æ–‡ä¸¢å¤±è€Œå¤±æ•ˆ
            # æˆ‘ä»¬ç›´æ¥æŠŠ dataset é‡Œçš„å€¼èµ‹ç»™ data é‡Œçš„å€¼
            if 'flow_data_path' in dataset_cfg:
                # å¤„ç†ç›¸å¯¹è·¯å¾„é—®é¢˜ï¼šå¦‚æœ yaml é‡Œæ˜¯ processed_data/..., å³ä½¿æ­£ç¡®ä¹Ÿå¯èƒ½å› ä¸º cwd é—®é¢˜æ‰¾ä¸åˆ°
                # è¿™é‡Œæˆ‘ä»¬å‡è®¾ config é‡Œçš„è·¯å¾„æ˜¯ç›¸å¯¹äºé¡¹ç›®æ ¹ç›®å½•çš„
                raw_path = dataset_cfg.flow_data_path
                # å¦‚æœåŒ…å«æ’å€¼å˜é‡ ${dataset.name}ï¼Œæ‰‹åŠ¨æ›¿æ¢
                if "${dataset.name}" in raw_path:
                    raw_path = raw_path.replace("${dataset.name}", dataset_cfg.name)

                cfg.data.flow_data_path = raw_path
                logger.info(f"ğŸ”§ Patched cfg.data.flow_data_path = {cfg.data.flow_data_path}")
        else:
            logger.error(f"âŒ Dataset config not found at: {dataset_yaml_path}")
            raise FileNotFoundError("Critical config missing")

    # 4. å¼ºåˆ¶è®¾ç½® Flow æ¨¡å¼ (æ¶ˆèå®éªŒè¦æ±‚)
    cfg.data.split_mode = "flow"
    if hasattr(cfg.data, 'sampling'):
        cfg.data.sampling.random = False

    logger.info(
        f"ğŸ”§ Final Config: Dataset={cfg.dataset.get('name')}, Mode={cfg.data.split_mode}, Shuffle={cfg.data.sampling.random}")
    # =================================================================

    # å‡†å¤‡æ•°æ®
    try:
        dm = MultiviewFlowDataModule(cfg)
        dm.setup(stage="test")
        test_loader = dm.test_dataloader()
    except Exception as e:
        logger.error(f"Failed to initialize DataModule: {e}")
        # æ‰“å°éƒ¨åˆ† Config å¸®åŠ©è°ƒè¯•
        logger.error(f"cfg.data.flow_data_path: {cfg.data.get('flow_data_path', 'MISSING')}")
        raise e

    # åŠ è½½æ¨¡å‹
    ckpt_path = "checkpoints/best_model.ckpt"
    if not os.path.exists(ckpt_path):
        logger.warning(f"âš ï¸ Checkpoint {ckpt_path} not found. Using random weights!")
        model = FlowBertMultiview(cfg)
    else:
        model = FlowBertMultiview.load_from_checkpoint(ckpt_path, cfg=cfg)

    results = []

    # å®éªŒ 1: BNDM (Proposed)
    logger.info("ğŸ§ª Starting Experiment 1: BNDM Detector")
    runner_bndm = DriftExperimentRunner(model, cfg, "bndm")
    runner_bndm.run_stream(test_loader)
    results.append(runner_bndm.get_results())

    # å®éªŒ 2: ADWIN (Baseline)
    logger.info("ğŸ§ª Starting Experiment 2: ADWIN Detector")
    # é‡ç½®æ¨¡å‹
    if os.path.exists(ckpt_path):
        model = FlowBertMultiview.load_from_checkpoint(ckpt_path, cfg=cfg)
    else:
        model = FlowBertMultiview(cfg)

    runner_adwin = DriftExperimentRunner(model, cfg, "adwin")
    runner_adwin.run_stream(test_loader)
    results.append(runner_adwin.get_results())

    df = pd.DataFrame(results)
    print("\n" + "=" * 60)
    print("ğŸ“Š æ¦‚å¿µæ¼‚ç§»ä¸é€‚åº” - æ¶ˆèå®éªŒæŠ¥å‘Š")
    print("=" * 60)
    print(df.to_markdown(index=False))
    print("=" * 60)


if __name__ == "__main__":
    main()