#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
analyze_all_flow_dataset_leakage.py
===================================

æœ¬è„šæœ¬ç”¨äºç³»ç»Ÿæ€§æ£€æµ‹å„ç±»ç‰¹å¾å¯¹ç›®æ ‡æ ‡ç­¾ `is_malicious` çš„æ½œåœ¨æ³„éœ²é£é™©ï¼Œ
ç”¨äºè¯†åˆ«èƒ½å¤Ÿâ€œå•ç‹¬é¢„æµ‹æ¶æ„â€çš„é«˜å±ç‰¹å¾ï¼Œé¿å…æ¨¡å‹åœ¨è®­ç»ƒä¸­å—åˆ°æ•°æ®æ³„éœ²æ±¡æŸ“ã€‚

æœ¬å·¥å…·è‡ªåŠ¨åˆ†ææ‰€æœ‰ç‰¹å¾åˆ—ï¼Œè‡ªåŠ¨åˆ¤æ–­ç‰¹å¾ç±»å‹ï¼ˆæ•°å€¼ / ç±»åˆ« / å­—ç¬¦ä¸² / åµŒå…¥å‘é‡ï¼‰ï¼Œ
å¯¹æ¯ç±»ç‰¹å¾æ‰§è¡Œå¯¹åº”çš„æ³„éœ²è¯„ä¼°æ–¹æ³•ï¼Œè¾“å‡ºæ’åºåçš„å¼ºæ³„éœ²ç‰¹å¾æ¦œå•ï¼Œå¹¶ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨ã€‚

-----------------------------------
1) ç‰¹å¾ç±»å‹è‡ªåŠ¨è¯†åˆ«
-----------------------------------
è„šæœ¬è‡ªåŠ¨åˆ¤æ–­ç‰¹å¾å±äºä»¥ä¸‹ç±»åˆ«ï¼š

- Numericalï¼ˆæ•°å€¼ç‰¹å¾ï¼‰
    * int / float
    * å­—ç¬¦ä¸²å½¢å¼æ•°å­—ï¼ˆå¦‚ "123", "3.14"ï¼‰
- Categoricalï¼ˆç±»åˆ«ç‰¹å¾ï¼‰
    * object / string
    * ç¦»æ•£æ•´æ•°ï¼ˆunique å€¼æ•°é‡ < é˜ˆå€¼ï¼‰
- Embeddingï¼ˆåµŒå…¥å‘é‡ï¼‰
    * list/ndarray/å­—ç¬¦ä¸²å½¢å¼åˆ—è¡¨
    * è‡ªåŠ¨è¿‡æ»¤ç©ºå‘é‡ã€å…¨ 0ã€å¸¸æ•°å‘é‡

-----------------------------------
2) æ•°å€¼å‹ç‰¹å¾æ³„éœ²æ£€æŸ¥ï¼ˆNumeric Leakageï¼‰
-----------------------------------
ä»¥ä¸‹æŒ‡æ ‡ç”¨äºè¡¡é‡æ•°å€¼ç‰¹å¾ä¸ `is_malicious` çš„å¯åŒºåˆ†æ€§ï¼š

- Pearson Correlationï¼ˆçº¿æ€§ç›¸å…³ï¼‰
- Mutual Informationï¼ˆéçº¿æ€§ä¾èµ–ï¼‰
- AUC Scoreï¼ˆç‰¹å¾ç›´æ¥ä½œä¸ºåˆ†ç±»å™¨æ—¶çš„åŒºåˆ†èƒ½åŠ›ï¼‰
- Numeric Leakage Scoreï¼ˆç»¼åˆæ³„éœ²åˆ†ï¼šç”± Pearson/MI/AUC åŠ æƒï¼‰

å¼ºæ³„éœ²åˆ¤æ–­æ ‡å‡†ï¼ˆå‚è€ƒï¼‰ï¼š
- Pearson |corr| > 0.5
- MI é«˜
- AUC > 0.8

-----------------------------------
3) ç±»åˆ«/å­—ç¬¦ä¸²ç‰¹å¾æ³„éœ²æ£€æŸ¥ï¼ˆCategorical Leakageï¼‰
-----------------------------------
å¯¹ç±»åˆ«å‹ç‰¹å¾è¿›è¡Œä»¥ä¸‹åˆ†æï¼š

- Leakage Ratioï¼ˆtest ä¸­çš„ç±»åˆ«åœ¨ train ä¸­å‡ºç°çš„å æ¯”ï¼‰
- Conditional Entropyï¼ˆæ¡ä»¶ç†µï¼Œè¶Šå°è¶Šå®¹æ˜“æ³„éœ²ï¼‰
- NA Ratioï¼ˆç¼ºå¤±å€¼/æ— æ•ˆå€¼å æ¯”ï¼‰
- Categorical Leakage Scoreï¼ˆç»¼åˆæ³„éœ²åˆ†ï¼‰

å¼ºæ³„éœ²åˆ¤æ–­æ ‡å‡†ï¼ˆå‚è€ƒï¼‰ï¼š
- Test â†” Train çš„ç±»åˆ«å…±äº« > 80%
- å•ä¸ªç±»åˆ«çš„æ ‡ç­¾åˆ†å¸ƒæåº¦åå‘ï¼ˆä¾‹å¦‚ 90%+ éƒ½æ˜¯æ¶æ„/æ­£å¸¸ï¼‰
- Conditional Entropy < 0.3

-----------------------------------
4) è¾“å‡ºå†…å®¹
-----------------------------------
è„šæœ¬ä¼šç”Ÿæˆå¹¶ä¿å­˜ä»¥ä¸‹å†…å®¹ï¼š

- CSV æŠ¥å‘Šï¼ˆæ•°å€¼ç‰¹å¾ / ç±»åˆ«ç‰¹å¾åˆ†åˆ«ä¿å­˜ï¼‰
- æ¡å½¢å›¾ï¼ˆBar Plotï¼‰
    * æ•°å€¼ç‰¹å¾æ³„éœ²æ¡å½¢å›¾
    * ç±»åˆ«ç‰¹å¾æ³„éœ²æ¡å½¢å›¾
- é›·è¾¾å›¾ï¼ˆRadar Plotï¼‰
    * æ•°å€¼ç‰¹å¾æ³„éœ²é›·è¾¾å›¾
    * ç±»åˆ«ç‰¹å¾æ³„éœ²é›·è¾¾å›¾

æ‰€æœ‰ç»“æœä¼šä¿å­˜åˆ°ï¼š
    ConfigManager.read_plot_data_path_config() / "leakage_reports"

-----------------------------------
5) Split æ”¯æŒ
-----------------------------------
æ”¯æŒä¸¤ç§æ•°æ®åˆ’åˆ†ï¼š
- flow-split  ï¼ˆå®¹æ˜“æ³„éœ²ï¼Œç”¨äºè¯Šæ–­é—®é¢˜ï¼‰
- session-splitï¼ˆå®é™…æ³›åŒ–æµ‹è¯•ã€ä¸¥è°¨è¯„ä¼°ï¼‰

-----------------------------------
6) ä½¿ç”¨åœºæ™¯
-----------------------------------
- æ£€æŸ¥æ¶æ„æµé‡æ£€æµ‹æ¨¡å‹æ˜¯å¦å› å¼ºæ³„éœ²ç‰¹å¾è€Œè™šé«˜
- åˆ†ææ•°æ®é›†æ˜¯å¦å­˜åœ¨éšå«æ³„éœ²ï¼ˆå¦‚äº”å…ƒç»„æ³„éœ²ï¼‰
- è¯„ä¼°æ–°å¢ç‰¹å¾æ˜¯å¦å¯èƒ½å¯¼è‡´æ•°æ®æ³„éœ²é£é™©
- åœ¨æ¨¡å‹è®­ç»ƒå‰æ‰§è¡Œæ•°æ®å¥åº·æ£€æŸ¥ï¼ˆData Health Checkï¼‰
"""

import pandas as pd
import numpy as np
import re
from sklearn.model_selection import train_test_split
import warnings
warnings.filterwarnings("ignore")
import matplotlib.pyplot as plt
import argparse
import sys
import os
from sklearn.feature_selection import mutual_info_classif
from sklearn.metrics import roc_auc_score
import ipaddress
import json
import ast

import logging
logging.getLogger('matplotlib.font_manager').setLevel(logging.ERROR)
logging.getLogger('matplotlib').setLevel(logging.WARNING)
logging.getLogger("PIL").setLevel(logging.INFO)
logging.getLogger("PIL.PngImagePlugin").setLevel(logging.WARNING)

import matplotlib
matplotlib.rcParams.update({
    "figure.max_open_warning": 0,  # disable "More than 20 figures" warning
})

# å¯¼å…¥é…ç½®ç®¡ç†å™¨å’Œç›¸å…³æ¨¡å—
try:
    # æ·»åŠ ../../utilsç›®å½•åˆ°Pythonæœç´¢è·¯å¾„
    utils_path = os.path.join(os.path.dirname(__file__), '..', 'utils')
    sys.path.insert(0, utils_path)    
    import config_manager as ConfigManager
    from logging_config import setup_preset_logging
    # ä½¿ç”¨ç»Ÿä¸€çš„æ—¥å¿—é…ç½®
    logger = setup_preset_logging(log_level=logging.DEBUG)
except ImportError as e:
    print(f"å¯¼å…¥é”™è¯¯: {e}")
    print("è¯·ç¡®ä¿é¡¹ç›®ç»“æ„æ­£ç¡®ï¼Œæ‰€æœ‰ä¾èµ–æ¨¡å—å¯ç”¨")
    sys.exit(1)

# ========= é…ç½® =========

FIVE_TUPLE_COLS = [
    "flowmeter.id.orig_h",
    "flowmeter.id.orig_p",
    "flowmeter.id.resp_h",
    "flowmeter.id.resp_p",
    "flowmeter.proto",
]

TRAIN_RATIO = 0.7
SEED = 42


# ========= è¾“å‡ºè¾…åŠ© =========

def color(text, c):
    COLORS = {
        "red": "\033[91m", "green": "\033[92m",
        "yellow": "\033[93m", "blue": "\033[94m",
        "end": "\033[0m",
    }
    return f"{COLORS.get(c,'')}{text}{COLORS['end']}"

def info(msg): print(color(f"[INFO] {msg}", "blue"))
def warn(msg): print(color(f"[WARN] {msg}", "yellow"))
def error(msg): print(color(f"[ERROR] {msg}", "red"))


# ========= Step 1: äº”å…ƒç»„æ„å»º =========

def build_five_tuple(df):
    df["five_tuple"] = df.apply(
        lambda r: tuple(r[c] for c in FIVE_TUPLE_COLS), axis=1
    )
    return df


# ---------------- Step 2: split å®ç° ----------------

#CSV_PATH = "processed_data/CIC-IDS-2017/all_embedded_flow.csv"
FLOW_CSV_PATH = os.path.join(ConfigManager.read_plot_data_path_config(), "all_embedded_flow.csv")
SESSION_CSV_PATH = os.path.join(ConfigManager.read_plot_data_path_config(), "all_split_session.csv")

def split_flow(df):
    """flow-level split"""
    labels = df["is_malicious"].values
    train_df, temp_df = train_test_split(
        df, test_size=1 - TRAIN_RATIO, stratify=labels, random_state=SEED
    )

    temp_labels = temp_df["is_malicious"].values
    val_df, test_df = train_test_split(temp_df, test_size=0.5,
                                      stratify=temp_labels, random_state=SEED)
    return train_df, val_df, test_df


def split_session_by_index(df_flow, session_csv_path=SESSION_CSV_PATH):
    """
    ä½¿ç”¨ SESSION_CSV_PATH ä¸­çš„ split åˆ’åˆ†ç»“æœï¼Œ
    å°† split æ ‡ç­¾ä¼ æ’­å› flow CSVã€‚
    """
    info(f"Loading session split rules: {session_csv_path}")
    df_sess = pd.read_csv(session_csv_path, low_memory=False)

    split_map = {}
    for _, row in df_sess.iterrows():
        if pd.isna(row["flow_uid_list"]):
            continue
        uids = eval(row["flow_uid_list"])  # å­—ç¬¦ä¸²è½¬åˆ—è¡¨
        for uid in uids:
            split_map[uid] = row["split"]  # train/validate/test

    # æ·»åŠ  split åˆ—
    if "uid" not in df_flow.columns:
        raise RuntimeError("Flow CSV ä¸­æœªæ‰¾åˆ° uid åˆ—ï¼Œæ— æ³•æ‰§è¡Œ session split")

    df_flow["split"] = df_flow["uid"].map(split_map)

    missing = df_flow["split"].isna().mean() * 100
    if missing > 1e-6:
        warn(f"â— {missing:.2f}% flows missing split mapping")

    # è¿‡æ»¤æ‰ split æœªå®šä¹‰çš„æ•°æ®
    df_flow = df_flow[df_flow["split"].notna()]

    train_df = df_flow[df_flow["split"] == "train"]
    val_df   = df_flow[df_flow["split"] == "validate"]
    test_df  = df_flow[df_flow["split"] == "test"]

    info(f"Session split summary:")
    info(f"Train: {len(train_df):,} rows")
    info(f"Val:   {len(val_df):,} rows")
    info(f"Test:  {len(test_df):,} rows")

    return train_df, val_df, test_df


# ========= Step 3: äº”å…ƒç»„æ³„éœ²ç»Ÿè®¡ =========

def check_five_tuple_leakage(train_df, test_df, split_mode="flow"):
    train_set = set(train_df["five_tuple"])
    test_set = set(test_df["five_tuple"])
    shared = train_set & test_set

    leakage_ratio = len(shared) / len(test_set) * 100

    print(f"\n=== Step 2: äº”å…ƒç»„æ³„éœ²æƒ…å†µ ({split_mode}-split) ===")
    print(f"Train äº”å…ƒç»„æ•°é‡: {len(train_set):,}")
    print(f"Test äº”å…ƒç»„æ•°é‡ : {len(test_set):,}")
    print(f"å…±äº«äº”å…ƒç»„æ•°é‡ : {len(shared):,}")
    print(color(f"ğŸ”¥ äº”å…ƒç»„æ³„éœ²æ¯”ä¾‹: {leakage_ratio:.2f}%", 
                "red" if leakage_ratio > 1 else "green"))

    if len(shared) > 0:
        print("\nç¤ºä¾‹å…±äº«äº”å…ƒç»„ï¼ˆå‰ 10 é¡¹ï¼‰:")
        print(list(shared)[:10])

    print("""
ğŸ“Œã€å…³äºäº”å…ƒç»„æ³„éœ²çš„è¯´æ˜ã€‘

"äº”å…ƒç»„æ³„éœ²" æ„å‘³ç€ï¼šåŒä¸€ä¸ª (srcIP, srcPort, dstIP, dstPort, proto)
åŒæ—¶å‡ºç°åœ¨äº† Train å’Œ Test ä¸­ï¼Œè¿™å°†å¯¼è‡´æ¨¡å‹ï¼š
    âœ“ è®°å¿†äº”å…ƒç»„æ¨¡å¼è€Œä¸æ˜¯æ³›åŒ–
    âœ“ Flow-Split æ—¶æ€§èƒ½è™šé«˜
    âœ“ Session-Split æ‰èƒ½çœŸå®è¯„ä»·æ³›åŒ–æ€§èƒ½
""")


# ========= Step 4: äº”å…ƒç»„æ ‡ç­¾å†²çª =========

def check_label_conflicts(train_df):
    group = train_df.groupby("five_tuple")["is_malicious"].unique()
    conflicts = [(k, v.tolist()) for k, v in group.items() if len(v) > 1]

    print("\n=== Step 3: äº”å…ƒç»„æ ‡ç­¾å†²çªæ£€æµ‹ï¼ˆTrain å†…éƒ¨ï¼‰ ===")
    print(f"Train ä¸­å‘ç°æ ‡ç­¾å†²çªçš„äº”å…ƒç»„æ•°é‡: {len(conflicts):,}")

    if conflicts:
        print("\nç¤ºä¾‹å†²çªï¼ˆå‰ 10 é¡¹ï¼‰:")
        print(conflicts[:10])

    print("""
    ğŸ“Œã€è¯´æ˜ã€‘

    æ ‡ç­¾å†²çª = åŒä¸€äº”å…ƒç»„åœ¨è®­ç»ƒæ•°æ®ä¸­åŒæ—¶å­˜åœ¨æ¶æ„ä¸æ­£å¸¸æ ·æœ¬ã€‚
    
    è¯´æ˜ï¼š
        â€¢ äº”å…ƒç»„æ— æ³•å”¯ä¸€å¯¹åº”æ¶æ„è¡Œä¸º
        â€¢ æ¨¡å‹ä¸èƒ½ä¾èµ–äº”å…ƒç»„ç»“æ„è®°å¿†
        â€¢ æ”»å‡»è€…å¤ç”¨åŒä¸€ IP/ç«¯å£è¿›è¡Œå¤šç±»å‹é€šä¿¡

    å†²çªè¶Šå¤š â†’ è¶Šè¯´æ˜éœ€è¦ session æˆ–å†…å®¹çº§åˆ«ç‰¹å¾å»ºæ¨¡
    """)

    return conflicts


# ========= å·¥å…·å‡½æ•°ï¼šç±»åˆ«å‹ç‰¹å¾çš„æ¡ä»¶ç†µ =========

def conditional_entropy(col_series, target_series):
    """
    H(Y | X) = Î£ P(X=x) * H(Y | X=x)
    è¶Šä½ = æ³„éœ²è¶Šå¼º
    """
    eps = 1e-9
    df = pd.DataFrame({"x": col_series, "y": target_series}).dropna()

    H = 0
    for v, subset in df.groupby("x"):
        p_x = len(subset) / len(df)
        p1 = (subset["y"] == 1).mean()
        p0 = 1 - p1
        h = - (p1 * np.log2(p1 + eps) + p0 * np.log2(p0 + eps))
        H += p_x * h
    return H

class FeaturePatternDetector:

    # ========== 1) æ£€æµ‹æ˜¯å¦ä¸º embedding/list-like ==========
    def _safe_sample_series(self, series, max_n=200):
        """ä»éç©ºå€¼ä¸­å®‰å…¨é‡‡æ ·ä¸€éƒ¨åˆ†ï¼Œç”¨äºåšæ¨¡å¼è¯†åˆ«ã€‚"""
        s = series.dropna()
        if s.empty:
            return s
        if len(s) > max_n:
            return s.sample(max_n, random_state=0)
        return s

    def _parse_numeric_array(self, value):
        """å°½é‡æŠŠä¸€ä¸ªå€¼è§£ææˆæ•°å€¼æ•°ç»„ï¼ˆç”¨äº sequence/embedding åˆ¤åˆ«ï¼‰ã€‚"""
        if value is None or (isinstance(value, float) and pd.isna(value)):
            return None

        # å¦‚æœæœ¬èº«å°±æ˜¯ list/tuple
        if isinstance(value, (list, tuple)):
            vals = []
            for x in value:
                try:
                    vals.append(float(x))
                except Exception:
                    return None
            return vals if len(vals) > 0 else None

        # å­—ç¬¦ä¸²æƒ…å†µ
        if isinstance(value, str):
            v = value.strip()
            if v == "":
                return None
            # å°è¯• JSON
            try:
                obj = json.loads(v)
                if isinstance(obj, (list, tuple)):
                    vals = []
                    for x in obj:
                        try:
                            vals.append(float(x))
                        except Exception:
                            return None
                    return vals if len(vals) > 0 else None
            except Exception:
                pass
            # å°è¯• Python literal
            try:
                obj = ast.literal_eval(v)
                if isinstance(obj, (list, tuple)):
                    vals = []
                    for x in obj:
                        try:
                            vals.append(float(x))
                        except Exception:
                            return None
                    return vals if len(vals) > 0 else None
            except Exception:
                pass
            # å°è¯•ç”¨é€—å·åˆ†å‰²çš„ç®€å•å½¢å¼
            if "," in v:
                parts = [p.strip() for p in v.split(",")]
                vals = []
                for p in parts:
                    try:
                        vals.append(float(p))
                    except Exception:
                        return None
                return vals if len(vals) > 0 else None

        return None

    def detect_array_semantic(self, col_name: str, series, sample_n: int = 100):
        """
        å°è¯•åŒºåˆ†ï¼š
        - embedding å‘é‡ï¼šé•¿åº¦å›ºå®šã€è¾ƒå°ï¼ˆæ¯”å¦‚ <=128ï¼‰ï¼Œåå­—ä¸­å¸¸å¸¦ freq/embedding/vector ç­‰
        - sequence åºåˆ—ï¼šé•¿åº¦å˜åŒ–è¾ƒå¤§ï¼Œæˆ–åå­—ä¸­å¸¦ packet/bulk/iat ç­‰
        """
        name = col_name.lower()
        s = self._safe_sample_series(series, max_n=sample_n)

        lengths = []
        ok = 0
        for v in s:
            arr = self._parse_numeric_array(v)
            if arr is None:
                continue
            ok += 1
            lengths.append(len(arr))

        if ok < max(3, len(s) * 0.3):
            # å¯è§£æçš„å¤ªå°‘ï¼Œè®¤ä¸ºä¸æ˜¯æ•°ç»„å‹
            return None

        if not lengths:
            return None

        min_len, max_len = min(lengths), max(lengths)
        span = max_len - min_len

        # åå­—ç‰¹å¾
        name_has_packet = any(k in name for k in ["packet", "bulk", "seq", "sequence", "iat"])
        name_has_embedding = any(k in name for k in ["embedding", "emb", "freq", "vector", "hist"])

        # embeddingï¼šé•¿åº¦è¾ƒå° & åŸºæœ¬å›ºå®š & åå­—åƒ embedding/freq
        if max_len <= 256 and span <= max_len * 0.1 and name_has_embedding:
            return "embedding"

        # sequenceï¼šé•¿åº¦æ³¢åŠ¨è¾ƒå¤§ æˆ– åå­—æ˜æ˜¾æ˜¯åºåˆ—
        if span > max_len * 0.1 or name_has_packet:
            return "sequence"

        # å¦‚æœé•¿åº¦å›ºå®šä½†æ²¡æœ‰æ˜æ˜¾åå­—çº¿ç´¢ï¼Œé»˜è®¤ä¹Ÿå embedding
        if max_len <= 256 and span == 0:
            return "embedding"

        # å¦åˆ™å½“æˆ sequence
        return "sequence"

    # ========== 2) æ£€æµ‹æ˜¯å¦ä¸º IP åœ°å€ ==========

    def _looks_like_ipv4(self, s: str) -> bool:
        try:
            ipaddress.IPv4Address(s)
            return True
        except Exception:
            return False

    def _looks_like_ipv6(self, s: str) -> bool:
        try:
            ipaddress.IPv6Address(s)
            return True
        except Exception:
            return False

    def looks_like_ip(self, colname, series):
        name = colname.lower()

        # ç‰¹å¾å heuristic
        name_ip_hint = any(h in name for h in [
            "id.orig_h", "id.resp_h", ".orig_h", ".resp_h"
        ])

        s = self._safe_sample_series(series.astype(str).str.strip())
        if s.empty:
            return False

        valid_cnt = 0
        checked = 0
        for v in s:
            v = v.split(',')[0].strip()

            if v in ["", "nan", "none", "-", "null", "unknown"]:
                continue
            checked += 1

            if self._looks_like_ipv4(v) or self._looks_like_ipv6(v):
                valid_cnt += 1

        if checked == 0:
            return False

        ratio = valid_cnt / checked

        # åªè¦åç§°æç¤º + â‰¥20% æ ·æœ¬åˆæ³•å³å¯è§†ä¸º IP
        if name_ip_hint and ratio >= 0.20:
            return True

        return ratio >= 0.40

    # ========== 3) æ£€æµ‹æ˜¯å¦ä¸º domain ==========
    def looks_like_domain(self, colname, series):
        s = self._safe_sample_series(series.astype(str).str.strip().str.lower())
        if s.empty:
            return False
        
        tlds = [".com", ".net", ".org", ".cn", ".io", ".edu", ".gov"]
        cnt = 0
        for v in s:
            if " " in v or "@" in v:
                continue
            if any(tld in v for tld in tlds):
                cnt += 1
        return cnt >= max(3, len(s) * 0.5)

    # ========== 4) æ£€æµ‹æ˜¯å¦ä¸º cipher åç§° ==========
    def looks_like_protocol_version(self, colname, series):
        name = colname.lower()
        if "version" not in name:
            return False

        SAMPLE = self._safe_sample_series(series.astype(str))
        cnt = 0
        for v in SAMPLE:
            v_lower = v.lower()
            if v_lower.startswith("tls") or v_lower.startswith("ssl"):
                cnt += 1

        return cnt >= max(3, len(SAMPLE) * 0.5)
    
    def looks_like_cipher(self, series):
        sample = series.dropna().astype(str).head(20)
        cnt = sum("TLS" in v.upper() or "AES" in v.upper() or "CHACHA" in v.upper()
                for v in sample)
        return cnt >= len(sample) * 0.5

    # ========== 5) æ£€æµ‹æ˜¯å¦ä¸º portï¼ˆç«¯å£ï¼‰ ==========
    def looks_like_port(self, colname, series):
        name = colname.lower()

        # å¿…é¡»ä¸¥æ ¼åŒ¹é…ç«¯å£å­—æ®µå‘½åï¼Œä¸å…è®¸ç»Ÿè®¡å­¦å…³é”®å­—
        PORT_HINTS = [
            ".orig_p", ".resp_p",
            "id.orig_p", "id.resp_p"
        ]
        if not any(name.endswith(h) for h in PORT_HINTS):
            return False
        
        # ç¦æ­¢ï¼šç»Ÿè®¡ / æ—¶é—´ / åŒ…é•¿åº¦ç­‰å­—æ®µè¢«å½“ç«¯å£
        BAD_HINTS = [
            "max", "min", "std", "avg", "mean", "tot",
            "size", "window", "payload", "pkts", "bytes"
        ]
        if any(b in name for b in BAD_HINTS):
            return False

        # æ•°å€¼éªŒè¯
        s = pd.to_numeric(self._safe_sample_series(series), errors="coerce").dropna()
        if s.empty:
            return False

        return s.min() >= 0 and s.max() <= 65535    
    
    # ========== 6) æ•°å€¼å­—ç¬¦ä¸²è‡ªåŠ¨åˆ¤æ–­ ==========
    def numeric_string_type(self, series):
        s = series.dropna().astype(str).str.strip()
        if len(s) == 0:
            return None
        # int
        if s.str.fullmatch(r"^-?\d+$").all():
            return "int"
        # float
        if s.str.fullmatch(r"^-?\d+\.\d+$").all():
            return "float"
        return None

    # ========== ğŸ† ä¸»å‡½æ•°ï¼šè‡ªåŠ¨ç±»å‹è¯†åˆ« ==========
    def detect_feature_type(self, colname, series):
        """
        è¿”å›:
            dtype_str: string, int64/float64/object/embedding/ip/domain/cipher
            feature_type: numeric / categorical / embedding / ignore / ip / domain / cipher / port
        """
        # ---------- 0) embedding æ£€æµ‹ ----------
        array_sem = self.detect_array_semantic(colname, series)
        if array_sem == "embedding":
            return "embedding", "embedding"
        elif array_sem == "sequence":
            return "sequence", "categorical"

        # ---------- 1) IP åœ°å€ ----------
        if self.looks_like_ip(colname, series):
            return "ip", "categorical"   # IP é€‚åˆä½œä¸º categorical

        # ---------- 2) Domain ----------
        if self.looks_like_domain(colname, series):
            return "domain", "categorical"

        # ---------- 3) Cipher ----------
        if self.looks_like_protocol_version(colname, series):
            return "protocol_version", "categorical"

        if self.looks_like_cipher(series):
            return "cipher", "categorical"

        # ---------- 4) Port ----------
        if self.looks_like_port(colname, series):
            return "port", "categorical"

        # ---------- 5) æ•°å€¼å‹ detection (åŒ…å« numeric string) ----------
        dtype = str(series.dtype)

        # ä¼˜å…ˆè¯†åˆ« object é‡Œçš„æ•°å­—å­—ç¬¦ä¸²
        if series.dtype == object:
            numeric_type = self.numeric_string_type(series)
            if numeric_type == "int":
                return "int64", "numeric"
            if numeric_type == "float":
                return "float64", "numeric"

        # pandas int/float
        if dtype.startswith("int"):
            return "int64", "numeric"
        if dtype.startswith("float"):
            return "float64", "numeric"

        # ---------- 6) é»˜è®¤ä½œä¸º categorical ----------
        return "object", "categorical"

# ========= Step 5: æ‰©å±•ç‰ˆé«˜å±ç‰¹å¾æ³„éœ²æ‰«æ =========

def check_high_risk_feature_leakage(train_df, test_df, top_k=100):
    print("\n=== Step 4: é«˜å±ç‰¹å¾æ³„éœ²æ£€æŸ¥ï¼ˆæ‰©å±•ç‰ˆï¼‰ ===")

    high_risk_results = []

    for col in train_df.columns:
        if col in ["five_tuple", "is_malicious"]:
            continue

        # è·³è¿‡ 99% ä»¥ä¸Šä¸ºç©ºçš„åˆ—ï¼ˆæ— æ„ä¹‰ï¼‰
        if train_df[col].isna().mean() > 0.99:
            continue

        filtered_train_df = filter_invalid_embedding_rows(train_df, col)
        if filtered_train_df.empty:
            continue  # æ•´åˆ—éƒ½æ— æ•ˆï¼Œè·³è¿‡

        filtered_test_df  = filter_invalid_embedding_rows(test_df, col)

        # ========= ç±»å‹åˆ¤æ–­ =========
        detector = FeaturePatternDetector()
        dtype_str, feature_type = detector.detect_feature_type(col, train_df[col])

        # ========= ç±»åˆ«å‹ç‰¹å¾ =========
        is_categorical = (feature_type == "categorical")
        is_numeric =  (feature_type == "numeric")
        if is_categorical:
            # ========= 1ï¼‰å­—ç¬¦ä¸²æˆ–Categoricalç‰¹å¾ =========
            # =============== è®¡ç®—Leakage Ratio ===============
            # è¿‡æ»¤æ— æ•ˆå€¼
            train_valid = filtered_train_df[col].dropna().astype(str).str.strip()
            test_valid  = filtered_test_df[col].dropna().astype(str).str.strip()

            min_support_train = 3  # å¯è°ƒï¼Œå»ºè®® 3~5
            min_support_test = 1

            # ä»…ä¿ç•™å‡ºç°æ¬¡æ•° >= min_support çš„ç±»åˆ«
            train_valid = train_valid[train_valid.groupby(train_valid).transform("count") >= min_support_train]
            test_valid  = test_valid[test_valid.groupby(test_valid).transform("count") >= min_support_test]

            train_u = set(train_valid[train_valid != ""])
            test_u  = set(test_valid[test_valid != ""])

            if len(test_u) == 0:
                continue

            shared = train_u & test_u
            leakage_ratio = len(shared) / len(test_u) * 100

            # =============== è®¡ç®—ç‰¹å¾åˆ—å’Œç›®æ ‡åˆ—çš„æ¡ä»¶ç†µ ===============
            valid_mask = filtered_train_df[col].notna()
            if valid_mask.sum() > 0:
                col_series = filtered_train_df.loc[valid_mask, col].astype(str)
                target_series = filtered_train_df.loc[valid_mask, "is_malicious"]

                if col_series.nunique() > 1:
                    cond_ent = conditional_entropy(col_series, target_series)
                else:
                    cond_ent = None
            else:
                cond_ent = None

            na_ratio = compute_na_ratio(train_df, test_df, col)

            leakage_score = leakage_risk_score(leakage_ratio, cond_ent, na_ratio)
            risk_lv = risk_level(leakage_score)

            high_risk_results.append((col, "categorical", dtype_str, leakage_ratio, cond_ent, na_ratio, leakage_score, risk_lv))
            
        elif is_numeric:
            # ========= 2) æ•°å€¼å‹ç‰¹å¾ï¼ˆå¢å¼ºç‰ˆæ³„éœ²æ£€æµ‹ï¼‰ =========
            # Pandas ä¼šæŠŠéƒ¨åˆ†åˆ—è¯†åˆ«æˆ objectï¼Œä½†å®é™…æ˜¯æ•°å­—ï¼Œéœ€è¦å¼ºåˆ¶è½¬æ¢
            try:
                numeric_col = pd.to_numeric(train_df[col], errors="coerce")
            except Exception:
                continue

            if numeric_col.nunique() <= 1:
                continue

            numeric_valid = numeric_col[train_df[col].notna()]
            label_valid = train_df.loc[numeric_valid.index, "is_malicious"]

            # Pearson
            pearson_corr = abs(numeric_valid.corr(label_valid))
            if np.isnan(pearson_corr):
                continue

            # Mutual Information
            try:
                MI = mutual_info_classif(
                    numeric_valid.values.reshape(-1, 1),
                    label_valid.values,
                    discrete_features=False
                )[0]
                MI_norm = MI / np.log1p(min(50, numeric_valid.nunique()))
            except Exception:
                MI_norm = 0.0

            # AUC Score (ç”¨ Logistic/éšæœºåˆ†ç±»å™¨)
            try:
                auc_score = roc_auc_score(
                    label_valid,
                    numeric_valid.fillna(numeric_valid.mean())
                )
            except Exception:
                auc_score = 0.5  # ä¸å¯åŒºåˆ†

            # æ•°å€¼æ³„éœ²ç»¼åˆåˆ†
            numeric_leak_score = (
                0.2 * pearson_corr +
                0.5 * MI_norm +
                0.3 * auc_score
            )

            risk_lv = risk_level(numeric_leak_score)

            high_risk_results.append((
                col, "numeric", dtype_str,
                pearson_corr, MI_norm, auc_score,
                numeric_leak_score, risk_lv
            ))
    # ============ è¾“å‡ºå¯é…ç½® top_k çš„ç»“æœ ============

    # ======== 1ï¼‰æ‰“å°ç±»åˆ«ç‰¹å¾ç»“æœ =========
    print(color(f"\nğŸ”¥ æœ€å¼ºæ³„éœ²çš„ç±»åˆ«å‹ç‰¹å¾ TOP {top_k}:", "red"))
    print("æ³¨æ„ï¼šæ¡ä»¶ç†µè¶Šå°ï¼Œç‰¹å¾å¯¹æ ‡ç­¾è¶Šæœ‰ç”¨ï¼ˆç›¸å…³æ€§æ›´å¼ºï¼‰")
    cat_feats = sorted(
        [r for r in high_risk_results if r[1] == "categorical"],
        key=lambda x: -x[6])[:top_k] # x[6] æ˜¯ç»¼åˆå¾—åˆ† leakage_score

    for col, _, dtype, leak, cond_ent, na, leak_score, risk_lvl in cat_feats:
        ce_str = f"{cond_ent:.4f}" if cond_ent is not None else "N/A"
        print(f"{col:45s} | ç±»å‹={dtype:10s} | æ³„éœ²ç‡={leak:6.2f}% | æ¡ä»¶ç†µ={ce_str} | N/Aç‡={na:.3f} | æ³„éœ²ç»¼åˆåˆ†={leak_score:.4f} | æ³„éœ²ç­‰çº§={risk_lvl}")

    print("æ³¨æ„ï¼šN/A è¡¨ç¤ºè¯¥ç‰¹å¾å€¼å”¯ä¸€æˆ–å‡ ä¹å”¯ä¸€æ— æ³•åˆ¤æ–­ç†µ")
    print("æ³¨æ„ï¼šæ¡ä»¶ç†µè¶Šå°ï¼Œç‰¹å¾å¯¹æ ‡ç­¾è¶Šæœ‰ç”¨ï¼ˆç›¸å…³æ€§æ›´å¼ºï¼‰")

    # ======== 2ï¼‰æ‰“å°æ•°å€¼å‹ç‰¹å¾ç»“æœ =========
    print(color(f"\nğŸ”¥ æœ€å¼ºæ³„éœ²çš„æ•°å€¼å‹ç‰¹å¾ TOP {top_k} (æŒ‰æ³„éœ²ç»¼åˆåˆ†):", "red"))
    print("æ³¨æ„ï¼šMI å’Œ AUC èƒ½æ•è·éçº¿æ€§ä¸å¯åˆ†ç±»æ³„éœ²é£é™©")

    num_feats = sorted(
        [r for r in high_risk_results if r[1] == "numeric"],
        key=lambda x: -x[6]  # x[6] æ˜¯ numeric_leak_score
    )[:top_k]

    for col, _, dtype, pearson_corr, MI_norm, auc_score, leak_score, risk_lvl in num_feats:
        print(
            f"{col:45s} | ç±»å‹={dtype:10s}"
            f" | Pearson={pearson_corr:.4f}"
            f" | MI={MI_norm:.4f}"
            f" | AUC={auc_score:.4f}"
            f" | æ³„éœ²ç»¼åˆåˆ†={leak_score:.4f}"
            f" | æ³„éœ²ç­‰çº§={risk_lvl}"
        )

    return high_risk_results

def leakage_risk_score(leak_ratio, ce, na_ratio):
    """æ³„éœ²ç»¼åˆå¾—åˆ†ï¼šæ³„éœ²ç‡ Ã— (1-CE_norm) Ã— (1-NA_ratio)"""
    if ce is None:
        return 0.0  # æ— ç»Ÿè®¡æ„ä¹‰
    ce_norm = min(max(ce, 0.0), 1.0)
    return (leak_ratio / 100.0) * (1.0 - ce_norm) * (1.0 - na_ratio)


def compute_na_ratio(df_train, df_test, col, zero_tol=1e-12):
    """
    è®¡ç®— N/A æ¯”ä¾‹ï¼š
    - å¸¸è§„åˆ—ï¼šåªæŠŠ NaN è§†ä¸ºç¼ºå¤±
    - ç±» embedding åˆ—ï¼ˆå¦‚ dns.query*_freq / ssl.server_name*_freqï¼‰ï¼š
        * NaN
        * è§£æä¸ºå‘é‡å â€œå…¨ 0â€ æˆ– â€œå¸¸æ•°å‘é‡â€
      éƒ½è§†ä¸º N/A
    """
    s_train = df_train[col]
    s_test  = df_test[col]

    total = len(s_train) + len(s_test)
    if total == 0:
        return 1.0

    # å…ˆç»Ÿè®¡æ™®é€š NaN
    na_mask_train = s_train.isna()
    na_mask_test  = s_test.isna()

    def is_zero_or_const_embedding(v):
        """
        ä»…åœ¨ â€œçœ‹èµ·æ¥åƒå‘é‡â€ çš„æƒ…å†µä¸‹ï¼Œé¢å¤–åˆ¤æ–­æ˜¯å¦å…¨ 0 / å¸¸æ•°ã€‚
        å¦åˆ™ä¸€å¾‹è¿”å› Falseï¼ˆé¿å…è¯¯ä¼¤æ™®é€šå­—ç¬¦ä¸²æˆ–æ•°å€¼åˆ—ï¼‰ã€‚
        """
        # NaN ç›´æ¥äº¤ç»™å¤–å±‚ na_mask å¤„ç†
        if v is None or (isinstance(v, float) and np.isnan(v)):
            return False

        # å­—ç¬¦ä¸²ï¼šå¿…é¡»åƒ "[0.1, 0.2]" æˆ– "0.1,0.2" æ‰å½“æˆ embedding
        if isinstance(v, str):
            txt = v.strip()
            if not txt:
                return False
            if not (("[" in txt and "]" in txt) or ("," in txt)):
                return False
            try:
                vals = [float(x) for x in txt.strip("[]").split(",") if x.strip() != ""]
            except Exception:
                return False

        # list / ndarrayï¼šç›´æ¥å½“å‘é‡å¤„ç†
        elif isinstance(v, (list, np.ndarray)):
            vals = list(v)

        else:
            # å…¶å®ƒç±»å‹ï¼ˆçº¯ float/int/string æ— é€—å·ï¼‰ï¼Œä¸å½“ embedding
            return False

        if len(vals) == 0:
            return True  # è§†ä¸ºæ— ä¿¡æ¯

        # å…¨ 0 æˆ–å¸¸æ•°å‘é‡ â†’ è§†ä¸º N/A
        if all(abs(x) < zero_tol for x in vals):
            return True
        if len(set(vals)) == 1:
            return True
        return False

    # åªå¯¹å½“å‰åˆ—è·‘ä¸€æ¬¡ applyï¼Œä»£ä»·è¿˜å¯ä»¥æ¥å—
    extra_na_train = s_train.apply(is_zero_or_const_embedding)
    extra_na_test  = s_test.apply(is_zero_or_const_embedding)

    na_count = (na_mask_train | extra_na_train).sum() + (na_mask_test | extra_na_test).sum()
    return na_count / total


def risk_level(score):
    if score >= 0.50: return "CRITICALğŸ”¥"
    if score >= 0.30: return "HIGHğŸš¨"
    if score >= 0.10: return "MEDIUMâš ï¸"
    if score > 0.00:  return "LOWğŸ™‚"
    return "NONE"

def filter_invalid_embedding_rows(df, col, zero_tol=1e-12):
    """ä»…å¤„ç† Embedding åˆ—ã€‚å¦‚æœå†…å®¹ä¸æ˜¯å¯è§£æåˆ—è¡¨ï¼Œç›´æ¥è¿”å›åŸ df[col]"""
    valid_mask = []
    parse_failed = False

    for v in df[col]:
        if isinstance(v, str):
            # å¿…é¡»åŒæ—¶æ»¡è¶³ï¼šæœ‰æ‹¬å·ä¸”æœ‰é€—å·ï¼Œæ‰è§†ä¸ºå‘é‡
            if not (("[" in v and "]" in v) or ("," in v)):
                parse_failed = True
                break

            try:
                vals = [float(x) for x in v.strip("[]").split(",") if x]
            except Exception:
                parse_failed = True
                break

        elif isinstance(v, (list, np.ndarray)):
            vals = list(v)

        else:
            # éå‘é‡ï¼Œä¸å¤„ç†
            parse_failed = True
            break

        if len(vals) == 0:
            valid_mask.append(False)
            continue
        
        # è¿‡æ»¤çº¯é›¶æˆ–å¸¸æ•°å‘é‡
        if all(abs(x) < zero_tol for x in vals) or len(set(vals)) == 1:
            valid_mask.append(False)
        else:
            valid_mask.append(True)

    # ğŸ‘‰ å¦‚æœè¯¥åˆ—ä¸æ˜¯ embedding åˆ—ï¼Œç›´æ¥è¿”å›åŸ df
    if parse_failed:
        return df
    
    return df[valid_mask]


def save_csv_reports(split_name, high_risk_results, output_dir="leakage_reports"):
    os.makedirs(output_dir, exist_ok=True)

    # åˆ†ç¦» categorical å’Œ numeric ç‰¹å¾
    cat_rows = [r for r in high_risk_results if r[1] == "categorical"]
    num_rows = [r for r in high_risk_results if r[1] == "numeric"]

    # ç±»åˆ«å‹ç‰¹å¾è¡¨å¤´
    cat_columns = [
        "feature",
        "type",
        "dtype",
        "leak_ratio",
        "conditional_entropy",
        "na_ratio",
        "leakage_score",
        "severity"
    ]
    df_cat = pd.DataFrame(cat_rows, columns=cat_columns)
    cat_path = os.path.join(output_dir,
        f"split_{split_name}_feature_leakage_categorical_top{len(cat_rows)}.csv"
    )
    df_cat.to_csv(cat_path, index=False, encoding="utf-8")
    print(f"ğŸ“ åˆ†ç±»æ³„éœ²æŠ¥å‘Šä¿å­˜: {cat_path}")

    # æ•°å€¼å‹ç‰¹å¾è¡¨å¤´
    num_columns = [
        "feature",
        "type",
        "dtype",
        "pearson_corr",
        "mutual_info_norm",
        "auc_score",
        "leakage_score",
        "severity"
    ]
    df_num = pd.DataFrame(num_rows, columns=num_columns)
    num_path = os.path.join(output_dir, 
        f"split_{split_name}_feature_leakage_numeric_top{len(num_rows)}.csv"
    )
    df_num.to_csv(num_path, index=False, encoding="utf-8")
    print(f"ğŸ“ æ•°å€¼æ³„éœ²æŠ¥å‘Šä¿å­˜: {num_path}")


def plot_leakage_bar(split_name, high_risk_results, top_k=50, output_dir="leakage_reports"):
    os.makedirs(output_dir, exist_ok=True)

    # ===== Categorical: ç”¨æ³„éœ²ç‡ =====
    cat_results = sorted(
        [r for r in high_risk_results if r[1] == "categorical"],
        key=lambda x: -x[3]   # leakage_ratio %
    )[:top_k]

    if len(cat_results) > 0:
        feats = [r[0] for r in cat_results]
        leaks = [r[3] for r in cat_results]

        plt.figure(figsize=(12, 6))
        plt.barh(feats, leaks)
        plt.xlabel("Leakage Ratio (%)")
        plt.ylabel("Feature")
        plt.title(f"Categorical Leakage Ratio Top-{len(feats)} ({split_name})")
        plt.tight_layout()

        out_path = os.path.join(output_dir, f"split_{split_name}_categorical_leakage_bar.png")
        plt.savefig(out_path, dpi=200)
        plt.close()
        print(f"ğŸ“Š Saved: {out_path}")

    else:
        print("âš  No categorical features for bar plot.")

    # ===== Numeric: ç”¨æ³„éœ²ç»¼åˆåˆ† =====
    num_results = sorted(
        [r for r in high_risk_results if r[1] == "numeric"],
        key=lambda x: -x[6]  # leakage_score
    )[:top_k]

    if len(num_results) > 0:
        feats = [r[0] for r in num_results]
        scores = [r[6] for r in num_results]

        plt.figure(figsize=(12, 6))
        plt.barh(feats, scores)
        plt.xlabel("Leakage Score")
        plt.ylabel("Feature")
        plt.title(f"Numeric Leakage Score Top-{len(feats)} ({split_name})")
        plt.tight_layout()

        out_path = os.path.join(output_dir, f"split_{split_name}_numeric_leakage_bar.png")
        plt.savefig(out_path, dpi=200)
        plt.close()
        print(f"ğŸ“ˆ Saved: {out_path}")
    else:
        print("âš  No numeric features for bar plot.")

def plot_leakage_radar(split_name, high_risk_results, top_k=10, output_dir="leakage_reports"):
    os.makedirs(output_dir, exist_ok=True)

    # ---------------- Categorical Radar ---------------- #
    cat_feats = [
        r for r in high_risk_results if r[1] == "categorical"
    ]
    cat_feats = sorted(cat_feats, key=lambda x: -x[3])[:top_k]  # ä½¿ç”¨ leakage_ratio

    if len(cat_feats) >= 3:  # è‡³å°‘éœ€è¦3ä¸ªï¼Œå¦åˆ™ä¸æ˜¯æœ‰æ•ˆé›·è¾¾å›¾
        feats = [r[0] for r in cat_feats]
        leak_vals = [float(r[3]) for r in cat_feats]  # r[3] æ³„éœ²ç‡

        N = len(feats)
        angles = np.linspace(0, 2 * np.pi, N, endpoint=False).tolist()
        angles += angles[:1]
        leak_vals += leak_vals[:1]

        plt.figure(figsize=(8, 8))
        ax = plt.subplot(111, polar=True)
        ax.plot(angles, leak_vals, marker="o")
        ax.fill(angles, leak_vals, alpha=0.3)
        ax.set_thetagrids(np.degrees(angles[:-1]), feats)

        plt.title(f"Categorical Leakage Radar (Top-{N}) ({split_name})")
        out_path = os.path.join(output_dir, f"split_{split_name}_categorical_leakage_radar.png")
        plt.savefig(out_path, dpi=200, bbox_inches="tight")
        plt.close()
        print(f"ğŸ“ˆ Radar Chart saved to {out_path}")
    else:
        print("âš  Too few categorical features for radar plot.")

    # ---------------- Numeric Radar ---------------- #
    num_feats = [
        r for r in high_risk_results if r[1] == "numeric"
    ]
    num_feats = sorted(num_feats, key=lambda x: -x[6])[:top_k]  # ä½¿ç”¨ numeric_leak_score = r[6]

    if len(num_feats) >= 3:
        feats = [r[0] for r in num_feats]
        leak_vals = [float(r[6]) for r in num_feats]  # r[6] ä¸º numeric_leak_score

        N = len(feats)
        angles = np.linspace(0, 2 * np.pi, N, endpoint=False).tolist()
        angles += angles[:1]
        leak_vals += leak_vals[:1]

        plt.figure(figsize=(8, 8))
        ax = plt.subplot(111, polar=True)
        ax.plot(angles, leak_vals, marker="o")
        ax.fill(angles, leak_vals, alpha=0.3)
        ax.set_thetagrids(np.degrees(angles[:-1]), feats)

        plt.title(f"Numeric Leakage Radar (Top-{N}) ({split_name})")
        out_path = os.path.join(output_dir, f"split_{split_name}_numeric_leakage_radar.png")
        plt.savefig(out_path, dpi=200, bbox_inches="tight")
        plt.close()
        print(f"ğŸ“ˆ Radar Chart saved to {out_path}")
    else:
        print("âš  Too few numeric features for radar plot.")


# ========= ä¸»ç¨‹åº =========

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--split_mode", type=str, default="flow",
                        choices=["flow","session"], help="flow or session split")
    parser.add_argument("--topk", type=int, default=200)
    args = parser.parse_args()

    output_dir = os.path.join(ConfigManager.read_plot_data_path_config(), "leakage_reports")
    os.makedirs(output_dir, exist_ok=True)
    log_path = os.path.join(output_dir, f"split_{args.split_mode}_leakage_analysis.log")

    class DualLogger(object):
        def __init__(self, *files):
            self.files = files
        def write(self, msg):
            for f in self.files:
                f.write(msg)
                f.flush()
        def flush(self):
            for f in self.files:
                f.flush()

    log_file = open(log_path, "w", encoding="utf-8")
    sys.stdout = DualLogger(sys.stdout, log_file)
    sys.stderr = DualLogger(sys.stderr, log_file)

    print(f"ğŸ” æ‰€æœ‰è¾“å‡ºå·²åŒæ—¶å†™å…¥æ—¥å¿—æ–‡ä»¶ï¼š{log_path}")

    info(f"Loading dataset: {FLOW_CSV_PATH}")
    df = pd.read_csv(FLOW_CSV_PATH, low_memory=False)
    info(f"Loaded {len(df):,} rows")

    # Step 1
    df = build_five_tuple(df)

    # Step 2: split
    if args.split_mode == "flow":
        train_df, val_df, test_df = split_flow(df)
    else:
        train_df, val_df, test_df = split_session_by_index(df)

    # Step 3: leakage
    check_five_tuple_leakage(train_df, test_df, args.split_mode)

    # Step 4: label conflicts
    check_label_conflicts(train_df)

    # Step 5: feature leakage
    high_risk_results = check_high_risk_feature_leakage(train_df, test_df, args.topk)

    print("\n=== Done ===")

    # å½“å‰è¿™ä¸ª split çš„åå­—ï¼Œç”¨æ¥åŒºåˆ†è¾“å‡ºæ–‡ä»¶
    split_name = f"{args.split_mode}_split"

    # ä¿å­˜ CSV
    save_csv_reports(split_name, high_risk_results, output_dir=output_dir)

    # å¯è§†åŒ–
    plot_leakage_bar(split_name, high_risk_results, top_k=args.topk, output_dir=output_dir)
    plot_leakage_radar(split_name, high_risk_results, top_k=min(10, args.topk), output_dir=output_dir)


if __name__ == "__main__":
    main()
