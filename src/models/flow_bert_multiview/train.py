import hydra
from omegaconf import DictConfig, OmegaConf, ListConfig
import pytorch_lightning as pl
from pytorch_lightning.loggers import TensorBoardLogger
from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping, LearningRateMonitor
import os
import sys
import logging
import torch
import shutil  # ç”¨äºæ–‡ä»¶å¤åˆ¶
from pytorch_lightning import seed_everything
from data.prepare_flow_file_pipeline import prepare_sampled_data_files

# ==========================================
# ğŸ”§ [æ ¸å¿ƒä¿®å¤] è·¯å¾„è‡ªåŠ¨å®šä½
# ==========================================
# è·å– train.py æ‰€åœ¨çš„ç»å¯¹è·¯å¾„
current_file_path = os.path.abspath(__file__)
# å›é€€ 4 å±‚æ‰¾åˆ°é¡¹ç›®æ ¹ç›®å½• code_TangYuJie
# (src/models/flow_bert_multiview/train.py -> src/models/flow_bert_multiview -> src/models -> src -> ROOT)
PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(current_file_path))))

# è®¾ç½® utils è·¯å¾„
utils_path = os.path.join(PROJECT_ROOT, 'src', 'utils')
sys.path.insert(0, utils_path)

# è®¾ç½®æ—¥å¿—
from logging_config import setup_preset_logging

# ä½¿ç”¨ç»Ÿä¸€çš„æ—¥å¿—é…ç½®
logger = setup_preset_logging(log_level=logging.INFO)

# æ·»åŠ  src åˆ° Python è·¯å¾„
sys.path.append(os.path.join(PROJECT_ROOT, 'src'))

from models.flow_bert_multiview import FlowBertMultiview
from data.flow_bert_multiview_dataset import MultiviewFlowDataModule


def setup_environment(cfg: DictConfig):
    """æ ¹æ®ç¡¬ä»¶å¯ç”¨æ€§è‡ªåŠ¨è®¾ç½®è®­ç»ƒç¯å¢ƒ"""
    force_single_gpu = cfg.get('force_single_gpu', False)

    if force_single_gpu:
        os.environ["CUDA_VISIBLE_DEVICES"] = "0"
        os.environ["PYTORCH_MPS_DISABLE"] = "1"
        cfg.data.num_workers = 0
        logger.info("âœ… å¼ºåˆ¶ä½¿ç”¨å• GPU è®­ç»ƒ")

    torch.set_float32_matmul_precision('high')
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

    if torch.cuda.is_available():
        cuda_count = torch.cuda.device_count()
        logger.info(f"æ£€æµ‹åˆ° {cuda_count} ä¸ªCUDAè®¾å¤‡å¯ç”¨")

        os.environ['PYTORCH_MPS_DISABLE'] = '1'
        accelerator = "gpu"
        if force_single_gpu:
            devices = 1
            strategy = "auto"
        else:
            devices = torch.cuda.device_count()
            strategy = "ddp_find_unused_parameters_true" if devices > 1 else "auto"
        logger.info("ä½¿ç”¨GPUè¿›è¡Œè®­ç»ƒ")
    else:
        logger.info("æœªæ£€æµ‹åˆ°GPUï¼Œä½¿ç”¨CPUè¿›è¡Œè®­ç»ƒ")
        accelerator = "cpu"
        devices = 1
        strategy = "auto"

    return accelerator, devices, strategy


def validate_config(cfg: DictConfig):
    """éªŒè¯é…ç½®å®Œæ•´æ€§ (ç®€åŒ–ç‰ˆ)"""
    if not hasattr(cfg.optimizer, 'default_total_steps'):
        cfg.optimizer.default_total_steps = 10000

    # ç®€å•çš„å¿…è¦æ€§æ£€æŸ¥
    if not cfg.model.bert.model_name:
        raise ValueError("Config Error: model.bert.model_name is missing")


def resolve_dataset_paths(cfg):
    dataset_cfg = cfg.datasets
    cfg.data.flow_data_path = dataset_cfg.flow_data_path
    cfg.data.session_split.session_split_path = dataset_cfg.session_split_path
    logger.info(f"ğŸ“¦ ä½¿ç”¨æ•°æ®é›†: {cfg.data.dataset}")


@hydra.main(config_path="./config", config_name="flow_bert_multiview_config", version_base=None)
def main(cfg: DictConfig):
    os.environ['HYDRA_FULL_ERROR'] = '1'
    logger.info(f"ğŸ“ é¡¹ç›®æ ¹ç›®å½•å®šä½: {PROJECT_ROOT}")

    try:
        seed_everything(cfg.data.random_state, workers=True)

        # åˆ‡æ¢å›é¡¹ç›®æ ¹ç›®å½•ï¼Œç¡®ä¿ç›¸å¯¹è·¯å¾„æ­£ç¡®
        if os.path.exists(PROJECT_ROOT):
            os.chdir(PROJECT_ROOT)

        accelerator, devices, strategy = setup_environment(cfg)
        validate_config(cfg)
        resolve_dataset_paths(cfg)

        prepare_sampled_data_files(cfg)

        datamodule = MultiviewFlowDataModule(cfg)
        datamodule.setup("fit")

        model = FlowBertMultiview(cfg, dataset=datamodule.train_dataset)

        # æ¼‚ç§»æ£€æµ‹å™¨åˆå§‹åŒ–æ—¥å¿—
        if hasattr(model, 'drift_detector') and model.drift_detector is not None:
            stats = model.drift_detector.get_statistics()
            logger.info(f"âœ… æ¼‚ç§»æ£€æµ‹å™¨å°±ç»ª: {stats}")

        logger_tb = TensorBoardLogger(
            save_dir=cfg.logging.save_dir,
            name=cfg.logging.name,
            version=cfg.logging.get('version')
        )

        # 1. ç¡®å®šä¿å­˜ç›®å½•: code_TangYuJie/processed_data
        export_dir_name = cfg.training.get('model_export_dir', 'processed_data')

        # å¦‚æœæ˜¯ç»å¯¹è·¯å¾„åˆ™ç›´æ¥ä½¿ç”¨ï¼Œå¦åˆ™æ‹¼æ¥åœ¨ PROJECT_ROOT ä¸‹
        if os.path.isabs(export_dir_name):
            checkpoint_dir = export_dir_name
        else:
            checkpoint_dir = os.path.join(PROJECT_ROOT, export_dir_name)

        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs(checkpoint_dir, exist_ok=True)
        logger.info(f"ğŸ“‚ æ¨¡å‹å°†å¼ºåˆ¶ä¿å­˜è‡³: {checkpoint_dir}")

        checkpoint_callback = ModelCheckpoint(
            dirpath=checkpoint_dir,  # âœ… å¼ºåˆ¶ç›´æ¥å†™å…¥ç›®æ ‡æ–‡ä»¶å¤¹ï¼Œä¸ä½¿ç”¨é»˜è®¤çš„ lightning_logs
            monitor=cfg.training.model_checkpoint.monitor,
            mode=cfg.training.model_checkpoint.mode,
            save_top_k=cfg.training.model_checkpoint.save_top_k,
            filename=cfg.training.model_checkpoint.filename,  # ä¾‹å¦‚ "best-{epoch}"
            save_last=True,  # âœ… å¼ºåˆ¶ä¿å­˜ last.ckptï¼Œå³ä½¿éªŒè¯é›†æŒ‡æ ‡æœªè§¦å‘
            auto_insert_metric_name=False
        )

        early_stopping = EarlyStopping(
            monitor=cfg.training.early_stopping.monitor,
            patience=cfg.training.early_stopping.patience,
            mode=cfg.training.early_stopping.mode,
            min_delta=cfg.training.early_stopping.get('min_delta', 0.001)
        )

        # æ¼‚ç§»æ£€æµ‹å›è°ƒ
        drift_callbacks = []
        if hasattr(cfg.training, 'drift_monitor') and cfg.training.drift_monitor.enabled:
            try:
                from pytorch_lightning.callbacks import Callback
                class DriftMonitorCallback(Callback):
                    def __init__(self, check_interval=50):
                        super().__init__()
                        self.check_interval = check_interval
                        self.batch_count = 0

                    def on_train_batch_end(self, trainer, pl_module, outputs, batch, batch_idx):
                        self.batch_count += 1
                        if self.batch_count % self.check_interval == 0:
                            if hasattr(pl_module, 'drift_detector') and pl_module.drift_detector is not None:
                                is_drift, B, info = pl_module.drift_detector.detect_drift()
                                # ä»…åšæ—¥å¿—è®°å½•ï¼Œä¸å¹²é¢„è®­ç»ƒ

                drift_callback = DriftMonitorCallback(
                    check_interval=cfg.training.drift_monitor.get('check_interval', 50)
                )
                drift_callbacks.append(drift_callback)
            except Exception as e:
                logger.warning(f"æ¦‚å¿µæ¼‚ç§»å›è°ƒåˆ›å»ºå¤±è´¥: {e}")

        callbacks = [checkpoint_callback, early_stopping, LearningRateMonitor(logging_interval='step')]
        callbacks.extend(drift_callbacks)

        trainer = pl.Trainer(
            max_epochs=cfg.training.max_epochs,
            accelerator=accelerator,
            devices=devices,
            precision=cfg.training.precision,
            # gradient_clip_val=cfg.training.gradient_clip_val,
            callbacks=callbacks,
            strategy=strategy,
            enable_progress_bar=True,
            logger=logger_tb,
            detect_anomaly=cfg.training.get('detect_anomaly', False)
        )

        logger.info("ğŸš€ å¼€å§‹è®­ç»ƒ...")
        trainer.fit(model, datamodule=datamodule)
        logger.info("ğŸ è®­ç»ƒå®Œæˆï¼")

        # ==========================================================
        # ğŸ”§ [æ ¸å¿ƒä¿®å¤] ç¡®ä¿ best_model_path æœ‰æ•ˆå¹¶å¤åˆ¶ä¸ºæ ‡å‡†åç§°
        # ==========================================================
        best_model_path = checkpoint_callback.best_model_path

        # 1. æ£€æŸ¥ best_model æ˜¯å¦å­˜åœ¨
        if not best_model_path or not os.path.exists(best_model_path):
            logger.warning("âš ï¸ æœªæ‰¾åˆ° Best Model (å¯èƒ½éªŒè¯é›†æŒ‡æ ‡æœªè§¦å‘)ï¼Œå°è¯•ä½¿ç”¨ Last Model...")
            best_model_path = checkpoint_callback.last_model_path

        # 2. å¦‚æœæ‰¾åˆ°äº†æœ‰æ•ˆæ¨¡å‹ (best æˆ– last)
        if best_model_path and os.path.exists(best_model_path):
            logger.info(f"âœ… æ•è·åˆ°æ¨¡å‹è·¯å¾„: {best_model_path}")

            # 3. ç»Ÿä¸€å¤åˆ¶ä¸º 'best_model.ckpt' ä»¥ä¾›ä¸‹æ¸¸ä»»åŠ¡ä½¿ç”¨
            final_path = os.path.join(checkpoint_dir, "best_model.ckpt")

            # é¿å…è‡ªæˆ‘å¤åˆ¶
            if os.path.abspath(best_model_path) != os.path.abspath(final_path):
                shutil.copy(best_model_path, final_path)
                logger.info(f"ğŸ“¦ å·²å°†æ¨¡å‹å½’æ¡£è‡³: {final_path}")

            # æ›´æ–°è·¯å¾„å˜é‡ç”¨äºæµ‹è¯•
            best_model_path = final_path
        else:
            logger.error("âŒ ä¸¥é‡é”™è¯¯: æœªç”Ÿæˆä»»ä½•æ¨¡å‹æ–‡ä»¶ï¼(best_model_path å’Œ last_model_path å‡æ— æ•ˆ)")
            logger.error("è¯·æ£€æŸ¥: 1. max_epochs æ˜¯å¦å¤ªå°? 2. æ•°æ®é›†æ˜¯å¦ä¸ºç©º? 3. ç£ç›˜ç©ºé—´æ˜¯å¦å·²æ»¡?")
            return  # é€€å‡ºï¼Œæ— æ³•æµ‹è¯•

        # ==========================================================
        # æµ‹è¯•é˜¶æ®µ
        # ==========================================================
        logger.info(f"ğŸ§ª å¼€å§‹æµ‹è¯• (åŠ è½½: {best_model_path})...")
        best_model = FlowBertMultiview.load_from_checkpoint(
            best_model_path,
            cfg=cfg,
            dataset=datamodule.train_dataset
        )

        trainer_test = pl.Trainer(
            accelerator=accelerator,
            devices=1,
            logger=False,
            enable_checkpointing=False,
        )
        trainer_test.test(best_model, datamodule=datamodule)

        if hasattr(best_model, 'drift_detector') and best_model.drift_detector is not None:
            best_model.on_test_end()

    except Exception as e:
        logger.error(f"âŒ è®­ç»ƒå¤±è´¥: {e}", exc_info=True)
        raise


if __name__ == "__main__":
    main()