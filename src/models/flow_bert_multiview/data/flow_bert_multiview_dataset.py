import pandas as pd
import numpy as np
import torch
from torch.utils.data import Dataset, DataLoader
import pytorch_lightning as pl
from omegaconf import DictConfig
import logging
import os,sys
import ast
from typing import List, Dict, Any, Tuple
import re
from tqdm import tqdm  # æ·»åŠ tqdmå¯¼å…¥
from collections import Counter

# å¯¼å…¥é…ç½®ç®¡ç†å™¨å’Œç›¸å…³æ¨¡å—
try:
    # æ·»åŠ ../../utilsç›®å½•åˆ°Pythonæœç´¢è·¯å¾„
    utils_path = os.path.join(os.path.dirname(__file__), '..', '..', '..', 'utils')
    sys.path.insert(0, utils_path)    
    import config_manager as ConfigManager
    from logging_config import setup_preset_logging
    # ä½¿ç”¨ç»Ÿä¸€çš„æ—¥å¿—é…ç½®
    logger = setup_preset_logging(log_level=logging.INFO)

except ImportError as e:
    print(f"å¯¼å…¥é”™è¯¯: {e}")
    print("è¯·ç¡®ä¿é¡¹ç›®ç»“æ„æ­£ç¡®ï¼Œæ‰€æœ‰ä¾èµ–æ¨¡å—å¯ç”¨")
    sys.exit(1)

class MultiviewFlowDataset(Dataset):
    """å¤šè§†å›¾æµé‡æ•°æ®é›†"""
    
    def __init__(self, df: pd.DataFrame, cfg: DictConfig, is_training: bool = True, 
                 train_categorical_mappings: Dict = None,
                 train_categorical_columns_effective: List[str] = None):
        self.flow_df = df.reset_index(drop=True)
        self.cfg = cfg
        self.labels_cfg = cfg.datasets.labels
        self.is_training = is_training
        self.max_seq_length = cfg.data.max_seq_length 
        self.mtu_normalize = getattr(self.cfg.data.sequence_features, "mtu_normalize", 1500)
        
        # å°è¯•ä»é…ç½®ä¸­è¯»å–session_label_id_mapçš„é•¿åº¦
        self.session_label_id_map = dict(ConfigManager.read_session_label_id_map(self.cfg.data.dataset))                    

        # æ£€æŸ¥å„ä¸ªå¯é€‰ç‰¹å¾æ˜¯å¦å¯ç”¨
        self.categorical_features_enabled = hasattr(cfg.data.tabular_features, "categorical_features") \
            and cfg.data.tabular_features.categorical_features is not None

        if self.categorical_features_enabled:
            self.categorical_columns = cfg.data.tabular_features.categorical_features
        else:
            self.categorical_columns = []

        # ğŸ”´ğŸ”´ğŸ”´ å…³é”®ä¿®æ”¹ï¼šéè®­ç»ƒé›†ä½¿ç”¨è®­ç»ƒé›†çš„æ˜ å°„
        if self.categorical_features_enabled and not is_training and train_categorical_mappings is not None:
            self.categorical_val2idx_mappings = train_categorical_mappings
            if train_categorical_columns_effective is None:
                raise ValueError("éè®­ç»ƒé›†å¿…é¡»æä¾› train_categorical_columns_effective å‚æ•°")
            
            if len(train_categorical_columns_effective) == 0:
                logger.warning("âš ï¸ train_categorical_columns_effective ä¸ºç©ºåˆ—è¡¨")

            self.train_categorical_columns_effective = train_categorical_columns_effective
            self.use_train_mappings = True
            logger.info(f"âœ… ä½¿ç”¨è®­ç»ƒé›†çš„ç±»åˆ«æ˜ å°„ï¼ˆæ˜ å°„å¤§å°: {len(train_categorical_mappings)}ï¼‰")
        else:
            self.categorical_val2idx_mappings = {}
            self.train_categorical_columns_effective = []
            self.use_train_mappings = False

        self.text_features_enabled = False
        if hasattr(cfg.data, 'text_features') and cfg.data.text_features is not None:
            if hasattr(cfg.data.text_features, 'enabled') and cfg.data.text_features.enabled:
                self.text_features_enabled = True
                
        self.domain_embedding_enabled = False
        if hasattr(cfg.data, 'domain_name_embedding_features') and hasattr(cfg.data.domain_name_embedding_features, 'enabled') and hasattr(cfg.data.domain_name_embedding_features, 'column_list'):
            if cfg.data.domain_name_embedding_features.enabled and len(cfg.data.domain_name_embedding_features.column_list) > 0:
                self.domain_embedding_enabled = True
        
        self.sequence_features_enabled = False
        if hasattr(cfg.data, 'sequence_features') and cfg.data.sequence_features is not None:
            if hasattr(cfg.data.sequence_features, 'enabled') and cfg.data.sequence_features.enabled:
                self.sequence_features_enabled = True
                
        # æ‰“å°åºåˆ—ç‰¹å¾æ˜¯å¦å¯ç”¨
        if self.sequence_features_enabled:
            logger.info("å¯ç”¨åºåˆ—ç‰¹å¾è§†å›¾")
        else:
            logger.info("åºåˆ—ç‰¹å¾æœªé…ç½®ï¼Œè·³è¿‡è¯¥è§†å›¾")

        # æ‰“å°æ–‡æœ¬ç‰¹å¾æ˜¯å¦å¯ç”¨
        if self.text_features_enabled:
            logger.info("å¯ç”¨æ–‡æœ¬ç‰¹å¾è§†å›¾")
        else:
            logger.info("æ–‡æœ¬ç‰¹å¾æœªé…ç½®ï¼Œè·³è¿‡è¯¥è§†å›¾")

        # æ‰“å°åŸŸååµŒå…¥ç‰¹å¾æ˜¯å¦å¯ç”¨
        if self.domain_embedding_enabled:
            logger.info(f"å¯ç”¨åŸŸååµŒå…¥ç‰¹å¾ï¼Œå…± {len(cfg.data.domain_name_embedding_features.column_list)} ä¸ªç‰¹å¾åˆ—")
            self.prob_list_length = len(self.session_label_id_map)
            # logger.info(f"ä»é…ç½®ä¸­è¯»å–åˆ°åŸŸååµŒå…¥çš„æ¦‚ç‡åˆ—è¡¨é•¿åº¦: {self.prob_list_length}, label_id_map = {label_id_map}")
        else:
            logger.info("åŸŸååµŒå…¥ç‰¹å¾æœªé…ç½®æˆ–ä¸ºç©ºï¼Œè·³è¿‡è¯¥è§†å›¾")

        # é¢„å¤„ç†æ•°æ®
        self._preprocess_data()

        # ===============================
        # å–„æ„/æ¶æ„è¡Œä¸ºçš„äºŒåˆ†ç±»æ ‡ç­¾é…ç½®
        # ===============================
        self.is_malicious_column = cfg.data.is_malicious_column
        if self.is_malicious_column in self.flow_df.columns:
            self.is_malicious_labels = self.flow_df[self.is_malicious_column].astype(int).values
        else:
            raise KeyError(f"æ‰¾ä¸åˆ° is_malicious æ ‡ç­¾åˆ— {self.is_malicious_column}ï¼Œè¯·æ£€æŸ¥ CSV æ–‡ä»¶ å’Œé…ç½®æ–‡ä»¶ is_malicious_column")

        # ===============================
        # å¤šåˆ†ç±»æ ‡ç­¾é…ç½®
        # ===============================
        self.multiclass_label_column = self.cfg.data.multiclass_label_column
        if not self.multiclass_label_column in self.flow_df.columns:
            raise KeyError(f"æ‰¾ä¸åˆ°å¤šåˆ†ç±»æ ‡ç­¾åˆ— {self.multiclass_label_column}ï¼Œè¯·æ£€æŸ¥ CSV å’Œé…ç½®æ–‡ä»¶ multiclass_label_column")

        # attack_family ç±»åˆ«ï¼ˆå»é™¤é¦–å°¾ç©ºç™½ï¼Œä¿æŒåŸå§‹å¤§å°å†™ï¼‰
        self.attack_family_classes = (
            [c.strip() for c in self.labels_cfg.attack_family.classes]
            if "attack_family" in self.labels_cfg
            else None
        )

        # attack_type ç±»åˆ«ï¼ˆå»é™¤é¦–å°¾ç©ºç™½ï¼Œä¿æŒåŸå§‹å¤§å°å†™ï¼‰
        self.attack_type_classes = (
            [c.strip() for c in self.labels_cfg.attack_type.classes]
            if "attack_type" in self.labels_cfg
            else None
        )

        # attack_type -> attack_family æ˜ å°„ï¼ˆåŒæ—¶ strip key å’Œ valueï¼‰
        self.attack_type_parent_mapping = (
            {k.strip(): v.strip() for k, v in self.labels_cfg.attack_type.parent_mapping.items()}
            if "attack_type" in self.labels_cfg
            and hasattr(self.labels_cfg.attack_type, "parent_mapping")
            else None
        )

        # æ£€æŸ¥attack_type -> attack_family æ˜ å°„çš„é…ç½®é”™è¯¯
        if self.attack_type_parent_mapping is not None:
            for t, f in self.attack_type_parent_mapping.items():
                assert self.attack_type_classes is None or t in self.attack_type_classes, \
                    f"attack_type '{t}' not in labels.attack_type.classes"
                assert self.attack_family_classes is None or f in self.attack_family_classes, \
                    f"attack_family '{f}' not in labels.attack_family.classes"

        if self.attack_family_classes is not None:
            logger.info(
                f"[Dataset] attack_family_classes ({len(self.attack_family_classes)}): "
                f"{self.attack_family_classes}"
            )

        if self.attack_type_classes is not None:
            logger.info(
                f"[Dataset] attack_type_classes ({len(self.attack_type_classes)}): "
                f"{self.attack_type_classes}"
            )

    def _preprocess_data(self):
        """é¢„å¤„ç†æ•°æ®"""
        if hasattr(self, '_preprocessed') and self._preprocessed:
            logger.info("æ•°æ®å·²ç»é¢„å¤„ç†è¿‡ï¼Œè·³è¿‡...")
            return
        
        self._preprocessed = True  # æ ‡è®°ä¸ºå·²å¤„ç†
        logger.info("é¢„å¤„ç†å¤šè§†å›¾æ•°æ®...")
    
        # è®¡ç®—å®é™…å¯ç”¨çš„è§†å›¾æ•°é‡
        self.num_optional_views = 1  # è¡¨æ ¼æ•°æ®ç‰¹å¾ï¼šæ•°å€¼ç‰¹å¾ï¼ˆå¿…é€‰ï¼‰ + åŸŸååµŒå…¥ç‰¹å¾ï¼ˆå¯é€‰ï¼‰
        if self.text_features_enabled:
            self.num_optional_views += 1 # æ–‡æœ¬è§†å›¾å¯é€‰
        if self.sequence_features_enabled:
            self.num_optional_views += 1 # æ•°æ®åŒ…åºåˆ—è§†å›¾å¯é€‰

        # ä½¿ç”¨tqdmæ·»åŠ è¿›åº¦æ¡ï¼›æ³¨æ„ï¼šé™¤äº†optional_viewsï¼Œè¿˜æœ‰å¿…é€‰çš„ç±»åˆ«å‹ç‰¹å¾+æ•°å€¼å‹ç‰¹å¾
        with tqdm(total=self.num_optional_views+2, desc="æ•°æ®é¢„å¤„ç†è¿›åº¦", position=0, leave=True) as pbar:
            # å¤„ç†æ•°æ®åŒ…åºåˆ—ç‰¹å¾ï¼ˆå¯é€‰ï¼‰
            if self.sequence_features_enabled:
                self._process_sequence_features()
                pbar.update(1)
                pbar.set_description("æ•°æ®é¢„å¤„ç†é˜¶æ®µï¼šIPæ•°æ®åŒ…åºåˆ—ç‰¹å¾å¤„ç†å®Œæˆ")
            else:
                # åˆ›å»ºç©ºçš„åºåˆ—ç‰¹å¾å ä½ç¬¦
                if not hasattr(self, 'sequences') or not self.sequences:
                    self.sequences = [{
                        'directions': [0.0] * self.max_seq_length,
                        'payload_sizes': [0.0] * self.max_seq_length,
                        'iat_times': [0.0] * self.max_seq_length,
                        'packet_numbers': [0.0] * self.max_seq_length,
                        'avg_payload_sizes': [0.0] * self.max_seq_length,
                        'durations': [0.0] * self.max_seq_length,                        
                        'sequence_mask': [0] * self.max_seq_length,
                        'original_length': 0
                    } for _ in range(len(self.flow_df))]
                pbar.set_description("æ•°æ®é¢„å¤„ç†é˜¶æ®µï¼šè·³è¿‡åºåˆ—ç‰¹å¾å¤„ç†")
            
            # å¤„ç†æ–‡æœ¬ç‰¹å¾
            if self.text_features_enabled:
                self._process_text_features()
                pbar.update(1)
                pbar.set_description("æ•°æ®é¢„å¤„ç†é˜¶æ®µï¼šæ–‡æœ¬ç‰¹å¾å¤„ç†å®Œæˆ")
            else:
                # åˆ›å»ºç©ºçš„æ–‡æœ¬ç‰¹å¾å ä½ç¬¦
                if not hasattr(self, 'text_features') or not self.text_features:
                    self.text_features = [{} for _ in range(len(self.flow_df))]
                pbar.set_description("æ•°æ®é¢„å¤„ç†é˜¶æ®µï¼šè·³è¿‡æ–‡æœ¬ç‰¹å¾å¤„ç†")               
            
            # å¤„ç†åŸŸååµŒå…¥ç‰¹å¾ï¼ˆæ¦‚ç‡åˆ—è¡¨ï¼‰
            if self.domain_embedding_enabled:
                self._process_domain_embedding_features()
                pbar.update(1)
                pbar.set_description("æ•°æ®é¢„å¤„ç†é˜¶æ®µï¼šåŸŸååµŒå…¥ç‰¹å¾å¤„ç†å®Œæˆ")
            else:
                # åˆ›å»ºç©ºçš„åŸŸååµŒå…¥ç‰¹å¾å ä½ç¬¦
                if not hasattr(self, 'domain_embedding_features') or not self.domain_embedding_features:
                    self.domain_embedding_features = [[] for _ in range(len(self.flow_df))]
                pbar.set_description("æ•°æ®é¢„å¤„ç†é˜¶æ®µï¼šè·³è¿‡åŸŸååµŒå…¥ç‰¹å¾å¤„ç†")                              

            # å¤„ç†ç±»åˆ«å‹ç‰¹å¾ï¼ˆå¿…é€‰ï¼‰            
            if self.categorical_features_enabled:
                self._process_categorical_features()
                pbar.update(1)
                pbar.set_description("æ•°æ®é¢„å¤„ç†é˜¶æ®µï¼šç±»åˆ«ç‰¹å¾å¤„ç†å®Œæˆ")

            # å¤„ç†æ•°å€¼å‹ç‰¹å¾ï¼ˆå¿…é€‰ï¼‰
            self._process_numeric_features()
            pbar.update(1)
            pbar.set_description("æ•°æ®é¢„å¤„ç†é˜¶æ®µï¼šé€æµæ•°å€¼ç‰¹å¾å¤„ç†å®Œæˆ")

        logger.info("æ•°æ®é¢„å¤„ç†å®Œæˆ")
    
    def _process_sequence_features(self):
        """å¤„ç†æ•°æ®åŒ…åºåˆ—ç‰¹å¾"""
        self.sequences = []
        
        # æ·»åŠ è¯¦ç»†çš„è¿›åº¦æ¡
        with tqdm(total=len(self.flow_df), desc="å¤„ç†æ•°æ®åŒ…åºåˆ—ç‰¹å¾", position=0, leave=True) as pbar:
            for idx, row in self.flow_df.iterrows():
                sequence_data = self._parse_sequence_row(row)
                self.sequences.append(sequence_data)
                pbar.update(1)
    
        # ç»Ÿè®¡ç©ºåºåˆ—æ•°é‡ï¼ˆç”¨äºæ—¥å¿—è®°å½•ï¼‰
        empty_count = sum(1 for seq in self.sequences if seq['original_length'] == 0)
        if empty_count > 0:
            logger.info(f"åºåˆ—ç‰¹å¾å¤„ç†å®Œæˆï¼Œå‘ç° {empty_count} ä¸ªç©ºåºåˆ—ï¼ˆå·²ç”¨é›¶å¡«å……ï¼‰")
        else:
            logger.info("åºåˆ—ç‰¹å¾å¤„ç†å®Œæˆï¼Œæ‰€æœ‰åºåˆ—æ•°æ®æœ‰æ•ˆ")

    
    def _parse_sequence_row(self, row: pd.Series) -> Dict[str, Any]:
        """è§£æå•è¡Œçš„åºåˆ—æ•°æ®ï¼Œè¿”å›èåˆäº†æ–¹å‘ä¿¡æ¯çš„ç‰¹å¾"""
        try:
            # -------------------------------
            # è·å– flow uidï¼Œæ‰“å°è°ƒè¯•ä¿¡æ¯ç”¨
            # -------------------------------
            # å°½é‡å…¼å®¹å¸¸è§ UID å­—æ®µå
            flow_uid = (
                row.get("uid") 
                or row.get("flow_uid") 
                or row.get("flowID") 
                or row.get("Flow ID") 
                or "UNKNOWN_UID"
            )

            # è§£ææ–¹å‘å‘é‡
            packet_directions_str = row.get(self.cfg.data.sequence_features.packet_direction, '[]')
            packet_directions = self._safe_parse_list(packet_directions_str)
            
            # è§£æè½½è·å¤§å°å‘é‡ï¼ˆ0-1500å­—èŠ‚èŒƒå›´ï¼‰
            packet_payload_sizes_str = row.get(self.cfg.data.sequence_features.packet_payload, '[]')
            packet_payload_sizes = self._safe_parse_list(packet_payload_sizes_str)
            
            # è§£ææ—¶é—´é—´éš”å‘é‡ï¼ˆæ¯«ç§’å•ä½ï¼Œå¯èƒ½è¾¾åˆ°ç™¾ä¸‡çº§ï¼‰
            packet_iat_times_str = row.get(self.cfg.data.sequence_features.packet_iat, '[]')
            packet_iat_times = self._safe_parse_list(packet_iat_times_str)

            # è§£ææ•°æ®åŒ…æ—¶é—´æˆ³åºåˆ—
            packet_timestamps_str = row.get(self.cfg.data.sequence_features.packet_timestamp, '[]')
            packet_timestamps = self._safe_parse_list(packet_timestamps_str)

            # è§£æå„bulkä¸­ç¬¬ä¸€ä¸ªæ•°æ®åŒ…çš„ä½ç½®çš„åºåˆ—
            # bulk_first_packet_index_vector_str = row.get(self.cfg.data.sequence_features.bulk_first_packet_index_vector, '[]')
            # bulk_first_packet_indices = self._safe_parse_list(bulk_first_packet_index_vector_str)

            # è§£æå„bulkçš„é•¿åº¦åºåˆ—
            bulk_length_vector_str = row.get(self.cfg.data.sequence_features.bulk_length_vector, '[]')
            bulk_lengths = self._safe_parse_list(bulk_length_vector_str)

            # è§£ææ‰¹é‡æ•°æ®åŒ…ä¼ è¾“çš„ç´¢å¼•åºåˆ—
            bulk_packet_index_str = row.get(self.cfg.data.sequence_features.bulk_packet_index, '[]')
            bulk_packet_indices = self._safe_parse_list(bulk_packet_index_str)

            # å¦‚æœæœ‰ bulk ä¿¡æ¯ï¼Œå°±ä¼šè¿”å› Noneï¼›å¦åˆ™ï¼Œä¼šæ„é€ ä¸€ä¸ªæ¶ˆæ¯åºåˆ—ï¼Œé‡Œé¢åŒ…å«å•åŒ…å’Œå¤šåŒ…æ¶ˆæ¯çš„ä¿¡æ¯
            msg_seq = self._extract_normalized_directed_msg_seq(flow_uid, 
                                                                packet_directions, 
                                                                packet_payload_sizes, 
                                                                packet_timestamps, 
                                                                bulk_lengths,
                                                                bulk_packet_indices)
            
            if msg_seq is not None:
                self._assert_directed_seq_output(msg_seq, self.max_seq_length, f"flow_uid={flow_uid} msg_seq")
                return msg_seq
            
            # å¦‚æœç½‘ç»œæµåªæœ‰é›¶è½½è·åŒ…ï¼Œæœ‰å¯èƒ½ä¼šå‡ºç°ä»¥ä¸‹æƒ…å†µï¼š
            # * è¯¥ç½‘ç»œæµåªæœ‰ SYN â†’ ACK â†’ FIN çš„æ•°æ®åŒ…æ—¶åºåºåˆ—ï¼Œæ¯”å¦‚SYNæ´ªæ°´æ”»å‡»ï¼›
            # * æˆ–è€…åœ¨å‘ç°æ‰«æç±»æ”»å‡»ï¼ˆå¤§é‡ 0 payload æ¢æµ‹ï¼‰ï¼›
            # * æˆ–è€…ï¼Œå¿ƒè·³ / keep-alive æ¨¡å¼ã€‚
            # ç”±äºé›¶è½½è·åŒ…ï¼ˆå¦‚ ACK / æ§åˆ¶åŒ…ï¼‰ä¸ä½œä¸º messageï¼Œ
            # æ‰€ä»¥åœ¨æ²¡æœ‰ bulk ä¿¡æ¯æ—¶ï¼Œéœ€è¦é€€å›åˆ°å•åŒ…åºåˆ—å¤„ç†ã€‚
            pkt_seq = self._extract_normalized_directed_pkt_seq(flow_uid, 
                                                                packet_directions, 
                                                                packet_payload_sizes, 
                                                                packet_iat_times)
            self._assert_directed_seq_output(pkt_seq, self.max_seq_length, f"flow_uid={flow_uid} pkt_seq")
            return pkt_seq
                
        except Exception as e:
            logger.warning(f"è§£æåºåˆ—æ•°æ®å¤±è´¥ï¼ˆflow_uid={flow_uid}ï¼‰: {e}")
            return {
                'directions': [0.0] * self.max_seq_length,
                'payload_sizes': [0.0] * self.max_seq_length,
                'iat_times': [0.0] * self.max_seq_length,
                'packet_numbers': [0.0] * self.max_seq_length,
                'avg_payload_sizes': [0.0] * self.max_seq_length,
                'durations': [0.0] * self.max_seq_length,
                'sequence_mask': [0] * self.max_seq_length,
                'original_length': 0
            }
            
    def _parse_direction_value(self, direction_val):
        """ç»Ÿä¸€è§£ææ–¹å‘å€¼"""
        if direction_val is None:
            return 1
            
        if isinstance(direction_val, str):
            direction_str = str(direction_val).lower().strip()
            if direction_str in ['true', '1', 'forward', 'fwd']:
                return 1
            elif direction_str in ['false', '0', 'backward', 'bwd']:
                return 0
            else:
                # å°è¯•æ•°å€¼è½¬æ¢
                try:
                    num_val = float(direction_val)
                    return 1 if num_val > 0 else 0
                except:
                    return 1  # é»˜è®¤å€¼
        
        elif isinstance(direction_val, bool):
            return 1 if direction_val else 0
        
        elif isinstance(direction_val, (int, float)):
            return 1 if direction_val > 0 else 0
        
        else:
            return 1  # é»˜è®¤å€¼

    @staticmethod
    def _safe_log_scale_normalize(value: float, eps=1e-6, scale=1.0) -> float:
        """
        å¯¹ä»¥ã€Œç§’ã€ä¸ºå•ä½çš„æ—¶é—´é—´éš”è¿›è¡Œç¨³å¥çš„ log ç¼©æ”¾ï¼Œç”¨äºæŠ‘åˆ¶è¶…é•¿æ—¶é—´é—´éš”å¯¹æ¨¡å‹è®­ç»ƒçš„ä¸ç¨³å®šå½±å“ã€‚
        """
        if abs(value) < eps:
            return 0.0
        
        sign = 1.0 if value > 0 else -1.0
        x = abs(value) + eps

        return sign * np.log1p(x / scale)

    def _assert_directed_seq_output(self, out, max_len, name):
        assert isinstance(out, dict), f"{name}: output is not dict"

        expected_keys = {
            'directions',
            'payload_sizes',
            'iat_times',
            'packet_numbers',
            'avg_payload_sizes',
            'durations',
            'sequence_mask',
            'original_length',
        }
        assert set(out.keys()) == expected_keys, \
            f"{name}: keys mismatch, got {set(out.keys())}"

        # ---------- ç±»å‹æ£€æŸ¥ ----------
        assert isinstance(out['directions'], list), \
            f"{name}: directions is not list"
        assert isinstance(out['payload_sizes'], list), \
            f"{name}: payload_sizes is not list"
        assert isinstance(out['iat_times'], list), \
            f"{name}: iat_times is not list"
        assert isinstance(out['packet_numbers'], list), \
            f"{name}: packet_numbers is not list"
        assert isinstance(out['avg_payload_sizes'], list), \
            f"{name}: avg_payload_sizes is not list"
        assert isinstance(out['durations'], list), \
            f"{name}: durations is not list"
        assert isinstance(out['sequence_mask'], list), \
            f"{name}: sequence_mask is not list"
        assert isinstance(out['original_length'], int), \
            f"{name}: original_length is not int"

        # ---------- é•¿åº¦æ£€æŸ¥ ----------
        assert len(out['directions']) == max_len, \
            f"{name}: directions length {len(out['directions'])} != {max_len}"
        assert len(out['payload_sizes']) == max_len, \
            f"{name}: payload_sizes length {len(out['payload_sizes'])} != {max_len}"
        assert len(out['iat_times']) == max_len, \
            f"{name}: iat_times length {len(out['iat_times'])} != {max_len}"
        assert len(out['packet_numbers']) == max_len, \
            f"{name}: packet_numbers length {len(out['packet_numbers'])} != {max_len}"
        assert len(out['avg_payload_sizes']) == max_len, \
            f"{name}: avg_payload_sizes length {len(out['avg_payload_sizes'])} != {max_len}"
        assert len(out['durations']) == max_len, \
            f"{name}: durations length {len(out['durations'])} != {max_len}"
        assert len(out['sequence_mask']) == max_len, \
            f"{name}: sequence_mask length {len(out['sequence_mask'])} != {max_len}"

        # ---------- å…ƒç´ ç±»å‹ä¸å–å€¼æ£€æŸ¥ ----------
        for v in out['directions']:
            assert isinstance(v, (int, float)), \
                f"{name}: directions contains non-numeric value {v}"

        for v in out['payload_sizes']:
            assert isinstance(v, (int, float)), \
                f"{name}: payload_sizes contains non-numeric value {v}"

        for v in out['iat_times']:
            assert isinstance(v, (int, float)), \
                f"{name}: iat_times contains non-numeric value {v}"
        
        for v in out['packet_numbers']:
            assert isinstance(v, (int, float)), \
                f"{name}: packet_numbers contains non-numeric value {v}"

        for v in out['avg_payload_sizes']:
            assert isinstance(v, (int, float)), \
                f"{name}: avg_payload_sizes contains non-numeric value {v}"

        for v in out['durations']:
            assert isinstance(v, (int, float)), \
                f"{name}: durations contains non-numeric value {v}"
            
        for v in out['sequence_mask']:
            assert isinstance(v, int), \
                f"{name}: sequence_mask contains non-int value {v}"
            assert v in (0, 1), \
                f"{name}: sequence_mask contains invalid value {v}"

        # ---------- è¯­ä¹‰ä¸€è‡´æ€§ ----------
        valid_len = sum(out['sequence_mask'])
        assert out['original_length'] == valid_len, \
            f"{name}: original_length {out['original_length']} != mask sum {valid_len}"
        

    def _extract_normalized_directed_pkt_seq(self, flow_uid, packet_directions, packet_payload_sizes, packet_iat_times):
        # å¯¹æ•°æ®åŒ…æ–¹å‘åšå½’ä¸€åŒ–æˆä¸º0æˆ–è€…1
        normalized_pkt_directions = [self._parse_direction_value(x) for x in packet_directions]  # 0 æˆ– 1

        # å¯¹æ•°æ®åŒ…è½½è·å¤§å°åšå½’ä¸€åŒ–
        mtu_float = float(self.mtu_normalize) # é»˜è®¤1500å­—èŠ‚ï¼Œæ²¡æœ‰ç»§ç»­é‡‡ç”¨ pkt_payload_size / mtu çš„å½’ä¸€åŒ–æ–¹å¼
        normalized_pkt_payload_sizes = [x / mtu_float for x in packet_payload_sizes]  # å½’ä¸€åŒ–åˆ°0-1
        # normalized_pkt_payload_sizes = [self._safe_log_scale_normalize(x) for x in packet_payload_sizes]  # å½’ä¸€åŒ–åˆ°0-1

        # Zeek Flowmeteræ’ä»¶é‡Œé¢packet_iat_vectorçš„æ—¶é—´å•ä½æ˜¯microsecondsï¼Œéœ€è¦logå½’ä¸€åŒ–ï¼Œå¦åˆ™æ¨¡å‹è®­ç»ƒä¼šä¸ç¨³å®šã€‚
        # å…·ä½“å‚è€ƒæ’ä»¶ä»£ç 700è¡Œå·¦å³ï¼Œå¯¹iat_vectorçš„æ“ä½œã€‚
        # https://gitee.com/seu-csqjxiao/zeek-flowmeter/blob/seu-devel/scripts/flowmeter.zeek
        #         # add the flow IAT, after converting it to microseconds, to the flow IAT vector
        #          # iat_vector[c$uid]["flow"] += |iat$flow| * 1000000.0;
        if packet_iat_times:
            # å¤„ç†æ—¶é—´é—´éš”ï¼Œæ·»åŠ ç¬¬ä¸€ä¸ªåŒ…çš„æ—¶é—´é—´éš”ä¸º0
            packet_iat_times = [0.0] + packet_iat_times  # ç¬¬ä¸€ä¸ªåŒ…çš„æ—¶é—´é—´éš”ä¸º0
            
            # ä½¿ç”¨ç»Ÿä¸€çš„_safe_log_scale_timeå‡½æ•°å¤„ç†æ—¶é—´é—´éš”
            processed_pkt_iat_times = []
            for x in packet_iat_times:
                x = x / 1000000.0  # è½¬æ¢ä¸ºç§’
                # ä¸ºé¿å…è¶…é•¿çš„æ•°æ®åŒ…æ—¶é—´é—´éš”å¯¼è‡´è®­ç»ƒä¸ç¨³å®šï¼Œè¿›è¡Œlogç¼©æ”¾
                processed_x = self._safe_log_scale_normalize(x)
                processed_pkt_iat_times.append(processed_x)
            normalized_pkt_iat_times = processed_pkt_iat_times
        else:
            normalized_pkt_iat_times = [0.0]

        # è®¡ç®—æ¯ä¸ªæ¶ˆæ¯çš„æ•°æ®åŒ…æ•°é‡åˆ—è¡¨
        pkt_packet_numbers = [1] * len(normalized_pkt_directions)

        # è®¡ç®—å¹³å‡è½½è·å¤§å°åºåˆ—
        normalized_pkt_avg_payload_sizes = normalized_pkt_payload_sizes.copy()

        # è®¡ç®—æ¯ä¸ªå•åŒ…æ¶ˆæ¯çš„æŒç»­æ—¶é—´åºåˆ—ï¼Œè¿™é‡Œç®€å•è®¾ä¸º0
        durations = [0] * len(normalized_pkt_directions)

        # -----------------------------------------------------------------------------------------
        # ç©ºåºåˆ—æ£€æµ‹ - è¿”å›ç©ºåºåˆ—è€Œä¸æ˜¯Noneã€‚æ³¨æ„å¿…é¡»åœ¨packet_iat_timesåšäº†è¡¥å‰å¯¼é›¶ä¹‹åï¼Œæ‰èƒ½æ£€æµ‹
        # -----------------------------------------------------------------------------------------
        if len(normalized_pkt_directions) == 0 or len(normalized_pkt_payload_sizes) == 0 or len(normalized_pkt_iat_times) == 0:
            logger.error(f"_extract_normalized_directed_pkt_seq()ï¼šæ£€æµ‹åˆ°ç©ºåºåˆ—æ•°æ®ï¼Œå°†è·³è¿‡æ­¤æ ·æœ¬ flow_uid={flow_uid}, "
                        f"directions={len(normalized_pkt_directions)}, payload={len(normalized_pkt_payload_sizes)}, iat={len(normalized_pkt_iat_times)}")
                        # è¿”å›å…¨é›¶çš„ç©ºåºåˆ—ï¼Œè€Œä¸æ˜¯None
            return {
                'directions': [0.0] * self.max_seq_length,
                'iat_times': [0.0] * self.max_seq_length,
                'payload_sizes': [0.0] * self.max_seq_length,
                'packet_numbers': [0.0] * self.max_seq_length,
                'avg_payload_sizes': [0.0] * self.max_seq_length,
                'durations': [0.0] * self.max_seq_length,
                'sequence_mask': [0] * self.max_seq_length,
                'original_length': 0
            }

        # ç»Ÿä¸€æˆªæ–­åˆ° max_seq_length
        pkt_seq_len = len(normalized_pkt_directions)
        assert pkt_seq_len == len(normalized_pkt_payload_sizes) == len(normalized_pkt_iat_times) == len(pkt_packet_numbers) == len(normalized_pkt_avg_payload_sizes) == len(durations), \
            f"åºåˆ—é•¿åº¦ä¸ä¸€è‡´: flow uid = {flow_uid}, directions={len(normalized_pkt_directions)}, payload={len(normalized_pkt_payload_sizes)}, iat={len(normalized_pkt_iat_times)}, packet_numbers={len(pkt_packet_numbers)}, avg_payload_sizes={len(normalized_pkt_avg_payload_sizes)}, durations={len(durations)}"

        if pkt_seq_len > self.max_seq_length:
            pkt_sequence_mask = [1] * self.max_seq_length

            # è¶…é•¿æˆªæ–­
            normalized_pkt_directions = normalized_pkt_directions[:self.max_seq_length]
            normalized_pkt_payload_sizes = normalized_pkt_payload_sizes[:self.max_seq_length]
            normalized_pkt_iat_times = normalized_pkt_iat_times[:self.max_seq_length]
            pkt_packet_numbers = pkt_packet_numbers[:self.max_seq_length]
            normalized_pkt_avg_payload_sizes = normalized_pkt_avg_payload_sizes[:self.max_seq_length]
            durations = durations[:self.max_seq_length]

        else:
            pkt_sequence_mask = [1] * pkt_seq_len

            # çŸ­åºåˆ—è¡¥é½
            if pkt_seq_len < self.max_seq_length:
                pad_len = self.max_seq_length - pkt_seq_len
                normalized_pkt_directions.extend([0.0] * pad_len)
                normalized_pkt_payload_sizes.extend([0.0] * pad_len)
                normalized_pkt_iat_times.extend([0.0] * pad_len)
                pkt_packet_numbers.extend([0.0] * pad_len)
                normalized_pkt_avg_payload_sizes.extend([0.0] * pad_len)                
                durations.extend([0.0] * pad_len)
                pkt_sequence_mask.extend([0] * pad_len)

        return {
                'directions': normalized_pkt_directions,
                'iat_times': normalized_pkt_iat_times,
                'payload_sizes': normalized_pkt_payload_sizes,
                'packet_numbers': pkt_packet_numbers,
                'avg_payload_sizes': normalized_pkt_avg_payload_sizes,
                'durations': durations,
                'sequence_mask': pkt_sequence_mask,
                'original_length': pkt_seq_len,
                }

    @staticmethod
    def build_pkt_to_bulk_idx_map(
        bulk_lengths,
        bulk_packet_indices,
    ):
        """
        æ„å»º packet_index -> bulk_idx çš„æ˜ å°„å­—å…¸ã€‚

        è¯´æ˜ï¼š
        - bulk_length_vector è®°å½•çš„æ˜¯ data_size > 0 çš„æ•°æ®åŒ…æ•°é‡ï¼Œ
        packet index åœ¨ flow ä¸­å¯èƒ½æ˜¯ä¸è¿ç»­çš„ï¼›
        - bulk_packet_indices å·²æŒ‰ bulk é¡ºåºæ‹¼æ¥ï¼Œä»…åŒ…å« data_size > 0 çš„åŒ…ï¼›
        - å› æ­¤ï¼Œé€šè¿‡ bulk_length_vector å¯¹ bulk_packet_indices é¡ºåºåˆ‡åˆ†ï¼Œ
        å¯ä»¥å‡†ç¡®æ¢å¤æ¯ä¸ª bulk å†…çš„ packet index é›†åˆã€‚
        """
        pkt_to_bulk_idx = {}

        offset = 0
        for bulk_idx, bulk_len in enumerate(bulk_lengths):
            # å–å±äºè¯¥ bulk çš„ data packetsï¼ˆä¸è¦æ±‚ index è¿ç»­ï¼‰
            bulk_pkts = bulk_packet_indices[offset : offset + bulk_len]

            for pkt_idx in bulk_pkts:
                pkt_to_bulk_idx[pkt_idx] = bulk_idx

            offset += bulk_len

        return pkt_to_bulk_idx
    
    def _extract_normalized_directed_msg_seq(self, flow_uid, 
                                             packet_directions, packet_payload_sizes, packet_timestamps, 
                                             bulk_lengths, bulk_packet_indices):
        '''
        æŠŠpacketåºåˆ—å˜æ¢æˆsingle packet æˆ–è€… bulk çš„ä¿¡æ¯åºåˆ—
        '''

        # -------------------------------
        # ç©ºåºåˆ—æ£€æµ‹ - è¿”å›ç©ºåºåˆ—è€Œä¸æ˜¯None
        # -------------------------------
        if len(packet_directions) == 0 or len(packet_payload_sizes) == 0 or len(packet_timestamps) == 0:
            logger.error(f"_extract_normalized_directed_msg_seq()ï¼šæ£€æµ‹åˆ°ç©ºåºåˆ—æ•°æ®ï¼Œå°†è·³è¿‡æ­¤æ ·æœ¬ flow_uid={flow_uid}, directions={len(packet_directions)}, "
                        f"payload={len(packet_payload_sizes)}, timestamps={len(packet_timestamps)}")
                        # è¿”å›å…¨é›¶çš„ç©ºåºåˆ—ï¼Œè€Œä¸æ˜¯None
            return {
                'directions': [0.0] * self.max_seq_length,
                'payload_sizes': [0.0] * self.max_seq_length,                
                'iat_times': [0.0] * self.max_seq_length,
                'packet_numbers': [0.0] * self.max_seq_length,
                'avg_payload_sizes': [0.0] * self.max_seq_length,
                'durations': [0.0] * self.max_seq_length,
                'sequence_mask': [0] * self.max_seq_length,
                'original_length': 0
            }

        # bulk_lengths ä¸ bulk_packet_indices é•¿åº¦ä¸€è‡´
        if sum(bulk_lengths) != len(bulk_packet_indices):
            logger.warning(f"bulk_lengths ä¸ bulk_packet_indices é•¿åº¦ä¸ä¸€è‡´ï¼Œflow_uid={flow_uid}. "
                           f"å°†å¿½ç•¥ bulk ä¿¡æ¯ï¼ŒæŒ‰å•åŒ…å¤„ç†ã€‚")
            return None

        pkt_to_bulk_idx = self.build_pkt_to_bulk_idx_map(bulk_lengths, bulk_packet_indices)

        msg_directions = [] 
        msg_payload_sizes = []
        msg_iat_times = []
        msg_packet_numbers = []
        msg_avg_payload_sizes = []
        msg_durations = []

        prev_msg_timestamp = 0
        current_bulk_direction = None
        current_bulk_bytes = []
        current_bulk_timestamps = []
        current_bulk_idx = -1
        
        for pkt_idx in range(len(packet_payload_sizes)):
            pkt_direction = packet_directions[pkt_idx] if pkt_idx < len(packet_directions) else 0
            pkt_payload_size = packet_payload_sizes[pkt_idx] if pkt_idx < len(packet_payload_sizes) else 0            
            pkt_timestamp = packet_timestamps[pkt_idx] if pkt_idx < len(packet_timestamps) else 0
            
            # if pkt_payload_size == 0:
            #     # è¿™é‡Œå¯ä»¥è€ƒè™‘å¿½ç•¥é›¶payload bytesçš„è½½è·åŒ…ï¼Œå› ä¸ºï¼š
            #     # é›¶è½½è·åŒ…ï¼ˆå¦‚ ACK / æ§åˆ¶åŒ…ï¼‰ä¸ä½œä¸º messageï¼Œ
            #     # é¿å…é«˜é¢‘æ§åˆ¶åŒ…å¹²æ‰° message-level è¡Œä¸ºå»ºæ¨¡ã€‚
            #     # ä¹Ÿå¯ä»¥é€‰æ‹©å°†å…¶ä½œä¸ºå•åŒ…æ¶ˆæ¯å¤„ç†ã€‚
            #     continue

            if pkt_payload_size == 0:
                # Flowmeteræ’ä»¶ä¸­ï¼Œé›¶è½½è·åŒ…ä¸å‚ä¸ bulk ä¼ è¾“çš„åˆ’åˆ†
                # å› æ­¤ï¼Œç›´æ¥å°†å…¶è§†ä¸ºä¸å±äºä»»ä½• bulkï¼Œä½†ä½œä¸ºå•åŒ…æ¶ˆæ¯å¤„ç†
                bulk_idx_of_current_pkt = None  
            else:
                bulk_idx_of_current_pkt = pkt_to_bulk_idx.get(pkt_idx, None)

            if bulk_idx_of_current_pkt is None or bulk_idx_of_current_pkt != current_bulk_idx:
                # å¦‚æœå½“å‰åŒ…ä¸å±äºä»»ä½• bulk æˆ–è€…å…¶æ‰€å±äºçš„bulk_idxä¸åŒäºå½“å‰bulk_idxï¼Œé‚£ä¹ˆå…ˆç»“æŸå½“å‰çš„ bulkï¼ˆå¦‚æœæœ‰ï¼‰
                if current_bulk_idx >= 0:
                    # bulk å‘ç”Ÿåˆ‡æ¢ï¼ˆæˆ–è¿›å…¥ / ç¦»å¼€ bulkï¼‰
                    # åˆ›å»ºä¸€ä¸ª multiple-packet message entry
                    prev_msg_timestamp = self._create_multiple_pkt_msg_return_msg_timestamp(
                                            current_bulk_direction, current_bulk_bytes, current_bulk_timestamps, prev_msg_timestamp, 
                                            msg_directions, msg_payload_sizes, msg_iat_times, msg_packet_numbers, msg_avg_payload_sizes, msg_durations)
                    # é€šè¿‡é‡ç½® current bulk infoï¼Œç»“æŸå½“å‰çš„ bulk
                    current_bulk_direction = None
                    current_bulk_bytes = []
                    current_bulk_timestamps = []
                    current_bulk_idx = -1

            if bulk_idx_of_current_pkt is None: 
                # Zeek Flowmeteræ’ä»¶åˆ›å»ºä¸€ä¸ªBulkæ—¶ï¼Œè¦æ±‚bulkå†…è‡³å°‘åŒ…å«5ä¸ªæ•°æ®åŒ…ã€‚æŸ¥çœ‹https://gitee.com/seu-csqjxiao/zeek-flowmeter
                # FlowMeter::bulk_min_length: The minimal number of data packets which have to be in 
                #                             a bulk transmission for it to be considered a bulk transmission. 
                #                             The default value is 5 packets.
                # FlowMeter::bulk_timeout: The maximal allowed inter-arrival time between two data packets 
                #                          so they are considered to be part of the same bulk transmission. 
                #                          The default value is 1s.
                # å› æ­¤ï¼Œå¦‚æœå½“å‰åŒ…ä¸å±äºä»»ä½• bulkï¼Œåˆ™åˆ›å»ºå•åŒ…æ¶ˆæ¯ã€‚
                prev_msg_timestamp = self._create_single_pkt_msg_return_msg_timestamp(
                                        pkt_direction, pkt_timestamp, pkt_payload_size, prev_msg_timestamp, 
                                        msg_directions, msg_payload_sizes, msg_iat_times, msg_packet_numbers, msg_avg_payload_sizes, msg_durations)
            else:
                if current_bulk_idx == -1:
                    # å¼€å§‹ä¸€ä¸ªæ–°çš„ bulkï¼Œå…¶æ–¹å‘ç”±ç¬¬ä¸€ä¸ªåŒ…å†³å®šï¼Œè¯¥bulkåç»­çš„åŒ…å¿…é¡»æ–¹å‘ä¸€è‡´
                    current_bulk_idx = bulk_idx_of_current_pkt
                    current_bulk_direction = pkt_direction

                assert pkt_direction == current_bulk_direction, \
                    f"æ£€æµ‹åˆ° bulk å†…æ–¹å‘ä¸ä¸€è‡´ï¼Œflow_uid={flow_uid}, bulk_idx={current_bulk_idx}"
                current_bulk_bytes.append(pkt_payload_size)
                current_bulk_timestamps.append(pkt_timestamp)

        
        if current_bulk_direction is not None:
            # å·²ç»æ‰«æå®Œæ•°æ®åŒ…åºåˆ—ã€‚å¦‚æœå½“å‰bulkè¿˜æ²¡æœ‰ç»“æŸï¼Œåˆ›å»ºä¸€ä¸ª multiple-packet message
            prev_msg_timestamp = self._create_multiple_pkt_msg_return_msg_timestamp(
                                        current_bulk_direction, current_bulk_bytes, current_bulk_timestamps, prev_msg_timestamp, 
                                        msg_directions, msg_payload_sizes, msg_iat_times, msg_packet_numbers, msg_avg_payload_sizes, msg_durations)
            # é€šè¿‡é‡ç½® current bulk infoï¼Œç»“æŸå½“å‰çš„ bulk
            current_bulk_direction = None
            current_bulk_bytes = []
            current_bulk_timestamps = []
            current_bulk_idx = -1

        # -------------------------------
        # ç»Ÿä¸€æˆªæ–­åˆ° max_seq_length
        # -------------------------------
        msg_seq_len = len(msg_payload_sizes)
        # message çš„ original_length å¯èƒ½ä¸º 0ï¼Œå¦‚æœï¼š
        # å…¨æ˜¯ payload_size == 0 çš„åŒ…ï¼Œæˆ–è€… bulk è¢«å¿½ç•¥
        # è¿™ç§æƒ…å†µä¸‹ï¼Œè¿”å› Noneï¼Œè¡¨ç¤ºæ²¡æœ‰æœ‰æ•ˆçš„æ¶ˆæ¯åºåˆ—ï¼Œç›´æ¥å›é€€ packet-level sequence å¤„ç†
        if msg_seq_len == 0:
            return None 

        msg_sequence_mask = [1] * msg_seq_len
        if msg_seq_len > self.max_seq_length:
            msg_seq_len = self.max_seq_length

            msg_directions = msg_directions[:self.max_seq_length]
            msg_payload_sizes = msg_payload_sizes[:self.max_seq_length]
            msg_iat_times = msg_iat_times[:self.max_seq_length]
            msg_packet_numbers = msg_packet_numbers[:self.max_seq_length]
            msg_avg_payload_sizes = msg_avg_payload_sizes[:self.max_seq_length]
            msg_durations = msg_durations[:self.max_seq_length]
            msg_sequence_mask = [1] * self.max_seq_length

        else:
            pad_len = self.max_seq_length - msg_seq_len
            if pad_len > 0:
                msg_directions.extend([0.0] * pad_len)
                msg_payload_sizes.extend([0.0] * pad_len)      
                msg_iat_times.extend([0.0] * pad_len)
                msg_packet_numbers.extend([0.0] * pad_len)
                msg_avg_payload_sizes.extend([0.0] * pad_len)
                msg_durations.extend([0.0] * pad_len)
                msg_sequence_mask.extend([0] * pad_len)

        return {
                'directions': msg_directions,
                'payload_sizes': msg_payload_sizes,                
                'iat_times': msg_iat_times,
                'packet_numbers': msg_packet_numbers,
                'avg_payload_sizes': msg_avg_payload_sizes,
                'durations': msg_durations,
                'sequence_mask': msg_sequence_mask,
                'original_length': msg_seq_len,
                }

    def _create_single_pkt_msg_return_msg_timestamp(self, pkt_direction, pkt_timestamp, pkt_payload_size, prev_msg_timestamp, 
                                                    msg_directions, msg_payload_sizes, msg_iat_times, msg_packet_numbers, msg_avg_payload_sizes, msg_durations):
        msg_direction_value = self._parse_direction_value(pkt_direction)
        msg_directions.append(msg_direction_value)

        msg_payload_size = pkt_payload_size / float(self.mtu_normalize)  # å½’ä¸€åŒ–åˆ°0-1
        # msg_payload_size = self._safe_log_scale_normalize(pkt_payload_size)
        msg_payload_sizes.append(msg_payload_size)

        # Zeek Flowmeteræ’ä»¶é‡Œé¢ï¼Œpkt_timestampçš„æ—¶é—´å•ä½æ˜¯ç§’ï¼Œæ— éœ€è½¬æ¢ã€‚
        # packet_timestamp_vector ä»¥Unixæ—¶é—´æˆ³æ ¼å¼è®°å½•flowä¸­å„æ•°æ®åŒ…çš„åˆ°è¾¾æ—¶é—´ã€‚
        # æŸ¥çœ‹ https://gitee.com/seu-csqjxiao/zeek-flowmeter/tree/seu-devel
        if prev_msg_timestamp == 0:
            # ç¬¬ä¸€ä¸ªæ¶ˆæ¯ï¼Œæ—¶é—´é—´éš”ä¸º0
            msg_iat_time = 0.0
        else:
            # message åˆ°è¾¾æ—¶é—´å®šä¹‰ä¸ºè¯¥æ¶ˆæ¯ä¸­ç¬¬ä¸€ä¸ª packet çš„æ—¶é—´æˆ³
            msg_iat_time = pkt_timestamp - prev_msg_timestamp
            msg_iat_time = self._safe_log_scale_normalize(msg_iat_time)
        
        msg_iat_times.append(msg_iat_time)

        msg_packet_numbers.append(1)  # å•åŒ…æ¶ˆæ¯ï¼ŒåŒ…æ•°ä¸º1

        msg_avg_payload_sizes.append(msg_payload_size)  # å•åŒ…æ¶ˆæ¯ï¼Œå¹³å‡è½½è·å¤§å°ç­‰äºè½½è·å¤§å°

        msg_durations.append(0)  # å•åŒ…æ¶ˆæ¯ï¼ŒæŒç»­æ—¶é—´ä¸º0ç§’

        return pkt_timestamp
        
    def _create_multiple_pkt_msg_return_msg_timestamp(self, current_bulk_direction, current_bulk_bytes, current_bulk_timestamps, prev_msg_timestamp, 
                                                      msg_directions, msg_payload_sizes, msg_iat_times, msg_packet_numbers, msg_avg_payload_sizes, msg_durations):
        msg_direction_value = self._parse_direction_value(current_bulk_direction)
        msg_directions.append(msg_direction_value)

        msg_payload_size = sum(current_bulk_bytes) / float(self.mtu_normalize)  # å½’ä¸€åŒ–åˆ°0-1
        # msg_payload_size = self._safe_log_scale_normalize(sum(current_bulk_bytes))
        msg_payload_sizes.append(msg_payload_size)

        # Zeek Flowmeteræ’ä»¶é‡Œé¢ï¼Œpkt_timestampçš„æ—¶é—´å•ä½æ˜¯ç§’ï¼Œæ— éœ€è½¬æ¢ã€‚
        # packet_timestamp_vector ä»¥Unixæ—¶é—´æˆ³æ ¼å¼è®°å½•flowä¸­å„æ•°æ®åŒ…çš„åˆ°è¾¾æ—¶é—´ã€‚
        # æŸ¥çœ‹ https://gitee.com/seu-csqjxiao/zeek-flowmeter/tree/seu-devel
        if prev_msg_timestamp == 0:
            # ç¬¬ä¸€ä¸ªæ¶ˆæ¯ï¼Œæ—¶é—´é—´éš”ä¸º0
            msg_iat_time = 0.0
        else:
            # message çš„åˆ°è¾¾æ—¶é—´å®šä¹‰ä¸º bulk ä¸­ç¬¬ä¸€ä¸ªæ•°æ®åŒ…çš„æ—¶é—´æˆ³
            msg_iat_time = current_bulk_timestamps[0] - prev_msg_timestamp
            msg_iat_time = self._safe_log_scale_normalize(msg_iat_time)
        
        msg_iat_times.append(msg_iat_time)

        msg_packet_numbers.append(len(current_bulk_bytes))  # å¤šåŒ…æ¶ˆæ¯ï¼ŒåŒ…æ•°ä¸º bulk å†…åŒ…æ•°

        # å¤šåŒ…æ¶ˆæ¯ï¼Œå¹³å‡è½½è·å¤§å°
        avg_payload_size = sum(current_bulk_bytes) / len(current_bulk_bytes) / float(self.mtu_normalize)
        # avg_payload_size = self._safe_log_scale_normalize(sum(current_bulk_bytes) / len(current_bulk_bytes))
        msg_avg_payload_sizes.append(avg_payload_size)  

        # å¤šåŒ…æ¶ˆæ¯ï¼ŒæŒç»­æ—¶é—´è®¾å®šä¸º bulk å†…æœ€åä¸€ä¸ªåŒ…çš„æ—¶é—´æˆ³ - ç¬¬ä¸€ä¸ªåŒ…çš„æ—¶é—´æˆ³ï¼Œæ—¶é—´å•ä½æ˜¯ç§’
        msg_duration = current_bulk_timestamps[-1] - current_bulk_timestamps[0]
        msg_durations.append(msg_duration)

        # è¿”å›è¯¥æ¶ˆæ¯çš„æ—¶é—´æˆ³ï¼ˆå³ bulk ä¸­æœ€åä¸€ä¸ªåŒ…çš„æ—¶é—´æˆ³ï¼‰
        return current_bulk_timestamps[-1] 

    def _process_categorical_features(self):
        """
        å¤„ç†ç±»åˆ«å‹ç‰¹å¾ï¼ˆLabel Encoding â†’ Embedding è¾“å…¥ï¼‰ï¼š

        æ¯ä¸€ä¸ªç±»åˆ«å­—æ®µï¼ˆcategorical columnï¼‰éƒ½ä¼šç»å†ä»¥ä¸‹å¤„ç†æµç¨‹ï¼š

        ------------------------------------------------------------
        â‘  è‹¥åˆ—å­˜åœ¨ï¼š
            - å°†è¯¥åˆ—çš„ç±»åˆ«å€¼æ˜ å°„ä¸ºæ•´æ•° IDï¼ˆLabel Encodingï¼‰
            - ID ä» 1 å¼€å§‹ï¼Œ0 ç”¨ä½œ OOVï¼ˆOut-Of-Vocabularyï¼‰
            - æŒ‰é¢‘ç‡æ’åºï¼Œä»…ä¿ç•™ Top-K ç±»åˆ«ï¼Œå‡å°‘ç¨€ç–æ€§
            - å°†æ˜ å°„åçš„æ•´æ•°åŠ å…¥ categorical_dataï¼ˆä¸€ä¸ªåˆ—è¡¨ï¼‰

        â‘¡ è‹¥åˆ—ç¼ºå¤±ï¼ˆä¾‹å¦‚ flow_df ä¸­æ²¡æœ‰è¿™ä¸ªåˆ—ï¼‰ï¼š
            - æ‰“å° warning
            - è·³è¿‡è¯¥åˆ—ï¼ˆä¸ä¼šåŠ å…¥ categorical_dataï¼‰
            - è¿™æ„å‘³ç€ **è¯¥åˆ—ä¸ä¼šå‡ºç°åœ¨æœ€ç»ˆ categorical_features çŸ©é˜µä¸­**

        âš ï¸ å› æ­¤ï¼š
            categorical_features çš„æœ€ç»ˆå½¢çŠ¶ä¸ºï¼š
                [num_flows, num_effective_categorical_columns]

        å…¶ä¸­ï¼š
            num_effective_categorical_columns â‰¤ len(self.categorical_columns)

        ï¼ˆåªç»Ÿè®¡å®é™…å­˜åœ¨ã€æˆåŠŸå¤„ç†çš„åˆ—ï¼‰

        è¿™æ˜¯é¢„æœŸè¡Œä¸ºï¼šå¦‚æœæŸäº›ç‰¹å¾åœ¨å®é™…æ•°æ®é›†ä¸­ä¸å­˜åœ¨ï¼Œå°±ä¸ä¼šå¼ºåˆ¶åŠ å…¥ 0 åˆ—ã€‚
        è¿™æ ·å¯ä»¥ä¿æŒ embedding çš„ç»´åº¦ä¸æœ‰æ•ˆæ•°æ®ä¸€è‡´ï¼Œé¿å…å…¨ 0 å™ªå£°åˆ—ã€‚

        ------------------------------------------------------------
        æœ€ç»ˆè¾“å‡ºï¼š
            self.categorical_val2idx_mappings: dict
                { column_name â†’ { category_value â†’ index_id } }

            self.categorical_features: LongTensor of shape
                [N, C]
                N = æµæ•°é‡ï¼ˆflow countï¼‰
                C = å®é™…å­˜åœ¨çš„ç±»åˆ«ç‰¹å¾åˆ—æ•°

            æ¯åˆ—å­˜å‚¨ä¸€ä¸ªæ•´æ•°ç±»åˆ« IDï¼ˆä» 0 å¼€å§‹ï¼Œ0 ä¸º OOVï¼‰ï¼Œ
            ä¹‹åä¼šé€å…¥ nn.Embedding ä½œä¸ºè¾“å…¥ã€‚
        """
        
        use_train_mappings = hasattr(self, 'use_train_mappings') and self.use_train_mappings
        if use_train_mappings:
            logger.info("ä½¿ç”¨è®­ç»ƒé›†çš„ç±»åˆ«æ˜ å°„å’Œæœ‰æ•ˆç±»åˆ«åˆ—åï¼Œè·³è¿‡ç‹¬ç«‹æ˜ å°„è®¡ç®—")
            self.categorical_columns_effective = self.train_categorical_columns_effective
        else:
            self.categorical_val2idx_mappings = {}      # æ¯åˆ—çš„ {ç±»åˆ«å€¼ â†’ index} æ˜ å°„
            self.categorical_columns_effective = [] # è·å¾—å­˜åœ¨çš„æœ‰æ•ˆcategorical_columnsçš„åºåˆ—ï¼Œç¡®ä¿æ˜¯æ•°æ®å±‚å’Œæ¨¡å‹å±‚çš„ç±»åˆ«åˆ—åé¡ºåºå®Œå…¨ä¸€è‡´
                    
        categorical_data = []               # å­˜å‚¨æ¯åˆ—çš„æ•´æ•° ID åºåˆ—
        
        # é’ˆå¯¹ä¸åŒå­—æ®µçš„ç±»åˆ«æ•°é‡ä¸Šé™ï¼ˆä¿æŒä¸ config çš„å­—æ®µç»Ÿè®¡ä¸€è‡´ï¼‰
        TOP_K_MAP = {
            "ssl_cipher": 50,       # cipher éå¸¸å¤šï¼Œé™åˆ¶ä¸º top 50
            "conn_history": 20,     # ä¼šè¯çŠ¶æ€åºåˆ— ShADd éå¸¸ç¨€ç–
            "ssl_version": 4,       # TLS1.0~1.3
            "ssl_curve": 5,
            "ssl_next_protocol": 10,
            "dns_qtype": 30,
            "dns_rcode_name": 20,
        }
        default_top_k = 20
        
        for col in self.categorical_columns:
             # â­åç§°å¯¹é½ï¼Œå°†åˆ—åä¸­éæ³•çš„"."å­—ç¬¦æ›¿æ¢æˆ"_"ï¼Œå¦åˆ™åç»­æ¨¡å‹ä¼šæŠ¥é”™ã€‚
            clean_col = col.replace(".", "_") 

            # â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
            # 0. åˆ—ä¸å­˜åœ¨ â†’ è·³è¿‡ã€‚æ³¨æ„ï¼š
            # categorical_features çš„åˆ—æ•° = å®é™…å­˜åœ¨çš„ç±»åˆ«åˆ—æ•°é‡ã€‚
            # è‹¥æŸåˆ—åœ¨ flow_df ä¸­ç¼ºå¤±ï¼Œåˆ™è·³è¿‡è¯¥åˆ—ï¼Œä¸ä¼šå¼ºåˆ¶åŠ å…¥ç©ºåˆ—ã€‚
            # â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
            if col not in self.flow_df.columns:
                logger.warning(f"ç±»åˆ«ç‰¹å¾ç¼ºå¤±åˆ— {col}ï¼Œè·³è¿‡")
                continue

            # â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
            # 1. å¡«è¡¥ç©ºå€¼å¹¶è½¬stringï¼ˆç»Ÿä¸€å½¢å¼ï¼‰
            # â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
            series = (
                self.flow_df[col]   # è¯»å–æ—¶ä¾æ—§ç”¨åŸå§‹ col
                .fillna("OOV")      # æ‰€æœ‰ç¼ºå¤±å€¼å½’å…¥â€œæœªçŸ¥ç±»åˆ«â€
                .astype(str)
                .str.strip()
            )

            # â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
            # 2. ç»Ÿè®¡é¢‘ç‡ â†’ é€‰æ‹© Top-K ç±»åˆ«
            #    è¿™æ˜¯ Embedding å¤„ç†é«˜åŸºæ•°ç±»åˆ«çš„é€šç”¨æ–¹å¼
            # â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
            counter = Counter(series)

            # æœ€å¸¸è§çš„å‰ K ä¸ªç±»åˆ«
            most_common = counter.most_common(TOP_K_MAP.get(clean_col, default_top_k))

            # ä»…ä¿ç•™ç±»åˆ«åç§°
            keep_values = [v for v, _ in most_common]
            # ç¡®ä¿è‡³å°‘ä¿ç•™ 1 ä¸ªçœŸå®åˆ†ç±»
            if len(keep_values) == 0:
                keep_values = ["OOV"]
                
            # â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
            # 3. å»ºç«‹ç±»åˆ« â†’ index æ˜ å°„
            #    index ä» 1 å¼€å§‹ï¼ˆ0 ç”¨ä½œ OOVï¼‰
            # â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”            
            if use_train_mappings:
                mapping = self.categorical_val2idx_mappings[clean_col] # ç›´æ¥æŸ¥è¯¢clean_colçš„val2idx
            else:
                mapping = {v: (i + 1) for i, v in enumerate(keep_values)} # å»ºç«‹æ˜ å°„
                mapping["OOV"] = 0
                self.categorical_val2idx_mappings[clean_col] = mapping

            # â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
            # 4. å°†æ¯ä¸ªå€¼æ˜ å°„ä¸ºæ•´æ•° ID
            # â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
            mapped = [mapping.get(v, 0) for v in series]
            categorical_data.append(mapped)
            
            if not use_train_mappings:
                self.categorical_columns_effective.append(clean_col)
            
        # â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
        # 5. å°†æ‰€æœ‰åˆ—å †å æˆçŸ©é˜µï¼šN Ã— C
        #    N = flow æ•°é‡
        #    C = ç±»åˆ«å‹ç‰¹å¾åˆ—æ•°é‡
        # â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
        if categorical_data:
            # zip(*list_of_lists) = æŒ‰åˆ—æ‹¼ä¸ºæŒ‰è¡Œ
            self.categorical_features = torch.tensor(
                list(zip(*categorical_data)), dtype=torch.long
            )
        else:
            # æ²¡æœ‰ç±»åˆ«å‹ç‰¹å¾
            self.categorical_features = torch.zeros(
                (len(self.flow_df), 0), dtype=torch.long
            )

        # å°†å…¨å±€categorical_columnsçš„åˆ—åæ¸…æ´—æ ‡å‡†åŒ–ï¼Œå°†åˆ—åä¸­éæ³•çš„"."å­—ç¬¦æ›¿æ¢æˆ"_"ï¼Œå¦åˆ™åç»­æ¨¡å‹ä¼šæŠ¥é”™ã€‚
        # self.categorical_columns = [col.replace(".", "_") for col in self.categorical_columns] # æœ€å¥½æ˜¯åˆ«åšè¿™ç§æ›¿æ¢ï¼Œå¯¼è‡´è¿™ä¸ªå‡½æ•°æ²¡æ³•é‡å…¥
        logger.info(f"[Dataset] æ‰€æœ‰ç±»åˆ«åˆ—ï¼ˆ{len(self.categorical_columns)}åˆ—ï¼‰ï¼š{self.categorical_columns}")
        logger.info(f"[Dataset] åˆ—åæ¸…æ´—åçš„æœ‰æ•ˆç±»åˆ«åˆ—ï¼ˆ{len(self.categorical_columns_effective)}åˆ—ï¼‰: {self.categorical_columns_effective}")
        # æ ¡éªŒç±»åˆ«ç‰¹å¾åˆ—æ•°æ˜¯å¦ä¸€è‡´
        if self.categorical_features.numel() > 0:  # é¿å…ç©º tensor æƒ…å†µ
            matrix_num_cols = self.categorical_features.shape[1]
        else:
            matrix_num_cols = 0

        if len(self.categorical_columns_effective) != matrix_num_cols:
            logger.warning(
                f"[Dataset] âš ç±»åˆ«ç‰¹å¾åˆ—æ•°ä¸åŒ¹é…ï¼šæœ‰æ•ˆåˆ— {len(self.categorical_columns_effective)} "
                f"!= çŸ©é˜µåˆ—æ•° {matrix_num_cols}ï¼Œè¯·æ£€æŸ¥ categorical_features æ„é€ é€»è¾‘"
            )
        
    def _process_text_features(self):
        """å¤„ç†æ–‡æœ¬ç‰¹å¾"""
        self.text_features = []
        
        text_columns = [
            self.cfg.data.text_features.dns_query,
            self.cfg.data.text_features.dns_answers,
            self.cfg.data.text_features.ssl_server_name,
            self.cfg.data.text_features.cert0_subject,
            self.cfg.data.text_features.cert0_issuer, 
            self.cfg.data.text_features.cert0_san_dns,
            self.cfg.data.text_features.cert1_subject,
            self.cfg.data.text_features.cert1_issuer,
            self.cfg.data.text_features.cert1_san_dns,
            self.cfg.data.text_features.cert2_subject,
            self.cfg.data.text_features.cert2_issuer,
            self.cfg.data.text_features.cert2_san_dns,
        ]
        
        def safe_to_str(v):
            """æ›´å®‰å…¨çš„å­—ç¬¦ä¸²è½¬æ¢å‡½æ•°"""
            if v is None or pd.isna(v):
                return ""
            
            if isinstance(v, list):
                # é€’å½’å¤„ç†åˆ—è¡¨ä¸­çš„æ¯ä¸ªå…ƒç´ 
                return " ".join([safe_to_str(x) for x in v if x is not None and not pd.isna(x)])

            if isinstance(v, dict):
                # å¤„ç†å­—å…¸ç»“æ„
                parts = []
                for k, val in v.items():
                    if val is not None and not pd.isna(val):
                        parts.append(f"{k}_{safe_to_str(val)}")
                return " ".join(parts) if parts else ""

            if isinstance(v, str):
                # å¯¹äºå­—ç¬¦ä¸²ï¼Œç›´æ¥è¿”å›ï¼ˆä¸å†å°è¯•è§£æï¼‰
                return v.strip()

            # å…¶ä»–ç±»å‹ç›´æ¥è½¬æ¢ä¸ºå­—ç¬¦ä¸²
            return str(v) if v is not None else ""   
        
        # æ·»åŠ è¯¦ç»†çš„è¿›åº¦æ¡
        with tqdm(total=len(self.flow_df), desc="å¤„ç†æ–‡æœ¬ç‰¹å¾", position=0, leave=True) as pbar:
            for idx, row in self.flow_df.iterrows():
                text_data = {}

                for col in text_columns:
                    if col in row.index:
                        value = row[col]
                        text_data[col] = safe_to_str(value) if pd.notna(value) else ""
                    else:
                        text_data[col] = ""

                self.text_features.append(text_data)
                pbar.update(1)
    
    def _process_numeric_features(self):
        """å¤„ç†æ•°å€¼ç‰¹å¾ï¼Œåªè´Ÿè´£ï¼š
        (1) æ£€æŸ¥æ•°å€¼åˆ—
        (2) è®­ç»ƒé›†ç»Ÿè®¡ numeric_stats
        (3) è°ƒç”¨ apply_numeric_stats() åšå½’ä¸€åŒ–
        """        
        # åªä½¿ç”¨ flow_featuresï¼Œæ’é™¤ domain_name_embedding_features
        flow_columns = self.cfg.data.tabular_features.numeric_features.flow_features
        x509_columns = self.cfg.data.tabular_features.numeric_features.x509_features
        dns_columns = self.cfg.data.tabular_features.numeric_features.dns_features

        numeric_columns = flow_columns + x509_columns + dns_columns

        # æ·»åŠ åˆ—å­˜åœ¨æ€§æ£€æŸ¥
        available_numeric_columns = [col for col in numeric_columns if col in self.flow_df.columns]
        
        # ---------- Step 1: ä¿è¯åˆ—ä¸º float ----------
        for col in available_numeric_columns:
            if self.flow_df[col].dtype == 'object':
                logger.warning(f"æ•°å€¼åˆ— '{col}' æ˜¯å¯¹è±¡ç±»å‹ï¼Œæ­£åœ¨è½¬æ¢ä¸ºæ•°å€¼ç±»å‹...")
                # å¼ºåˆ¶è½¬æ¢ä¸ºæ•°å€¼ç±»å‹ï¼Œæ— æ³•è½¬æ¢çš„è®¾ä¸ºNaN
                self.flow_df[col] = pd.to_numeric(self.flow_df[col], errors='coerce')
                
                # æ£€æŸ¥è½¬æ¢åçš„NaNæ¯”ä¾‹
                nan_ratio = self.flow_df[col].isna().mean()
                if nan_ratio > 0.5:
                    logger.warning(f"åˆ— '{col}' æœ‰ {nan_ratio:.1%} çš„å€¼æ— æ³•è½¬æ¢ä¸ºæ•°å€¼")

        # ---------- Step 2: è®­ç»ƒé˜¶æ®µè®¡ç®— numeric_stats ----------
        if self.is_training:
            self.numeric_stats = {}
            
            for col in available_numeric_columns:
                # è¿‡æ»¤æ‰å¼‚å¸¸å€¼å’ŒNaN
                col_data = self.flow_df[col].dropna()
                
                if not col_data.empty:
                    if col_data.dtype.kind in 'iufc':  # æ•´æ•°ã€æ— ç¬¦å·æ•´æ•°ã€æµ®ç‚¹æ•°ã€å¤æ•°
                        # ç§»é™¤æç«¯å¼‚å¸¸å€¼
                        q1 = col_data.quantile(0.25)
                        q3 = col_data.quantile(0.75)
                        iqr = q3 - q1
                        lower_bound = q1 - 1.5 * iqr
                        upper_bound = q3 + 1.5 * iqr
                        
                        filtered_data = col_data[(col_data >= lower_bound) & (col_data <= upper_bound)]
                        
                        if not filtered_data.empty:
                            self.numeric_stats[col] = {
                                'mean': filtered_data.mean(),
                                'std': max(filtered_data.std(), 1e-6)
                            }
                        else:
                            self.numeric_stats[col] = {
                                'mean': col_data.mean() if not col_data.empty else 0,
                                'std': max(col_data.std() if not col_data.empty else 1, 1e-6)
                            }
                    else:
                        logger.warning(f"åˆ— '{col}' ä¸æ˜¯æ•°å€¼ç±»å‹ï¼Œä½¿ç”¨é»˜è®¤ç»Ÿè®¡ä¿¡æ¯")
                        self.numeric_stats[col] = {'mean': 0, 'std': 1}
                else:
                    logger.warning(f"åˆ— '{col}' æ²¡æœ‰æœ‰æ•ˆæ•°å€¼æ•°æ®ï¼Œä½¿ç”¨é»˜è®¤ç»Ÿè®¡ä¿¡æ¯")
                    self.numeric_stats[col] = {'mean': 0, 'std': 1}
                    
            # è®­ç»ƒé›†ç«‹å³å½’ä¸€åŒ–
            self.apply_numeric_stats()
                
        else:
            # éªŒè¯/æµ‹è¯•ï¼šç­‰å¾…æ³¨å…¥è®­ç»ƒç»Ÿè®¡
            self.numeric_stats = None
            self.numeric_features = None # åé¢ä¼šä»available_numeric_columnsçœŸæ­£è¯»å–æ•°æ®å†…å®¹åˆ°self.numeric_featuresï¼Œç°åœ¨å…ˆç½®ç©º
            logger.info("éªŒè¯é›†/æµ‹è¯•é›†å°†ç­‰å¾…è®­ç»ƒé›†ç»Ÿè®¡ä¿¡æ¯æ³¨å…¥åå†å½’ä¸€åŒ–ã€‚")
        
    def apply_numeric_stats(self):
        """æ ¹æ®è®­ç»ƒé›† numeric_stats å¯¹ numeric_features åšå½’ä¸€åŒ–ï¼ˆå¸¦è¿›åº¦æ¡ï¼‰"""

        if not hasattr(self, "numeric_stats"):
            raise RuntimeError(
                "numeric_stats æœªè®¾ç½®ï¼è¯·å…ˆä»è®­ç»ƒé›†æ³¨å…¥ç»Ÿè®¡ä¿¡æ¯ã€‚"
            )
            
        # ---------- è‹¥ numeric_stats å…¨é»˜è®¤ {0,1}ï¼Œåˆ™è·³è¿‡ ----------
        if self.numeric_stats is not None:
            all_default = all(
                stats.get('mean',0)==0 and stats.get('std',1)==1
                for stats in self.numeric_stats.values()
            )
            if all_default:
                logger.info("æ•°å€¼ç‰¹å¾ä½¿ç”¨é»˜è®¤ {0,1} ç»Ÿè®¡ä¿¡æ¯ï¼Œè·³è¿‡å½’ä¸€åŒ–ã€‚")
                # ç›´æ¥ä½¿ç”¨åŸå§‹ç‰¹å¾ï¼Œä¸åšä»»ä½•å˜æ¢
                all_numeric_columns = (
                    self.cfg.data.tabular_features.numeric_features.flow_features +
                    self.cfg.data.tabular_features.numeric_features.x509_features +
                    self.cfg.data.tabular_features.numeric_features.dns_features
                )
                self.numeric_features = self.flow_df[all_numeric_columns].fillna(0.0).values.tolist()
                return
            
        # åªä½¿ç”¨ numeric_featuresï¼Œæ’é™¤ domain_name_embedding_features
        all_numeric_columns = (
            self.cfg.data.tabular_features.numeric_features.flow_features +
            self.cfg.data.tabular_features.numeric_features.x509_features +
            self.cfg.data.tabular_features.numeric_features.dns_features
        )
        # åˆ—å­˜åœ¨æ€§æ£€æŸ¥
        available_numeric_columns = [col for col in all_numeric_columns if col in self.flow_df.columns]
        
        new_numeric_features = []

        with tqdm(total=len(self.flow_df), desc="åº”ç”¨æ•°å€¼ç‰¹å¾å½’ä¸€åŒ–", position=0, leave=True) as pbar:
            for idx, row in self.flow_df.iterrows():
                numeric_row = []

                for col in available_numeric_columns:
                    value = row[col]

                    # è·å–è®­ç»ƒæ—¶ç»Ÿè®¡ä¿¡æ¯
                    stats = self.numeric_stats.get(col, {'mean': 0, 'std': 1})
                    mean = stats['mean']
                    std = max(stats['std'], 1e-6)

                    # è½¬æ¢ + å½’ä¸€åŒ–
                    if pd.isna(value):
                        normalized = 0.0
                    else:
                        try:
                            v = float(value)
                            normalized = (v - mean) / std
                            normalized = float(np.clip(normalized, -5, 5))
                        except:
                            normalized = 0.0

                    numeric_row.append(normalized)

                new_numeric_features.append(numeric_row)
                pbar.update(1)

        self.numeric_features = new_numeric_features

    def _process_domain_embedding_features(self):
        """å¤„ç†åŸŸååµŒå…¥ç‰¹å¾ï¼ˆæ¦‚ç‡åˆ—è¡¨ï¼‰"""
        self.domain_embedding_features = []
        
        domain_columns = self.cfg.data.domain_name_embedding_features.column_list
        
        # æ·»åŠ åˆ—å­˜åœ¨æ€§æ£€æŸ¥
        available_columns = [col for col in domain_columns if col in self.flow_df.columns]
        
        if not available_columns:
            logger.warning("é…ç½®äº†åŸŸååµŒå…¥ç‰¹å¾ï¼Œä½†æ•°æ®ä¸­æœªæ‰¾åˆ°å¯¹åº”çš„åˆ—")
            self.domain_embedding_enabled = False  # ç¦ç”¨è¯¥è§†å›¾
            return
    
        # è¯¦ç»†çš„ç»´åº¦ä¿¡æ¯
        logger.info(f"å¯ç”¨çš„åŸŸååµŒå…¥ç‰¹å¾åˆ—é•¿åº¦: {len(available_columns)}ï¼Œå…·ä½“åŸŸååµŒå…¥ç‰¹å¾åˆ—ï¼š{available_columns}")
        label_id_map = ConfigManager.read_session_label_id_map(self.cfg.data.dataset)
        logger.info(f"åµŒå…¥çš„æ¦‚ç‡åˆ—è¡¨é•¿åº¦ï¼ˆç±»åˆ«æ•°ï¼‰: {self.prob_list_length}ï¼Œå…·ä½“ç±»åˆ«æ˜ å°„: {label_id_map}")
        logger.info(f"æœŸæœ›çš„åŸŸååµŒå…¥ç‰¹å¾æ€»ç»´åº¦: {len(available_columns)} Ã— {self.prob_list_length} = {len(available_columns) * self.prob_list_length}")
        
        if not available_columns:
            logger.warning("æ²¡æœ‰æ‰¾åˆ°å¯ç”¨çš„åŸŸååµŒå…¥ç‰¹å¾åˆ—")

        with tqdm(total=len(self.flow_df), desc="å¤„ç†åŸŸååµŒå…¥ç‰¹å¾", position=0, leave=True) as pbar:
            for idx, row in self.flow_df.iterrows():
                embedding_data = []
                
                for col in available_columns:
                    value = row[col]
                    
                    if pd.isna(value) or value is None or value == '[]':
                        # å¦‚æœä¸ºç©ºï¼Œåˆ›å»ºé»˜è®¤çš„æ¦‚ç‡åˆ†å¸ƒï¼ˆå‡åŒ€åˆ†å¸ƒæˆ–é›¶å‘é‡ï¼‰
                        default_embedding = [0.0] * self.prob_list_length
                        embedding_data.extend(default_embedding)
                    else:
                        try:
                            # è§£ææ¦‚ç‡åˆ—è¡¨å­—ç¬¦ä¸²
                            if isinstance(value, str) and value.startswith('['):
                                prob_list = ast.literal_eval(value)
                            else:
                                prob_list = self._safe_parse_list(str(value))
                            
                            # ç¡®ä¿æ¦‚ç‡åˆ—è¡¨é•¿åº¦æ­£ç¡®
                            if len(prob_list) != self.prob_list_length:
                                logger.debug(f"åˆ— {col} çš„æ¦‚ç‡åˆ—è¡¨é•¿åº¦ä¸åŒ¹é…: æœŸæœ›{self.prob_list_length}, å®é™…{len(prob_list)}")
                                # è°ƒæ•´é•¿åº¦ï¼šæˆªæ–­æˆ–å¡«å……
                                if len(prob_list) > self.prob_list_length:
                                    prob_list = prob_list[:self.prob_list_length]
                                else:
                                    prob_list.extend([0.0] * (self.prob_list_length - len(prob_list)))
                            
                            # æ·»åŠ åˆ°åµŒå…¥æ•°æ®ä¸­
                            embedding_data.extend(prob_list)
                            
                        except Exception as e:
                            logger.debug(f"è§£æåŸŸååµŒå…¥ç‰¹å¾å¤±è´¥ {col}: {e}")                            
                            default_embedding = [0.0] * self.prob_list_length # ä½¿ç”¨é»˜è®¤å€¼
                            embedding_data.extend(default_embedding)
                
                self.domain_embedding_features.append(embedding_data)
                pbar.update(1)
    
    def _safe_parse_list(self, list_str: str) -> List:
        """å®‰å…¨è§£æåˆ—è¡¨å­—ç¬¦ä¸²ï¼Œå¤„ç†å„ç§æ ¼å¼"""
        if pd.isna(list_str) or list_str == '[]' or list_str == '':
            return []
        
        # å¦‚æœå·²ç»æ˜¯åˆ—è¡¨ç±»å‹ï¼Œç›´æ¥è¿”å›
        if isinstance(list_str, list):
            return list_str
        
        try:
            # é¦–å…ˆå°è¯• ast.literal_eval
            if isinstance(list_str, str) and list_str.startswith('['):
                return ast.literal_eval(list_str)
            else:
                # å¯¹äºéåˆ—è¡¨æ ¼å¼çš„å­—ç¬¦ä¸²ï¼Œç›´æ¥è¿”å›åŸå­—ç¬¦ä¸²ï¼ˆä½œä¸ºå•å…ƒç´ åˆ—è¡¨ï¼‰
                # æˆ–è€…æ ¹æ®å…·ä½“éœ€æ±‚å¤„ç†
                return [str(list_str)]
        except (ValueError, SyntaxError):
            # å¦‚æœè§£æå¤±è´¥ï¼Œå°è¯•å…¶ä»–æ ¼å¼
            try:
                # å°è¯•æå–æ‰€æœ‰å­—æ¯æ•°å­—å’Œå¸¸è§ç¬¦å·
                import re
                # åŒ¹é…åŒ…å«å­—æ¯ã€æ•°å­—ã€ç‚¹ã€è¿å­—ç¬¦ã€ä¸‹åˆ’çº¿çš„å•è¯
                words = re.findall(r'[a-zA-Z0-9.-_]+', str(list_str))
                return words if words else []
            except:
                return []
        except Exception as e:
            if self.debug_mode:
                logger.debug(f"è§£æåˆ—è¡¨å­—ç¬¦ä¸²å¤±è´¥: {e}, è¾“å…¥: {list_str[:100]}...")
            return []
            
    def __len__(self):
        return len(self.flow_df)
    
    def __getitem__(self, row_idx):
        row = self.flow_df.iloc[row_idx]
        
        # åºåˆ—æ•°æ®ï¼ˆå¯é€‰ï¼‰
        if self.sequence_features_enabled:
            # _parse_sequence_row çš„æ—¶å€™ï¼Œå·²ç»ç¡®ä¿äº†åºåˆ—é•¿åº¦ä¸€è‡´
            sequence_data = self.sequences[row_idx]

            directions = torch.tensor(sequence_data['directions'], dtype=torch.float32)
            payload_sizes = torch.tensor(sequence_data['payload_sizes'], dtype=torch.float32)
            iat_times = torch.tensor(sequence_data['iat_times'], dtype=torch.float32)
            packet_numbers = torch.tensor(sequence_data['packet_numbers'], dtype=torch.float32)
            avg_payload_sizes = torch.tensor(sequence_data['avg_payload_sizes'], dtype=torch.float32)
            durations = torch.tensor(sequence_data['durations'], dtype=torch.float32)
            sequence_mask = torch.tensor(sequence_data['sequence_mask'], dtype=torch.bool)

        else:
            # åˆ›å»ºç©ºçš„åºåˆ—ç‰¹å¾å ä½ç¬¦
            directions = torch.zeros(self.max_seq_length, dtype=torch.float32)
            payload_sizes = torch.zeros(self.max_seq_length, dtype=torch.float32)
            iat_times = torch.zeros(self.max_seq_length, dtype=torch.float32)
            packet_numbers = torch.zeros(self.max_seq_length, dtype=torch.float32)
            avg_payload_sizes = torch.zeros(self.max_seq_length, dtype=torch.float32)
            durations = torch.zeros(self.max_seq_length, dtype=torch.float32)            
            sequence_mask = torch.zeros(self.max_seq_length, dtype=torch.bool)
        
        # æ–‡æœ¬æ•°æ®ï¼ˆå¯é€‰ï¼‰
        if self.text_features_enabled:
            text_data = self.text_features[row_idx]
            # ä¿ç•™åˆå¹¶åŸå§‹æ–‡æœ¬
            combined_text = " ".join([text for text in text_data.values() if text.strip()])
            ssl_server_name = text_data.get(self.cfg.data.text_features.ssl_server_name, "")
            dns_query = text_data.get(self.cfg.data.text_features.dns_query, "")
            cert0_subject = text_data.get(self.cfg.data.text_features.cert0_subject, "")
            cert0_issuer = text_data.get(self.cfg.data.text_features.cert0_issuer, "")
        else:
            text_data = {}
            combined_text = ""
            ssl_server_name = ""
            dns_query = ""
            cert0_subject = ""
            cert0_issuer = ""            
                
        # åŸŸååµŒå…¥ç‰¹å¾çš„æ¦‚ç‡åˆ—è¡¨ï¼ˆå¯é€‰ï¼‰
        if self.domain_embedding_enabled:
            domain_embedding = self.domain_embedding_features[row_idx]
            actual_domain_dim = len(domain_embedding)
            expected_domain_dim = len(self.cfg.data.domain_name_embedding_features.column_list) * self.prob_list_length
            if len(domain_embedding) != expected_domain_dim:
                logger.warning(f"åŸŸååµŒå…¥ç‰¹å¾ç»´åº¦ä¸åŒ¹é…: æœŸæœ›{expected_domain_dim}, å®é™…{actual_domain_dim}")
                # è‡ªåŠ¨è°ƒæ•´ç»´åº¦
                if len(domain_embedding) > expected_domain_dim:
                    domain_embedding = domain_embedding[:expected_domain_dim]
                else:
                    domain_embedding.extend([0.0] * (expected_domain_dim - len(domain_embedding)))
            
            domain_embedding_features = torch.tensor(domain_embedding, dtype=torch.float32)
        else:
            # åˆ›å»ºç©ºçš„åŸŸååµŒå…¥ç‰¹å¾
            domain_embedding_features = torch.zeros(0, dtype=torch.float32)

        # ç±»åˆ«å‹ç‰¹å¾ï¼ˆå¿…é€‰ï¼‰
        if self.categorical_features_enabled:
            categorical_features = self.categorical_features[row_idx]
        else:
            categorical_features = torch.zeros(0, dtype=torch.long)

        # æ•°å€¼å‹ç‰¹å¾ï¼ˆå¿…é€‰ï¼‰
        numeric_features = torch.tensor(self.numeric_features[row_idx], dtype=torch.float32)
        all_numeric_columns = (
            self.cfg.data.tabular_features.numeric_features.flow_features +
            self.cfg.data.tabular_features.numeric_features.x509_features +
            self.cfg.data.tabular_features.numeric_features.dns_features
        )
        available_numeric_columns = [col for col in all_numeric_columns if col in self.flow_df.columns]
        expected_numeric_dim = len(available_numeric_columns)

        if len(numeric_features) != expected_numeric_dim:
            logger.warning(f"æ•°å€¼ç‰¹å¾ç»´åº¦ä¸åŒ¹é…: æœŸæœ›{expected_numeric_dim}, å®é™…{len(numeric_features)}")
            # è‡ªåŠ¨è°ƒæ•´
            if len(numeric_features) > expected_numeric_dim:
                numeric_features = numeric_features[:expected_numeric_dim]
            else:
                padding = torch.zeros(expected_numeric_dim - len(numeric_features))
                numeric_features = torch.cat([numeric_features, padding])

        # å–„æ„/æ¶æ„çš„äºŒåˆ†ç±»æ ‡ç­¾åˆ—
        is_malicious_value = row[self.is_malicious_column].astype(int)
        is_malicious_label = torch.tensor(is_malicious_value, dtype=torch.float32)
        
        # å¤šåˆ†ç±»æ ‡ç­¾åˆ—
        multiclass_label_string = row[self.multiclass_label_column]
        is_malicious_value_alt = self._compute_is_malicious_label(multiclass_label_string)
        if is_malicious_value != is_malicious_value_alt:
            raise ValueError(
                f"is_malicious ä¸ä¸€è‡´: csv={is_malicious_value}, "
                f"derived={is_malicious_value_alt}, "
                f"label={multiclass_label_string}"
            )
        attack_family_vec, attack_type_vec = self._compute_attack_family_and_type_labels(multiclass_label_string)

        data = {
            # åºåˆ—ç‰¹å¾ï¼ˆæ ¹æ®å¯ç”¨çŠ¶æ€ï¼‰
            'directions': directions,
            'payload_sizes': payload_sizes,
            'iat_times': iat_times,
            'packet_numbers': packet_numbers,
            'avg_payload_sizes': avg_payload_sizes,
            'durations': durations,
            'sequence_mask': sequence_mask, # æœ‰æ•ˆtokenæ©ç 

            # æ–‡æœ¬ç‰¹å¾ï¼ˆæ ¹æ®å¯ç”¨çŠ¶æ€ï¼‰
            'ssl_server_name': ssl_server_name,
            'dns_query': dns_query,
            'cert0_subject': cert0_subject,
            'cert0_issuer': cert0_issuer,
            'combined_text': combined_text, # åˆå¹¶æ–‡æœ¬ï¼ˆç»™BERTä½¿ç”¨ï¼‰
                                                
            # åŸŸååµŒå…¥ç‰¹å¾ï¼ˆæ ¹æ®å¯ç”¨çŠ¶æ€ï¼‰
            'domain_embedding_features': domain_embedding_features,

            # ç±»åˆ«å‹ç‰¹å¾ï¼ˆå¿…é€‰ï¼‰
            'categorical_features': categorical_features,

            # æ•°å€¼å‹ç‰¹å¾ï¼ˆå¿…é€‰ï¼‰
            'numeric_features': numeric_features,

            # æ ‡ç­¾
            'is_malicious_label': is_malicious_label,
            'multiclass_label_string': multiclass_label_string,
            'attack_family_label': attack_family_vec,
            'attack_type_label': attack_type_vec,
            
            # å…ƒæ•°æ®
            'uid': row.get('uid', ''),
            'idx': row_idx,

            # æ·»åŠ è§†å›¾å¯ç”¨æ ‡å¿—
            'sequence_features_enabled': self.sequence_features_enabled,
            'text_features_enabled': self.text_features_enabled,
            'domain_embedding_enabled': self.domain_embedding_enabled,
        }

        # æœ€ç»ˆéªŒè¯ï¼šç¡®ä¿æ‰€æœ‰tensoréƒ½æ²¡æœ‰NaN
        for key, value in data.items():
            if torch.is_tensor(value):
                if torch.isnan(value).any():
                    logger.error(f"ä¸¥é‡é”™è¯¯: idx={row_idx}, key={key} ä»ç„¶åŒ…å«NaNå€¼")
                    # å¼ºåˆ¶ä¿®å¤
                    data[key] = torch.nan_to_num(value, nan=0.0, posinf=1.0, neginf=-1.0)

        # print("DEBUG numeric len:", len(numeric_features[row_idx]))
        # print("DEBUG domain len:", len(self.domain_embedding_features[row_idx]) if self.domain_embedding_enabled else 0)
        # print("DEBUG categorical len:", len(self.categorical_features[row_idx]) if self.categorical_features is not None else 0)

        return data
    
    def _compute_is_malicious_label(self, multiclass_label_string):
        label_norm = str(multiclass_label_string).strip().lower()
        return 0 if label_norm == "benign" else 1

    def _compute_attack_family_and_type_labels(self, multiclass_label_string):
        """
        æ ¹æ® labels é…ç½®ä¸­å®šä¹‰çš„å±‚çº§è¯­ä¹‰ï¼ˆattack_type â†’ attack_familyï¼‰ï¼Œ
        å°†åŸå§‹å¤šåˆ†ç±»å­—ç¬¦ä¸²æ ‡ç­¾ï¼ˆå¦‚ 'DoS Hulk' / 'PortScan' / 'BENIGN'ï¼‰
        è§£æä¸º attack_type ä¸ attack_family ä¸¤ä¸ªå±‚çº§çš„ç›‘ç£ä¿¡å·ã€‚

        - attack_type_labelsï¼š
            * æ ‡ç­¾ç©ºé—´ï¼šlabels.attack_type.classes
            * è¾“å‡ºï¼šé•¿åº¦ä¸º num_types çš„ 0/1 å‘é‡ï¼ˆone-hot / multi-hotï¼‰
            * è‹¥ label å±äºæŸä¸ª attack_typeï¼Œåˆ™å¯¹åº”ä½ç½®ä¸º 1ï¼›
            è‹¥ä¸º benign æˆ–æœªçŸ¥ç±»åˆ«ï¼Œåˆ™ä¸ºå…¨ 0ï¼ˆnot applicableï¼‰

        - attack_family_labelsï¼š
            * æ ‡ç­¾ç©ºé—´ï¼šlabels.attack_family.classes
            * è¾“å‡ºï¼šé•¿åº¦ä¸º num_families çš„ 0/1 å‘é‡ï¼ˆone-hot / multi-hotï¼‰
            * è‹¥ label æœ¬èº«æ˜¯ familyï¼Œç›´æ¥ç½®ä½ï¼›
            è‹¥ label æ˜¯ attack_typeï¼Œåˆ™é€šè¿‡ parent_mapping ä¸Šæº¯åˆ° familyï¼›
            è‹¥ä¸º benign æˆ–æ— æ³•æ˜ å°„ï¼Œåˆ™ä¸ºå…¨ 0ï¼ˆnot applicableï¼‰

        è¯´æ˜ï¼š
        - benign æµé‡åœ¨ attack_family / attack_type å±‚çº§å‡è¿”å›å…¨ 0 å‘é‡ï¼Œ
        è¡¨ç¤ºå…¶ä¸å±äº malicious å­æ ‘ä¸­çš„ä»»ä½•æ”»å‡»ç±»åˆ«ã€‚
        - è¯¥ 0/1 è¡¨ç¤ºæ–¹å¼ç”¨äºæ”¯æŒ OVR + BCE çš„å¤šäºŒåˆ†ç±»å»ºæ¨¡ï¼Œ
        å¹¶ä¿è¯ Dataset / Model / Loss ä¹‹é—´çš„æ ‡ç­¾è¯­ä¹‰ä¸€è‡´æ€§ã€‚
        """
        label = str(multiclass_label_string).strip()
        label_norm = label.lower()

        # ç»Ÿä¸€ benign å¤„ç†
        if label_norm == "benign":
            return (
                torch.zeros(len(self.attack_family_classes)) if self.attack_family_classes else torch.zeros(0),
                torch.zeros(len(self.attack_type_classes)) if self.attack_type_classes else torch.zeros(0)
            )

        family_vec = None
        type_vec = None

        # =========================
        # attack_typeï¼ˆæ›´åº•å±‚ï¼‰
        # =========================
        if self.attack_type_classes is None:
            type_vec = torch.zeros(0)
        else:
            type_vec = torch.zeros(
                len(self.attack_type_classes),
                dtype=torch.float32
            )
            if label in self.attack_type_classes:
                idx = self.attack_type_classes.index(label)
                type_vec[idx] = 1.0

        # =========================
        # attack_familyï¼ˆé€šè¿‡æ˜ å°„ï¼‰
        # =========================
        if self.attack_family_classes is None:
            family_vec = torch.zeros(0)
        else:
            family_vec = torch.zeros(
                len(self.attack_family_classes),
                dtype=torch.float32
            )

            fam_name = None

            # æƒ…å†µ 1ï¼šlabel æœ¬èº«å°±æ˜¯ family
            if label in self.attack_family_classes:
                fam_name = label

            # æƒ…å†µ 2ï¼šlabel æ˜¯ attack_type â†’ æ˜ å°„åˆ° family
            elif (
                self.attack_type_parent_mapping is not None
                and label in self.attack_type_parent_mapping
            ):
                fam_name = self.attack_type_parent_mapping[label]

            if fam_name is not None:
                fam_idx = self.attack_family_classes.index(fam_name)
                family_vec[fam_idx] = 1.0
            
        return family_vec, type_vec


class MultiviewFlowDataModule(pl.LightningDataModule):
    """å¤šè§†å›¾æµé‡æ•°æ®æ¨¡å—"""
    
    def __init__(self, cfg: DictConfig):
        super().__init__()
        self.cfg = cfg

        # ç¼“å­˜å¸¸ç”¨é…ç½®
        self.flow_data_path = self.cfg.data.flow_data_path
        self.session_split_path = self.cfg.data.session_split.session_split_path
        self.batch_size = self.cfg.data.batch_size
        self.num_workers = self.cfg.data.num_workers

        # ç¼“å­˜ä¼šè¯åˆ’åˆ†é…ç½®
        self.split_config = cfg.data.session_split
        self.split_column = self.split_config.split_column
        self.flow_uid_list_column = self.split_config.flow_uid_list_column
        self.train_split = self.split_config.train_split
        self.validate_split = self.split_config.validate_split
        self.test_split = self.split_config.test_split

        self.train_dataset = None
        self.validate_dataset = None
        self.test_dataset = None

        # å…¶ä»–é…ç½®
        self.is_malicious_column = cfg.data.is_malicious_column
        self.multiclass_label_column = cfg.data.multiclass_label_column
        self.debug_mode = cfg.debug.debug_mode
                
        
    def prepare_data(self):
        # ä¸‹è½½æˆ–å‡†å¤‡æ•°æ®
        pass
    
    def setup(self, stage=None):
        # å¤„ç†ä¸åŒçš„stageè¾“å…¥ç±»å‹
        if stage is None:
            stage_name = "fit" # "fit" æ˜¯ Lightning çš„è§„èŒƒåç§°ï¼Œå«ä¹‰æ˜¯ â€œè®­ç»ƒ + éªŒè¯â€
        elif hasattr(stage, 'value'):  # å¦‚æœæ˜¯æšä¸¾ç±»å‹
            stage_name = stage.value.lower()  # ç›´æ¥ä½¿ç”¨æšä¸¾çš„valueå±æ€§
        elif isinstance(stage, str):
            stage_name = stage.lower()
        else:
            # å…¶ä»–æƒ…å†µï¼Œå°è¯•è½¬æ¢ä¸ºå­—ç¬¦ä¸²
            stage_name = str(stage).lower()
        
        # æ£€æŸ¥æ˜¯å¦å·²ç»åˆå§‹åŒ–
        if self._is_already_setup(stage_name):
            logger.info(f"{stage_name} é˜¶æ®µæ•°æ®é›†å·²åˆå§‹åŒ–ï¼Œè·³è¿‡é‡å¤setup")
            return

        logger.info(f"æ•°æ®æ¨¡å— setup é˜¶æ®µ: {stage_name}")        

        # ä¸€æ¬¡æ€§è¯»å–æ‰€æœ‰å¿…è¦æ•°æ®ï¼ˆæ”¯æŒç¼“å­˜ï¼‰
        self._load_data_with_cache(stage_name)

        if stage_name == "fit":
            self._create_datasets(stage_name)

        if stage in (None, "fit") and self.train_dataset is not None and self.validate_dataset is not None:
            train_is_malicious_labels = self.train_dataset.is_malicious_labels
            val_is_malicious_labels = self.validate_dataset.is_malicious_labels
            logger.info("==========================================")            
            logger.info(f"[è®­ç»ƒé›†] æ­£æ ·æœ¬={sum(train_is_malicious_labels)}, è´Ÿæ ·æœ¬={len(train_is_malicious_labels)-sum(train_is_malicious_labels)}, æ¯”ä¾‹={sum(train_is_malicious_labels)/len(train_is_malicious_labels):.4f}")
            logger.info(f"[éªŒè¯é›†] æ­£æ ·æœ¬={sum(val_is_malicious_labels)}, è´Ÿæ ·æœ¬={len(val_is_malicious_labels)-sum(val_is_malicious_labels)}, æ¯”ä¾‹={sum(val_is_malicious_labels)/len(val_is_malicious_labels):.4f}")

        if stage in (None, "test") and self.test_dataset is not None:
            test_is_malicious_labels = self.test_dataset.is_malicious_labels
            logger.info("==========================================")            
            logger.info(f"[æµ‹è¯•é›†] æ­£æ ·æœ¬={sum(test_is_malicious_labels)}, æ¯”ä¾‹={sum(test_is_malicious_labels)/len(test_is_malicious_labels):.4f}")


    def _is_already_setup(self, stage_name):
        """æ£€æŸ¥æ•°æ®é›†æ˜¯å¦å·²ç»åˆå§‹åŒ–"""
        if stage_name == 'fit':
            # fité˜¶æ®µéœ€è¦è®­ç»ƒé›†å’ŒéªŒè¯é›†éƒ½åˆå§‹åŒ–
            return self.train_dataset is not None and self.validate_dataset is not None
        elif stage_name == 'validate':
            # validateé˜¶æ®µåªéœ€è¦éªŒè¯é›†
            return self.validate_dataset is not None
        elif stage_name == 'test':
            # testé˜¶æ®µåªéœ€è¦æµ‹è¯•é›†
            return self.test_dataset is not None
        else:
            # å…¶ä»–æƒ…å†µè¿”å›False
            return False

    def read_large_csv_with_progress(self, filepath, description="è¯»å–æ•°æ®", verbose=True):
        if not verbose:
            return pd.read_csv(filepath)

        logger.info(f"{description} ä» {filepath}...")
        file_size = os.path.getsize(filepath) / (1024**3)
        logger.info(f"æ–‡ä»¶å¤§å°: {file_size:.2f} GB")

        sample_df = pd.read_csv(filepath, nrows=5)
        logger.info(f"æ£€æµ‹åˆ° {len(sample_df.columns)} åˆ—ï¼Œå¼€å§‹åˆ†å—è¯»å–...")

        with open(filepath, "r") as f:
            total_rows = sum(1 for _ in f) - 1

        chunk_size = 100_000  # æ¯æ¬¡è¯»å–10ä¸‡è¡Œ
        chunks = []

        # â­ å…³é”®ï¼šåªæœ‰ rank 0 æ‰å¼€ tqdmï¼Œå¦åˆ™æ¯ä¸ªgpuéƒ½æ‰“å°è¿›åº¦æ¡ï¼Œä¼šæ˜¾ç¤ºé”™ä¹±
        if self.trainer is not None:
            is_rank_zero = self.trainer.is_global_zero
        else:
            is_rank_zero = True  # é Lightning åœºæ™¯å…œåº•

        pbar = tqdm(
            total=total_rows,
            desc=description,
            unit="rows",
            dynamic_ncols=True,
            leave=True,
            disable=not is_rank_zero,   # â­â­ å…³é”®
        )

        for chunk in pd.read_csv(filepath, chunksize=chunk_size, low_memory=False):
            chunks.append(chunk)
            pbar.update(len(chunk))

        pbar.close()

        df = pd.concat(chunks, ignore_index=True)
        logger.info(f"{description} å®Œæˆ! æ•°æ®å½¢çŠ¶: {df.shape}")
        return df
    
    def _load_data_with_cache(self, stage_name):
        """å¸¦ç¼“å­˜çš„æ•°æ®åŠ è½½"""
        # å¦‚æœflow_dfå·²ç»å­˜åœ¨ï¼Œç›´æ¥ä½¿ç”¨ç¼“å­˜çš„æ•°æ®
        if hasattr(self, 'flow_df') and self.flow_df is not None:
            logger.info("ä½¿ç”¨ç¼“å­˜çš„flow_dfæ•°æ®ï¼Œè·³è¿‡é‡å¤è¯»å–")
        else:
            # åªåœ¨ç¬¬ä¸€æ¬¡éœ€è¦æ—¶è¯»å–æ•°æ®
            flow_df = self.read_large_csv_with_progress(self.flow_data_path)
            # å¢å¼ºæ•°æ®è´¨é‡æ£€æŸ¥
            self.flow_df = self._validate_data_quality(flow_df, stage=stage_name)  # âœ… æ­£ç¡®æ¥æ”¶ä¿®æ”¹åçš„flow_df

        # å¦‚æœsession_dfå·²ç»å­˜åœ¨ï¼Œç›´æ¥ä½¿ç”¨ç¼“å­˜çš„æ•°æ®
        if hasattr(self, 'session_df') and self.session_df is not None:
            logger.info("ä½¿ç”¨ç¼“å­˜çš„session_dfæ•°æ®ï¼Œè·³è¿‡é‡å¤è¯»å–")
        else:
            # åªåœ¨ç¬¬ä¸€æ¬¡éœ€è¦æ—¶è¯»å–sessionæ•°æ®
            self.session_df = self.read_large_csv_with_progress(self.session_split_path)

            # æ£€æŸ¥å¿…è¦çš„åˆ—æ˜¯å¦å­˜åœ¨
            required_columns = [self.split_column, self.flow_uid_list_column]
            missing_columns = [col for col in required_columns if col not in self.session_df.columns]
            if missing_columns:
                raise ValueError(f"session_dfç¼ºå°‘å¿…è¦åˆ—: {missing_columns}")
            
            # æ£€æŸ¥splitå€¼çš„æœ‰æ•ˆæ€§
            valid_splits = [self.train_split, self.validate_split, self.test_split]
            actual_splits = self.session_df[self.split_column].unique()
            invalid_splits = [split for split in actual_splits if split not in valid_splits]
            if invalid_splits:
                logger.warning(f"å‘ç°æ— æ•ˆçš„splitå€¼: {invalid_splits}")
        
        
    def _validate_data_quality(self, df, stage="unknown"):
        """ä»…æ ¡éªŒé…ç½®æ–‡ä»¶ä¸­ä½¿ç”¨åˆ°çš„åˆ—æ˜¯å¦åŒ…å« NaNï¼Œä¸æ£€æŸ¥å…¶å®ƒæ— å…³åˆ—ã€‚
        è‹¥ target åˆ—å­˜åœ¨ NaNï¼Œç«‹å³æŠ›å¼‚å¸¸ï¼Œè¦æ±‚ç”¨æˆ·å¤„ç†ã€‚"""

        logger.info(f"æ£€æŸ¥ {stage} é˜¶æ®µæ•°æ®è´¨é‡ï¼ˆä»…æ£€æŸ¥ cfg ä¸­å¼•ç”¨çš„åˆ—ï¼‰...")

        # ============ 1. æ”¶é›†æ‰€æœ‰éœ€è¦æ£€æŸ¥çš„åˆ— ============
        required_columns = set()

        # is_malicious å’Œ multiclass_label æ ‡ç­¾åˆ—ï¼ˆæœ€å…³é”®ï¼‰
        if self.is_malicious_column is not None:
            required_columns.add(self.is_malicious_column)
        if self.multiclass_label_column is not None:
            required_columns.add(self.multiclass_label_column)

        # æ•°å€¼ç‰¹å¾åˆ—
        if hasattr(self.cfg.data.tabular_features.numeric_features, "flow_features"):
            required_columns.update(self.cfg.data.tabular_features.numeric_features.flow_features)

        # åºåˆ—ç‰¹å¾åˆ—ï¼ˆå¯é€‰ï¼‰
        if hasattr(self.cfg.data, "sequence_features") and self.cfg.data.sequence_features is not None:
            seq_cfg = self.cfg.data.sequence_features
            required_columns.update([
                seq_cfg.packet_direction,
                seq_cfg.packet_iat,
                seq_cfg.packet_payload,
            ])

        # æ–‡æœ¬ç‰¹å¾åˆ—ï¼ˆå¯é€‰ï¼‰
        if hasattr(self.cfg.data, "text_features") and self.cfg.data.text_features is not None:
            txt_cfg = self.cfg.data.text_features
            required_columns.update([
                txt_cfg.ssl_server_name,
                txt_cfg.dns_query,
                txt_cfg.cert0_subject,
                txt_cfg.cert0_issuer,
            ])

        # åŸŸååµŒå…¥ç‰¹å¾åˆ—ï¼ˆå¯é€‰ï¼‰
        if hasattr(self.cfg.data, "domain_name_embedding_features"):
            if hasattr(self.cfg.data.domain_name_embedding_features, "column_list"):
                required_columns.update(self.cfg.data.domain_name_embedding_features.column_list)

        required_columns = list(required_columns)

        # ============ 2. å¿…é¡»å­˜åœ¨çš„åˆ—æ£€æŸ¥ ============
        missing_cols = [col for col in required_columns if col not in df.columns]
        if missing_cols:
            raise ValueError(f"âŒ æ•°æ®ç¼ºå°‘é…ç½®æ–‡ä»¶è¦æ±‚çš„åˆ—: {missing_cols}")

        # ============ 3. åªæ£€æŸ¥è¿™äº›åˆ—ä¸­çš„ NaN ============
        df_sub = df[required_columns]
        nan_counts = df_sub.isna().sum()

        # å…¨éƒ¨ä½¿ç”¨çš„åˆ—æ˜¯å¦æœ‰ NaN
        if nan_counts.any():
            nan_cols = nan_counts[nan_counts > 0]

            # ---- ç‰¹æ®Šå¤„ç†: is_malicious åˆ—å‡ºç° NaN â†’ ç›´æ¥æŠ¥é”™ ----
            if nan_counts[self.is_malicious_column] > 0:
                raise ValueError(
                    f"âŒ is_malicious åˆ— '{self.is_malicious_column}' å‡ºç° NaN å€¼: {nan_counts[self.is_malicious_column]} è¡Œã€‚\n"
                    f"è¯·åœ¨åŠ è½½ CSV å‰æ‰‹åŠ¨æ¸…æ´—æ•°æ®ï¼Œå¦åˆ™æ¨¡å‹æ— æ³•è®­ç»ƒã€‚"
                )

            # å…¶å®ƒåˆ—çš„ NaN â†’ ç»™ warningï¼Œè®©ç”¨æˆ·å¤„ç†
            logger.warning("âš ï¸ ä»¥ä¸‹ä½¿ç”¨åˆ°çš„ç‰¹å¾åˆ—åŒ…å« NaNï¼š")
            for col, count in nan_cols.items():
                logger.warning(f"  {col}: {count} ä¸ª NaN ({count / len(df) * 100:.2f}%)")

        logger.info(f"{stage} é˜¶æ®µæ•°æ®è´¨é‡æ£€æŸ¥å®Œæˆï¼ˆä»…æ£€æŸ¥ cfg ä½¿ç”¨çš„åˆ—ï¼‰")
        return df

    def _create_datasets(self, stage_name):
        split_mode = self.cfg.data.get("split_mode", "session").lower()
        logger.info(f"æ•°æ®åˆ’åˆ†æ¨¡å¼ï¼š{split_mode}")

        if split_mode == "session":
            logger.info("ä½¿ç”¨åŸºäº session_df çš„ä¼šè¯çº§åˆ’åˆ†ç­–ç•¥")
            assert self.split_column in self.session_df.columns
            assert "uid" in self.flow_df.columns

            # ä½¿ç”¨session_dfä¼šè¯è¿›è¡Œæ•°æ®é›†åˆ’åˆ†
            train_session_df = self.session_df[self.session_df[self.split_column] == self.train_split]
            validate_session_df = self.session_df[self.session_df[self.split_column] == self.validate_split]
            test_session_df = self.session_df[self.session_df[self.split_column] == self.test_split]

            # æå–æ‰€æœ‰è®­ç»ƒflowçš„UID
            train_flow_uids = []
            for uid_list in train_session_df[self.flow_uid_list_column]:
                if pd.notna(uid_list) and uid_list != '[]':
                    try:
                        uids = ast.literal_eval(uid_list) if isinstance(uid_list, str) else uid_list
                        train_flow_uids.extend(uids)
                    except:
                        continue

            # æå–æ‰€æœ‰éªŒè¯flowçš„UID
            validate_flow_uids = []
            for uid_list in validate_session_df[self.flow_uid_list_column]:
                if pd.notna(uid_list) and uid_list != '[]':
                    try:
                        uids = ast.literal_eval(uid_list) if isinstance(uid_list, str) else uid_list
                        validate_flow_uids.extend(uids)
                    except:
                        continue

            # æå–æ‰€æœ‰æµ‹è¯•flowçš„UID
            test_flow_uids = []
            for uid_list in test_session_df[self.flow_uid_list_column]:
                if pd.notna(uid_list) and uid_list != '[]':
                    try:
                        uids = ast.literal_eval(uid_list) if isinstance(uid_list, str) else uid_list
                        test_flow_uids.extend(uids)
                    except:
                        continue

            # æ ¹æ®UIDåˆ’åˆ†flowæ•°æ®é›†
            train_flow_df = self.flow_df[self.flow_df['uid'].isin(train_flow_uids)]
            validate_flow_df = self.flow_df[self.flow_df['uid'].isin(validate_flow_uids)]
            test_flow_df = self.flow_df[self.flow_df['uid'].isin(test_flow_uids)]

        elif split_mode == "flow":
            
            logger.info("ä½¿ç”¨åŸºäº flow_df çš„éšæœºé€æµåˆ’åˆ†ç­–ç•¥")

            self.flow_train_ratio = self.cfg.data.flow_split.train_ratio
            self.flow_validate_ratio = self.cfg.data.flow_split.validate_ratio
            self.flow_test_ratio = self.cfg.data.flow_split.test_ratio

            is_malicious_labels = self.flow_df[self.is_malicious_column].values

            from sklearn.model_selection import train_test_split

            train_df, temp_df = train_test_split(
                self.flow_df,
                test_size=1 - self.flow_train_ratio,
                stratify=is_malicious_labels,
                random_state=42,
                shuffle=False
            )

            temp_is_malicious_labels = temp_df[self.is_malicious_column].values
            val_ratio_in_temp = self.flow_validate_ratio / (self.flow_validate_ratio + self.flow_test_ratio)

            validate_df, test_df = train_test_split(
                temp_df,
                test_size=1 - val_ratio_in_temp,
                stratify=temp_is_malicious_labels,
                random_state=42,
                shuffle=False
            )

            train_flow_df = train_df
            validate_flow_df = validate_df
            test_flow_df = test_df
            
        else:
            raise ValueError(
                f"âŒ æ— æ•ˆçš„ split_mode='{split_mode}'ã€‚"
                f"è¯·åœ¨é…ç½®æ–‡ä»¶ä¸­ä½¿ç”¨ 'session' æˆ– 'flow'"
            )
                
        # åˆ›å»ºè®­ç»ƒé›† - ä¼ å…¥å®Œæ•´çš„ cfg å¯¹è±¡
        logger.info(f">>>> åˆ›å»ºè®­ç»ƒé›†MultiviewFlowDataset ...")
        self.train_dataset = MultiviewFlowDataset(train_flow_df, self.cfg, is_training=True)
        
        # åˆ›å»ºéªŒè¯é›†ï¼šåœ¨æ„é€ å‡½æ•°ä¸­æ³¨å…¥è®­ç»ƒé›†æ˜ å°„
        logger.info(f">>>> åˆ›å»ºéªŒè¯é›†MultiviewFlowDataset ...")
        self.validate_dataset = MultiviewFlowDataset(
            validate_flow_df, 
            self.cfg, 
            is_training=False,
            train_categorical_mappings=self.train_dataset.categorical_val2idx_mappings,
            train_categorical_columns_effective=self.train_dataset.categorical_columns_effective
        )
        
        # åˆ›å»ºæµ‹è¯•é›†ï¼šåœ¨æ„é€ å‡½æ•°ä¸­æ³¨å…¥è®­ç»ƒé›†æ˜ å°„
        logger.info(f">>>> åˆ›å»ºæµ‹è¯•é›†MultiviewFlowDataset ...")
        self.test_dataset = MultiviewFlowDataset(
            test_flow_df, 
            self.cfg, 
            is_training=False,
            train_categorical_mappings=self.train_dataset.categorical_val2idx_mappings,
            train_categorical_columns_effective=self.train_dataset.categorical_columns_effective
        )

        self._validate_categorical_consistency()
        
        # åº”è¯¥ç¡®ä¿éªŒè¯é›†ä½¿ç”¨è®­ç»ƒé›†çš„ç»Ÿè®¡ä¿¡æ¯
        if hasattr(self.train_dataset, 'numeric_stats'):
            self.validate_dataset.numeric_stats = self.train_dataset.numeric_stats  # âœ… ä¼ é€’ç»Ÿè®¡ä¿¡æ¯
            self.test_dataset.numeric_stats = self.train_dataset.numeric_stats  # âœ… ä¼ é€’ç»Ÿè®¡ä¿¡æ¯
            
            # é‡æ–°åº”ç”¨å½’ä¸€åŒ–
            logger.info(f"âœ… é‡æ–°åº”ç”¨æ•°å€¼ç‰¹å¾æ ‡å‡†åŒ–: éªŒè¯é›†ä½¿ç”¨è®­ç»ƒé›†çš„ç»Ÿè®¡ä¿¡æ¯")
            logger.info(f"   è¦†ç›–ç‰¹å¾æ•°é‡: {len(self.train_dataset.numeric_stats)}")
            self.validate_dataset.apply_numeric_stats()
            
            logger.info(f"âœ… é‡æ–°åº”ç”¨æ•°å€¼ç‰¹å¾æ ‡å‡†åŒ–: æµ‹è¯•é›†ä½¿ç”¨è®­ç»ƒé›†çš„ç»Ÿè®¡ä¿¡æ¯")
            logger.info(f"   è¦†ç›–ç‰¹å¾æ•°é‡: {len(self.train_dataset.numeric_stats)}")
            self.test_dataset.apply_numeric_stats()            
            
        else:
            logger.info("ğŸ”„ ä½¿ç”¨é»˜è®¤ç»Ÿè®¡ä¿¡æ¯è¿›è¡Œæ•°å€¼ç‰¹å¾æ ‡å‡†åŒ–")
            # ä¸ºæ¯ä¸ªæ•°å€¼åˆ—åˆ›å»ºé»˜è®¤ç»Ÿè®¡ä¿¡æ¯
            flow_columns = self.cfg.data.tabular_features.numeric_features.flow_features
            default_stats = {}
            for col in flow_columns:
                default_stats[col] = {'mean': 0, 'std': 1}
            
            self.validate_dataset.numeric_stats = default_stats
            self.test_dataset.numeric_stats = default_stats
            logger.info(f"   é»˜è®¤ç»Ÿè®¡ä¿¡æ¯è¦†ç›–ç‰¹å¾æ•°é‡: {len(default_stats)}")
        
        logger.info(f"æ•°æ®é›†åˆ’åˆ†: è®­ç»ƒé›† {len(train_flow_df)}, éªŒè¯é›† {len(validate_flow_df)}, æµ‹è¯•é›† {len(test_flow_df)}")

    # éªŒè¯æ˜ å°„ä¸€è‡´æ€§
    def _validate_categorical_consistency(self):
        """éªŒè¯æ‰€æœ‰æ•°æ®é›†çš„ç±»åˆ«æ˜ å°„ä¸€è‡´æ€§"""
        train_mappings = self.train_dataset.categorical_val2idx_mappings
        val_mappings = self.validate_dataset.categorical_val2idx_mappings
        test_mappings = self.test_dataset.categorical_val2idx_mappings
        
        assert train_mappings == val_mappings == test_mappings, "ç±»åˆ«æ˜ å°„ä¸ä¸€è‡´ï¼"
        logger.info("âœ… æ‰€æœ‰æ•°æ®é›†çš„ç±»åˆ«æ˜ å°„éªŒè¯ä¸€è‡´")
            
    def train_dataloader(self):
        return DataLoader(
            self.train_dataset,
            batch_size=self.batch_size,
            num_workers=self.num_workers,
            shuffle=True,
            pin_memory=True
        )
    
    def val_dataloader(self):
        return DataLoader(
            self.validate_dataset,
            batch_size=self.batch_size,
            num_workers=self.num_workers,
            shuffle=False,
            pin_memory=True
        )
    
    def test_dataloader(self):
        return DataLoader(
            self.test_dataset,
            batch_size=self.batch_size,
            num_workers=self.num_workers,
            shuffle=False,
            pin_memory=True
        )