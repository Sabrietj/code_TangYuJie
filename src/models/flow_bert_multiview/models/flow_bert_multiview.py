import importlib
from datetime import datetime
from concept_drift_detect.concept_drift_detector import ConceptDriftManager
import torch
import torch.nn as nn
import torch.nn.functional as F
import pytorch_lightning as pl
from omegaconf import DictConfig
import logging
from typing import Dict, List, Tuple, Optional, Any
import numpy as np
import os, sys
import matplotlib.pyplot as plt
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score,
    f1_score, classification_report, roc_auc_score,
    average_precision_score, confusion_matrix,
)
# æ³¨é‡Šæ‰åŸæ¥çš„SHAPå¯¼å…¥ï¼Œä½¿ç”¨æ–°çš„é€šç”¨æ¡†æ¶
# import shap    # å¯è§†åŒ– added  by qinyf
from pathlib import Path
import json

# å¯¼å…¥é…ç½®ç®¡ç†å™¨å’Œç›¸å…³æ¨¡å—
try:
    # æ·»åŠ ../../utilsç›®å½•åˆ°Pythonæœç´¢è·¯å¾„
    utils_path = os.path.join(os.path.dirname(__file__), '..', '..', '..', 'utils')
    sys.path.insert(0, utils_path)    
    import config_manager as ConfigManager
    from logging_config import setup_preset_logging
    # ä½¿ç”¨ç»Ÿä¸€çš„æ—¥å¿—é…ç½®
    logger = setup_preset_logging(log_level=logging.INFO)
except ImportError as e:
    print(f"å¯¼å…¥é”™è¯¯: {e}")
    print("è¯·ç¡®ä¿é¡¹ç›®ç»“æ„æ­£ç¡®ï¼Œæ‰€æœ‰ä¾èµ–æ¨¡å—å¯ç”¨")
    sys.exit(1)

# å¯¼å…¥æ–°çš„é€šç”¨SHAPåˆ†ææ¡†æ¶
try:
    # æ·»åŠ ../../utilsç›®å½•åˆ°Pythonæœç´¢è·¯å¾„
    hyper_path = os.path.join(os.path.dirname(__file__), '..', '..', '..', 'hyper_optimus')
    sys.path.insert(0, hyper_path) 
    # from shap_analysis import SHAPAnalyzeMixin,ShapAnalyzer

    from shap_analysis import ShapAnalyzer
except ImportError as e:
    print(f"å¯¼å…¥é”™è¯¯: {e}")
    print("è¯·ç¡®ä¿é¡¹ç›®ç»“æ„æ­£ç¡®ï¼Œæ‰€æœ‰ä¾èµ–æ¨¡å—å¯ç”¨")
    sys.exit(1)

try:
    # è¯·ç¡®ä¿ concept_drift_detector.py æ–‡ä»¶ä¸­åŒ…å«äº† ConceptDriftManager ç±»
    # è·¯å¾„æ ¹æ®æ‚¨çš„å®é™…é¡¹ç›®ç»“æ„å¯èƒ½éœ€è¦å¾®è°ƒ
    from concept_drift_detect.concept_drift_detector import ConceptDriftManager
    DRIFT_MANAGER_AVAILABLE = True
except ImportError as e:
    logger.warning(f"æ— æ³•å¯¼å…¥ ConceptDriftManager: {e}ã€‚æ¦‚å¿µæ¼‚ç§»æ£€æµ‹åŠŸèƒ½å°†ä¸å¯ç”¨ã€‚")
    DRIFT_MANAGER_AVAILABLE = False

# =============================================
# Safe Transformers Importï¼ˆä»…ä¿ç•™ BERT + schedulersï¼‰
# =============================================
try:
    from transformers import (
        BertModel,
        BertTokenizer,
        BertConfig,
        get_linear_schedule_with_warmup,
        get_constant_schedule_with_warmup,
        get_cosine_schedule_with_warmup,
    )
    _TRANSFORMERS_IMPORT_ERROR = None
except ImportError as exc:  # pragma: no cover - optional dependency guard
    BertModel = BertTokenizer = BertConfig = None  # type: ignore[assignment]
    get_linear_schedule_with_warmup = None  # type: ignore[assignment]
    get_constant_schedule_with_warmup = None  # type: ignore[assignment]
    get_cosine_schedule_with_warmup = None  # type: ignore[assignment]
    _TRANSFORMERS_IMPORT_ERROR = exc

try:
    from transformers import AdamW as _HFAdamW
except ImportError:  # pragma: no cover - optional dependency guard
    _HFAdamW = None

from torch.optim import AdamW as _TorchAdamW

AdamW = _HFAdamW or _TorchAdamW

def _require_transformers() -> None:
    """ç¡®ä¿ transformers å·²æ­£ç¡®å®‰è£…"""
    if _TRANSFORMERS_IMPORT_ERROR is not None:
        raise ImportError(
            "éœ€è¦å®‰è£… transformers æ‰èƒ½ä½¿ç”¨ `FlowBertMultiview`ï¼Œè¯·è¿è¡Œ `pip install transformers`ã€‚"
        ) from _TRANSFORMERS_IMPORT_ERROR

class SequenceEncoder(nn.Module):
    """åºåˆ—ç¼–ç å™¨ - å¤„ç†ä¸å®šé•¿åºåˆ—ç‰¹å¾"""
    
    def __init__(
        self, 
        embedding_dim: int, 
        num_layers: int,
        num_heads: int,  # å¤šå¤´æ³¨æ„åŠ›å¤´æ•°ï¼Œç»Ÿä¸€ä½¿ç”¨ num_heads
        dropout: float = 0.1,
        max_seq_length: int = 1000  # æä¾›é»˜è®¤å€¼ä»¥ä¿æŒå‘åå…¼å®¹ï¼Œä½†æ¨èä»å¤–éƒ¨ä¼ å…¥
    ):
        super().__init__()
        self.embedding_dim = embedding_dim
        
        # åºåˆ—ç‰¹å¾åµŒå…¥å±‚
        self.direction_projection = nn.Linear(1, embedding_dim)
        self.payload_projection = nn.Linear(1, embedding_dim)
        self.iat_projection = nn.Linear(1, embedding_dim)
        self.packet_number_projection = nn.Linear(1, embedding_dim)
        self.avg_payload_projection = nn.Linear(1, embedding_dim)
        self.duration_projection = nn.Linear(1, embedding_dim)

        # ç‰¹å¾èåˆæŠ•å½±å±‚ï¼šèåˆæ–¹å‘ã€è½½è·ã€IATã€æ•°æ®åŒ…æ•°ã€å¹³å‡è½½è·å¤§å°ã€æŒç»­æ—¶é—´ å…­ä¸ªç»´åº¦
        self.feature_fusion_projection = nn.Linear(6 * embedding_dim, embedding_dim)

        # ä½ç½®ç¼–ç 
        self.position_embedding = nn.Embedding(max_seq_length, embedding_dim)
        
        # æ·»åŠ LayerNormå±‚æé«˜ç¨³å®šæ€§
        # self.direction_norm = nn.LayerNorm(embedding_dim)
        # self.payload_norm = nn.LayerNorm(embedding_dim)
        # self.iat_norm = nn.LayerNorm(embedding_dim)
        # self.packet_number_norm = nn.LayerNorm(embedding_dim)
        # self.avg_payload_norm = nn.LayerNorm(embedding_dim)  
        # self.duration_norm = nn.LayerNorm(embedding_dim)
        # self.feature_fusion_norm = nn.LayerNorm(embedding_dim)
        # self.combined_norm = nn.LayerNorm(embedding_dim) 
        
        # Transformerç¼–ç å™¨
        # nn.TransformerEncoderLayer å‚æ•°è¯´æ˜ï¼š
        #   - d_model: è¾“å…¥ç»´åº¦
        #   - nhead: å¤šå¤´æ³¨æ„åŠ›å¤´æ•°ï¼ˆæ³¨æ„ï¼šè¿™é‡Œå‚æ•°åæ˜¯ nheadï¼Œä¸æ˜¯ num_headsï¼‰
        #   - dim_feedforward: å‰é¦ˆç½‘ç»œéšè—å±‚å¤§å°
        #   - dropout: dropoutæ¯”ä¾‹
        #   - batch_first: True è¡¨ç¤ºè¾“å…¥ä¸º [batch, seq_len, feature_dim]
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=embedding_dim,
            nhead=num_heads,
            dim_feedforward=4 * embedding_dim,
            dropout=dropout,
            batch_first=True
        )
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)
        
        # æ³¨æ„åŠ›æ± åŒ–
        # nn.MultiheadAttention å‚æ•°è¯´æ˜ï¼š
        #   - embed_dim: è¾“å…¥ç‰¹å¾ç»´åº¦
        #   - num_heads: å¤šå¤´æ³¨æ„åŠ›å¤´æ•°ï¼ˆè¿™é‡Œå‚æ•°åæ˜¯ num_headsï¼‰
        #   - dropout: dropoutæ¯”ä¾‹
        #   - batch_first: True è¡¨ç¤ºè¾“å…¥ä¸º [batch, seq_len, feature_dim]
        self.attention_pooling = nn.MultiheadAttention(
            embed_dim=embedding_dim, 
            num_heads=num_heads, 
            dropout=dropout, 
            batch_first=True
        )
        
    def forward(self, sequence_data: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:
        """
        Args:
            sequence_data: åŒ…å«åºåˆ—ç‰¹å¾çš„å­—å…¸ï¼ˆä¿®æ”¹åçš„ç»“æ„ï¼‰
                - directions: [batch_size, seq_len] ä¼ è¾“æ–¹å‘çš„åºåˆ—
                - payload_sizes: [batch_size, seq_len] è½½è·å¤§å°åºåˆ—
                - iat_times: [batch_size, seq_len] æ—¶é—´é—´éš”åºåˆ—
                - packet_numbers: [batch_size, seq_len] æ•°æ®åŒ…æ•°é‡åºåˆ—
                - avg_payload_sizes: [batch_size, seq_len] å¹³å‡è½½è·å¤§å°åºåˆ—           
                - durations: [batch_size, seq_len] æŒç»­æ—¶é—´åºåˆ—
                - sequence_mask: [batch_size, seq_len] åºåˆ—æ©ç 
                
        Returns:
            sequence_embeddings: [batch_size, embedding_dim] åºåˆ—åµŒå…¥è¡¨ç¤º
        """
        directions = sequence_data['directions']
        payload_sizes = sequence_data['payload_sizes']        
        iat_times = sequence_data['iat_times']
        packet_numbers = sequence_data['packet_numbers']
        avg_payload_sizes = sequence_data['avg_payload_sizes']
        durations = sequence_data['durations']
        sequence_mask = sequence_data['sequence_mask']

        batch_size, seq_len = directions.shape
               
        # 1ï¸âƒ£ å†…å®¹ç‰¹å¾ embedding
        directions_emb = self.direction_projection(directions.unsqueeze(-1))  # [batch_size, seq_len, emb_dim]
        # directions_emb = self.direction_norm(directions_emb)

        payload_emb = self.payload_projection(payload_sizes.unsqueeze(-1))  # [batch_size, seq_len, emb_dim]
        # payload_emb = self.payload_norm(payload_emb)        

        iat_emb = self.iat_projection(iat_times.unsqueeze(-1))  # [batch_size, seq_len, emb_dim]
        # iat_emb = self.iat_norm(iat_emb)

        packet_number_emb = self.packet_number_projection(packet_numbers.unsqueeze(-1))  # [batch_size, seq_len, emb_dim]
        # packet_number_emb = self.packet_number_norm(packet_number_emb)

        avg_payload_emb = self.avg_payload_projection(avg_payload_sizes.unsqueeze(-1))  # [batch_size, seq_len, emb_dim]
        # avg_payload_emb = self.avg_payload_norm(avg_payload_emb)

        duration_emb = self.duration_projection(durations.unsqueeze(-1))  # [batch_size, seq_len, emb_dim]
        # duration_emb = self.duration_norm(duration_emb)

        # 2ï¸âƒ£ å†…å®¹èåˆ
        combined_emb = torch.cat(
            [directions_emb, payload_emb, iat_emb, packet_number_emb, avg_payload_emb, duration_emb],
            dim=-1
        )
        combined_emb = self.feature_fusion_projection(combined_emb)        

        # 3ï¸âƒ£ åŠ ä½ç½®ç¼–ç 
        positions = torch.arange(seq_len, device=directions.device).unsqueeze(0).expand(batch_size, -1)
        pos_emb = self.position_embedding(positions)  # [batch_size, seq_len, emb_dim]
        
        # ç»„åˆtokenè¯­ä¹‰ç¼–ç å’Œä½ç½®ç¼–ç ï¼Œé€šè¿‡é€å…ƒç´ ç›¸åŠ ï¼ˆelement-wise sumï¼‰
        combined_emb = combined_emb + pos_emb
        
        # -------- æŠŠ mask ç”¨åœ¨ Transformer å’Œ AttentionPooling --------        
        # combined_emb = self.combined_norm(combined_emb)
        # Transformerç¼–ç  - ä½¿ç”¨æ­£ç¡®çš„æ©ç æ ¼å¼
        # src_key_padding_mask: Trueè¡¨ç¤ºéœ€è¦è¢«maskçš„ä½ç½®
        sequence_output = self.transformer(
            combined_emb,
            src_key_padding_mask=~sequence_mask.bool() if sequence_mask is not None else None  # src_key_padding_mask: Trueè¡¨ç¤ºéœ€è¦è¢«maskçš„ä½ç½®
        )
        
        # æ³¨æ„åŠ›æ± åŒ–å¾—åˆ°åºåˆ—è¡¨ç¤ºï¼Œè¾“å‡º pooled vector
        # queryï¼šåºåˆ—çš„å…¨å±€è¡¨ç¤ºï¼ˆå‡å€¼æˆ– learnable CLSï¼‰        
        query = torch.mean(sequence_output, dim=1, keepdim=True)  # [batch_size, 1, emb_dim]
        # å…³é”®ç‚¹ï¼šquery åªæœ‰ 1 ä¸ª tokenï¼Œkey/value æœ‰ L ä¸ª token        
        attn_output, attn_weights = self.attention_pooling(
            query, sequence_output, sequence_output, 
            key_padding_mask=~sequence_mask.bool() if sequence_mask is not None else None   # å–åï¼Œå› ä¸ºTransformeréœ€è¦Trueè¡¨ç¤ºmask
        )
        pooled_sequence_embedding = attn_output.squeeze(1)  # [B,H] = [batch_size, emb_dim]

        return {
            "sequence_embedding": pooled_sequence_embedding,
        }

class MultiViewFusionFactory:
    """å¤šè§†å›¾èåˆç­–ç•¥å·¥å‚"""
    
    @staticmethod
    def create_fusion_layer(cfg: DictConfig, hidden_size: int, num_views: int):
        fusion_method = cfg.model.multiview.fusion.method
        
        if fusion_method == "cross_attention":
            return CrossAttentionFusion(
                hidden_size=hidden_size,
                num_heads=cfg.model.multiview.fusion.cross_attention_heads,
                dropout=cfg.model.multiview.fusion.cross_attention_dropout,
                num_views=num_views
            )
        elif fusion_method == "weighted_sum":
            return WeightedSumFusion(
                hidden_size=hidden_size,
                num_views=num_views,
                learnable_weights=cfg.model.multiview.fusion.weighted_sum.learnable_weights,
                initial_weights=cfg.model.multiview.fusion.weighted_sum.initial_weights
            )
        elif fusion_method == "concat":
            return ConcatFusion(
                hidden_size=hidden_size,
                num_views=num_views,
            )
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„èåˆæ–¹æ³•: {fusion_method}")

class WeightedSumFusion(nn.Module):
    """åŠ æƒæ±‚å’Œå¤šè§†å›¾èåˆ"""
    
    def __init__(
        self, 
        hidden_size: int,
        num_views: int,
        learnable_weights: bool = True,
        initial_weights: List[float] = None
    ):
        super().__init__()
        self.hidden_size = hidden_size
        self.num_views = num_views
        
        # è§†å›¾æƒé‡
        if learnable_weights:
            if initial_weights is not None and len(initial_weights) == num_views:
                self.view_weights = nn.Parameter(torch.tensor(initial_weights, dtype=torch.float32))
            else:
                self.view_weights = nn.Parameter(torch.ones(num_views) / num_views)
        else:
            if initial_weights is not None and len(initial_weights) == num_views:
                self.register_buffer('view_weights', torch.tensor(initial_weights, dtype=torch.float32))
            else:
                self.register_buffer('view_weights', torch.ones(num_views) / num_views)
        
        # å¯é€‰çš„ç‰¹å¾å˜æ¢å±‚
        self.view_projections = nn.ModuleList([
            nn.Linear(hidden_size, hidden_size) for _ in range(num_views)
        ])
        
        # å±‚å½’ä¸€åŒ–
        self.layer_norm = nn.LayerNorm(hidden_size)
        
    def forward(self, view_embeddings: List[torch.Tensor]) -> torch.Tensor:
        # å¯¹æ¯ä¸ªè§†å›¾è¿›è¡ŒæŠ•å½±å˜æ¢
        projected_views = []
        for i, view in enumerate(view_embeddings):
            projected_view = self.view_projections[i](view)
            projected_views.append(projected_view)
        
        # åŠ æƒæ±‚å’Œ
        weights = F.softmax(self.view_weights, dim=0)
        fused = sum(weight * view for weight, view in zip(weights, projected_views))
        
        # å±‚å½’ä¸€åŒ–
        return self.layer_norm(fused)

class ConcatFusion(nn.Module):
    """ç®€åŒ–çš„æ‹¼æ¥å¤šè§†å›¾èåˆ"""
    
    def __init__(
        self, 
        hidden_size: int,
        num_views: int
    ):
        super().__init__()
        self.hidden_size = hidden_size
        self.num_views = num_views
        
        # æ‹¼æ¥åçš„æ€»ç»´åº¦ = hidden_size * num_views
        concat_dim = hidden_size * num_views
        
        self.projection = nn.Sequential(
            nn.Linear(concat_dim, hidden_size),  # ç›´æ¥æŠ•å½±åˆ° hidden_size
            nn.GELU(),
            nn.Dropout(0.1),
            nn.LayerNorm(hidden_size)
        )

        logger.info(f"[ConcatFusion] åˆå§‹åŒ–ï¼šè¾“å…¥ç»´åº¦ = {concat_dim}, è¾“å‡ºç»´åº¦ = {hidden_size}")
        
    def forward(self, view_embeddings: List[torch.Tensor]) -> torch.Tensor:
        # æ‹¼æ¥æ‰€æœ‰è§†å›¾ç‰¹å¾
        concatenated = torch.cat(view_embeddings, dim=1)

        # ---- æ‰“å°æ‹¼æ¥åçš„ç»´åº¦ ----
        # logger.info(f"[ConcatFusion] æ‹¼æ¥å concatenated.shape = {tuple(concatenated.shape)}")
                
        # ç›´æ¥æŠ•å½±åˆ°ç›®æ ‡ç»´åº¦
        fused = self.projection(concatenated)

        # ---- æ‰“å°æŠ•å½±è¾“å‡ºç»´åº¦ ----
        # logger.info(f"[ConcatFusion] æŠ•å½±å fused.shape = {tuple(fused.shape)}")
        
        return fused
    
class CrossAttentionFusion(nn.Module):
    """
    å¤šè§†å›¾äº¤å‰æ³¨æ„åŠ›èåˆå±‚ï¼Œæ”¯æŒè·å–æ³¨æ„åŠ›æƒé‡ã€‚
    ä¸ MultiViewFusionFactory.create_fusion_layer å®Œå…¨å…¼å®¹ã€‚
    """

    def __init__(
        self,
        hidden_size: int,
        num_heads: int,
        dropout: float = 0.1,
        num_views: int = 3,
    ):
        super().__init__()

        self.hidden_size = hidden_size
        self.num_heads = num_heads
        self.num_views = num_views
        self.dropout = dropout

        if hidden_size % num_heads != 0:
            raise ValueError(f"hidden_size={hidden_size} å¿…é¡»èƒ½è¢« num_heads={num_heads} æ•´é™¤")

        self.head_dim = hidden_size // num_heads

        # === ä¸ºæ¯ä¸ªè§†å›¾æ„å»º Q/K/V æ˜ å°„ ===
        self.query_proj = nn.ModuleList([nn.Linear(hidden_size, hidden_size) for _ in range(num_views)])
        self.key_proj   = nn.ModuleList([nn.Linear(hidden_size, hidden_size) for _ in range(num_views)])
        self.value_proj = nn.ModuleList([nn.Linear(hidden_size, hidden_size) for _ in range(num_views)])

        # === ä¸ºæ¯ä¸ªè§†å›¾çš„ æŸ¥è¯¢è§†å›¾ æ„å»ºä¸€ä¸ªäº¤å‰æ³¨æ„åŠ›æ¨¡å— ===
        self.cross_attn = nn.ModuleList([
            nn.MultiheadAttention(
                embed_dim=hidden_size,
                num_heads=num_heads,
                dropout=dropout,
                batch_first=True
            )
            for _ in range(num_views)
        ])

        # === è¾“å‡ºèåˆï¼ˆæ‹¼æ¥æ‰€æœ‰ attended viewï¼‰ ===
        self.output_linear = nn.Linear(hidden_size * num_views, hidden_size)
        self.dropout_layer = nn.Dropout(dropout)
        self.norm = nn.LayerNorm(hidden_size)

        self._init_weights()

    def _init_weights(self):
        for proj_list in [self.query_proj, self.key_proj, self.value_proj]:
            for proj in proj_list:
                nn.init.xavier_uniform_(proj.weight)
                nn.init.constant_(proj.bias, 0)
        nn.init.xavier_uniform_(self.output_linear.weight)
        nn.init.constant_(self.output_linear.bias, 0)

    def forward(self, view_embeddings: List[torch.Tensor]) -> torch.Tensor:
        """
        view_embeddings: List of [B, hidden_size]
        return: [B, hidden_size]
        """

        # è¿‡æ»¤æ‰ Noneï¼ˆä¾‹å¦‚ disabled viewï¼‰
        valid_views = [v for v in view_embeddings if v is not None]
        actual_num_views = len(valid_views)

        if actual_num_views == 0:
            raise ValueError("æ²¡æœ‰å¯ç”¨çš„è§†å›¾è¾“å…¥ï¼")

        batch_size = valid_views[0].size(0)

        attended_outputs = []

        for i in range(actual_num_views):
            # Query ä¸ºæœ¬è§†å›¾
            q = self.query_proj[i](valid_views[i]).unsqueeze(1)

            # å…¶ä»–è§†å›¾ä½œä¸º Key/Value
            other_indices = [j for j in range(actual_num_views) if j != i]

            k = torch.stack([self.key_proj[j](valid_views[j]) for j in other_indices], dim=1)
            v = torch.stack([self.value_proj[j](valid_views[j]) for j in other_indices], dim=1)

            out, _ = self.cross_attn[i](q, k, v, need_weights=False)
            attended_outputs.append(out.squeeze(1))

        # æ‹¼æ¥
        fused = torch.cat(attended_outputs, dim=1)

        # æŠ•å½±åˆ° hidden_size
        projected = self.output_linear(fused)

        # æ®‹å·®ï¼ˆä½¿ç”¨ç¬¬ä¸€ä¸ªè§†å›¾ï¼‰
        output = self.norm(valid_views[0] + self.dropout_layer(projected))

        return output

    def get_attention_weights(self, view_embeddings: List[torch.Tensor]) -> Dict[str, torch.Tensor]:
        """
        è¿”å›æ¯ä¸ªè§†å›¾ä½œä¸º Query æ—¶çš„ attention weight
        """
        valid_views = [v for v in view_embeddings if v is not None]
        actual_num_views = len(valid_views)

        weights = {}

        for i in range(actual_num_views):
            q = self.query_proj[i](valid_views[i]).unsqueeze(1)
            other_indices = [j for j in range(actual_num_views) if j != i]

            k = torch.stack([self.key_proj[j](valid_views[j]) for j in other_indices], dim=1)
            v = torch.stack([self.value_proj[j](valid_views[j]) for j in other_indices], dim=1)

            _, attn = self.cross_attn[i](q, k, v, need_weights=True)

            weights[f"view_{i}_attn"] = attn  # [B, 1, num_other]

        return weights
    

def get_project_root(start_path: str | None = None) -> str:
    import os, subprocess

    if start_path is None:
        start_path = os.path.abspath(os.path.dirname(__file__))

    # â‘  å°è¯•é€šè¿‡ Git
    try:
        root = subprocess.check_output(
            ["git", "rev-parse", "--show-toplevel"],
            stderr=subprocess.DEVNULL
        ).strip().decode("utf-8")
        return root
    except Exception:
        pass

    # â‘¡ å°è¯•æŸ¥æ‰¾å…³é”®æ–‡ä»¶
    markers = ("pyproject.toml", "setup.py", "requirements.txt", ".git")
    cur = start_path
    while True:
        if any(os.path.exists(os.path.join(cur, m)) for m in markers):
            return cur
        parent = os.path.abspath(os.path.join(cur, os.pardir))
        if parent == cur:
            break
        cur = parent

    # â‘¢ fallbackï¼šä½¿ç”¨ VSCode å·¥ä½œè·¯å¾„
    return os.environ.get("PWD", os.getcwd())

class FlowBertMultiview(pl.LightningModule):
    """å¤šè§†å›¾BERTæ¨¡å‹"""
    
    def __init__(self, cfg: DictConfig, dataset):
        
        # æ˜¾å¼åˆå§‹åŒ–ä¸¤ä¸ªçˆ¶ç±»ï¼Œé¿å…MROé—®é¢˜ ï¼Œ 2025-12-02 del by qinyf
        # pl.LightningModule.__init__(self)
        # SHAPAnalyzeMixin.__init__(self, cfg)

        super().__init__()
        
        # ä¿å­˜ cfgï¼Œä½†å¿½ç•¥ datasetï¼ˆä¸å¯åºåˆ—åŒ–ï¼‰
        self.save_hyperparameters(
            "cfg",
            logger=False,
            ignore=["dataset"]
        )
        self.cfg = cfg
        self.labels_cfg = cfg.datasets.labels

        # 1. ä» dataset è·å–ç±»åˆ«å‹ç‰¹å¾çš„æ˜ å°„å’Œæœ‰æ•ˆåˆ—
        if dataset is None:
            raise ValueError("dataset must be provided when initializing FlowBertMultiview "
                            "because categorical embeddings depend on dataset statistics.")
        
        self.categorical_val2idx_mappings = dataset.categorical_val2idx_mappings
        assert self.categorical_val2idx_mappings is not None, \
                "Model loaded without dataset stats â€” categorical val2idx embeddings invalid!"

        self.categorical_columns_effective = dataset.categorical_columns_effective
        assert self.categorical_columns_effective is not None, \
                "Model loaded without dataset stats â€” categorical columns effective invalid!"

        # 2. åˆå§‹åŒ– SHAP ç»„ä»¶ (æ”¾åœ¨ __init__ æœ€å)  2025-12-02 added by qinyf
        if self.cfg.shap.enable_shap:
            self.shap_analyzer = ShapAnalyzer(self)

        self.debug_mode = cfg.debug.debug_mode
        # ğŸ”´ æ ¹æ®debug_mode è®¾ç½® nan_check_enabled å±æ€§
        self.nan_check_enabled = getattr(cfg.debug, 'nan_check_enabled', self.debug_mode)

        # åˆå§‹åŒ–BERTæ¨¡å‹å’Œé…ç½®
        _require_transformers()
        self.bert, self.bert_config, self.tokenizer = self._load_bert_model(cfg)
        logger.info(f"åŠ è½½çš„BERTæ¨¡å‹çš„æ¯ä¸ª token çš„éšè—å‘é‡ç»´åº¦ï¼ˆhidden dimensionï¼‰ï¼šbert_config.hidden_size = {self.bert_config.hidden_size}")

        # === æ¦‚å¿µæ¼‚ç§»ç®¡ç†å™¨åˆå§‹åŒ– ===
        if DRIFT_MANAGER_AVAILABLE and hasattr(cfg, 'concept_drift') and cfg.concept_drift.enabled:
            logger.info(f"ğŸ¯ æ¦‚å¿µæ¼‚ç§»æ£€æµ‹åŠŸèƒ½å·²å¯ç”¨ (Algorithm: {cfg.concept_drift.get('algorithm', 'bndm')})")
            self.drift_enabled = True
            self.drift_manager = None  # å°†åœ¨ on_train_start ä¸­åˆå§‹åŒ–
        else:
            if hasattr(cfg, 'concept_drift') and not cfg.concept_drift.enabled:
                logger.info("â„¹ï¸ æ¦‚å¿µæ¼‚ç§»æ£€æµ‹åŠŸèƒ½å·²åœ¨é…ç½®ä¸­ç¦ç”¨")
            else:
                logger.info("â„¹ï¸ æœªæ£€æµ‹åˆ°æ¦‚å¿µæ¼‚ç§»é…ç½®æˆ–æ¨¡å—ä¸å¯ç”¨ï¼ŒåŠŸèƒ½ç¦ç”¨")
            self.drift_enabled = False
            self.drift_manager = None

        # æ£€æŸ¥å„è§†å›¾æ˜¯å¦å¯ç”¨
        self.text_features_enabled = False
        if hasattr(cfg.data, 'text_features') and cfg.data.text_features is not None:
            if hasattr(cfg.data.text_features, 'enabled') and cfg.data.text_features.enabled:
                self.text_features_enabled = True
                
        self.domain_embedding_enabled = False
        if hasattr(cfg.data, 'domain_name_embedding_features') and hasattr(cfg.data.domain_name_embedding_features, 'enabled') and hasattr(cfg.data.domain_name_embedding_features, 'column_list'):
            if cfg.data.domain_name_embedding_features.enabled and len(cfg.data.domain_name_embedding_features.column_list) > 0:
                self.domain_embedding_enabled = True
        
        self.sequence_features_enabled = False
        if hasattr(cfg.data, 'sequence_features') and cfg.data.sequence_features is not None:
            if hasattr(cfg.data.sequence_features, 'enabled') and cfg.data.sequence_features.enabled:
                self.sequence_features_enabled = True        

        logger.info(f"è§†å›¾å¯ç”¨çŠ¶æ€: æ•°å€¼ç‰¹å¾å‘é‡å¿…é€‰ï¼Œæ•°æ®åŒ…åºåˆ—={self.sequence_features_enabled}, æ–‡æœ¬={self.text_features_enabled}, åŸŸååµŒå…¥={self.domain_embedding_enabled}")
        # å®‰å…¨æ£€æŸ¥
        if self.text_features_enabled and not hasattr(self, 'bert_config'):
            raise ValueError("æ–‡æœ¬ç‰¹å¾å·²å¯ç”¨ä½†BERTé…ç½®æœªåˆå§‹åŒ–")

        # åˆå§‹åŒ–æ‰€æœ‰æŠ•å½±å±‚
        self._init_projection_layers(cfg)

        # è®¡ç®—å®é™…å¯ç”¨çš„è§†å›¾æ•°é‡
        self.num_views = 1  # è¡¨æ ¼æ•°æ®ç‰¹å¾ï¼šæ•°å€¼ç‰¹å¾ï¼ˆå¿…é€‰ï¼‰ + åŸŸååµŒå…¥ç‰¹å¾ï¼ˆå¯é€‰ï¼‰
        if self.text_features_enabled:
            self.num_views += 1 # æ–‡æœ¬è§†å›¾å¯é€‰
        if self.sequence_features_enabled:
            self.num_views += 1 # æ•°æ®åŒ…åºåˆ—è§†å›¾å¯é€‰
        
        logger.info(f"æ¨¡å‹ä½¿ç”¨çš„è§†å›¾æ•°é‡: {self.num_views}")
        logger.info(f"å¤šè§†å›¾èåˆæ–¹æ³•: {cfg.model.multiview.fusion.method}")
        
        # åˆå§‹åŒ–å¤šè§†å›¾èåˆå±‚
        self.fusion_layer = MultiViewFusionFactory.create_fusion_layer(
            cfg=cfg,
            hidden_size=self.bert_config.hidden_size,
            num_views=self.num_views
        )

        # åˆå§‹åŒ–åˆ†ç±»å™¨
        self._init_classifier(cfg)

        # åˆå§‹åŒ–åˆ†ç±»æŸå¤±å‡½æ•°
        self._init_loss_function(cfg)

        # SHAPåˆ†æç°åœ¨ç”±SHAPAnalyzeMixinç»Ÿä¸€ç®¡ç†ï¼Œæ— éœ€é‡å¤é…ç½®

        # =========================================================================
        # ğŸ”§ [æ–°å¢] è§£å†³æ¦‚å¿µæ¼‚ç§»å¯¼è‡´çš„ Embedding ç»´åº¦ä¸åŒ¹é…é—®é¢˜
        # =========================================================================
        def on_load_checkpoint(self, checkpoint: Dict[str, Any]) -> None:
            """
            åœ¨åŠ è½½æƒé‡å‰è§¦å‘ã€‚
            å¦‚æœåœ¨æµ‹è¯•/æ¼‚ç§»æ£€æµ‹é˜¶æ®µå‘ç°äº†æ–°ç±»åˆ«ï¼ˆå¯¼è‡´ Embedding å±‚å˜å¤§ï¼‰ï¼Œ
            è¿™é‡Œä¼šè‡ªåŠ¨å°† Checkpoint ä¸­çš„æ—§æƒé‡å¡«å……åˆ°æ–°æ¨¡å‹ä¸­ï¼Œæ–°ç±»åˆ«å¯¹åº”çš„æƒé‡ä¿æŒéšæœºåˆå§‹åŒ–ã€‚
            """
            state_dict = checkpoint["state_dict"]
            model_state_dict = self.state_dict()

            for key in list(state_dict.keys()):
                # ä»…å¤„ç† Embedding å±‚çš„ç»´åº¦ä¸åŒ¹é…é—®é¢˜
                if "categorical_embedding_layers" in key and key in model_state_dict:
                    ckpt_tensor = state_dict[key]
                    model_tensor = model_state_dict[key]

                    # å¦‚æœç»´åº¦ä¸ä¸€è‡´
                    if ckpt_tensor.shape != model_tensor.shape:
                        logger.warning(
                            f"âš ï¸ æ£€æµ‹åˆ° Embedding ç»´åº¦æ¼‚ç§» {key}: å­˜æ¡£ {ckpt_tensor.shape} vs å½“å‰ {model_tensor.shape}")

                        # ç¡®ä¿æ˜¯ç±»åˆ«æ•°é‡ï¼ˆè¡Œæ•°ï¼‰å‘ç”Ÿäº†å˜åŒ–ï¼Œä¸” Hidden Sizeï¼ˆåˆ—æ•°ï¼‰ä¸€è‡´
                        if ckpt_tensor.shape[1] == model_tensor.shape[1]:
                            old_rows = ckpt_tensor.shape[0]
                            new_rows = model_tensor.shape[0]

                            if new_rows > old_rows:
                                # ã€æƒ…å†µ1ï¼šæ–°æ•°æ®å‡ºç°äº†æ–°ç±»åˆ«ã€‘
                                # åˆ›å»ºä¸€ä¸ªä¸å½“å‰æ¨¡å‹å½¢çŠ¶ä¸€è‡´çš„ Tensor (åŒ…å«æ–°ç±»åˆ«çš„éšæœºåˆå§‹å€¼)
                                # æ³¨æ„ï¼šè¿™é‡Œç›´æ¥ä½¿ç”¨ model_tensor.clone() ä¼šå¤åˆ¶å½“å‰çš„éšæœºåˆå§‹åŒ–å€¼
                                new_ckpt_tensor = model_tensor.clone()

                                # å°†æ—§æƒé‡è¦†ç›–å›å» (ä¿ç•™è®­ç»ƒå¥½çš„çŸ¥è¯†)
                                new_ckpt_tensor[:old_rows] = ckpt_tensor

                                # æ›´æ–° checkpoint å­—å…¸ï¼Œéª—è¿‡ strict åŠ è½½æ£€æŸ¥
                                state_dict[key] = new_ckpt_tensor
                                logger.info(
                                    f"âœ… è‡ªåŠ¨æ‰©å±•æƒé‡ {key}: ä¿ç•™å‰ {old_rows} è¡Œæ—§æƒé‡ï¼Œæ–°å¢ {new_rows - old_rows} è¡Œéšæœºæƒé‡ç”¨äºå¢é‡å­¦ä¹ ã€‚")

                            elif new_rows < old_rows:
                                # ã€æƒ…å†µ2ï¼šå½“å‰æ•°æ®ç±»åˆ«å°‘äºè®­ç»ƒé›†ã€‘(è¾ƒå°‘è§ï¼Œä½†ä¸ºäº†å¥å£®æ€§å¤„ç†)
                                # ç›´æ¥æˆªå–æ—§æƒé‡çš„å‰ N è¡Œ
                                state_dict[key] = ckpt_tensor[:new_rows]
                                logger.info(f"âš ï¸ è‡ªåŠ¨è£å‰ªæƒé‡ {key}: {old_rows} -> {new_rows}")

    # =========================================================================
    # ğŸ”§ [æ–°å¢] è§£å†³æ¦‚å¿µæ¼‚ç§»å¯¼è‡´çš„ Embedding ç»´åº¦ä¸åŒ¹é…é—®é¢˜
    # =========================================================================
    def on_load_checkpoint(self, checkpoint: Dict[str, Any]) -> None:
        """
        åœ¨åŠ è½½æƒé‡å‰è§¦å‘ã€‚
        å¦‚æœåœ¨æµ‹è¯•/æ¼‚ç§»æ£€æµ‹é˜¶æ®µå‘ç°äº†æ–°ç±»åˆ«ï¼ˆå¯¼è‡´ Embedding å±‚å˜å¤§ï¼‰ï¼Œ
        è¿™é‡Œä¼šè‡ªåŠ¨å°† Checkpoint ä¸­çš„æ—§æƒé‡å¡«å……åˆ°æ–°æ¨¡å‹ä¸­ï¼Œæ–°ç±»åˆ«å¯¹åº”çš„æƒé‡ä¿æŒéšæœºåˆå§‹åŒ–ã€‚
        """
        state_dict = checkpoint["state_dict"]
        model_state_dict = self.state_dict()

        for key in list(state_dict.keys()):
            # ä»…å¤„ç† Embedding å±‚çš„ç»´åº¦ä¸åŒ¹é…é—®é¢˜
            if "categorical_embedding_layers" in key and key in model_state_dict:
                ckpt_tensor = state_dict[key]
                model_tensor = model_state_dict[key]

                # å¦‚æœç»´åº¦ä¸ä¸€è‡´
                if ckpt_tensor.shape != model_tensor.shape:
                    logger.warning(
                        f"âš ï¸ æ£€æµ‹åˆ° Embedding ç»´åº¦æ¼‚ç§» {key}: å­˜æ¡£ {ckpt_tensor.shape} vs å½“å‰ {model_tensor.shape}")

                    # ç¡®ä¿æ˜¯ç±»åˆ«æ•°é‡ï¼ˆè¡Œæ•°ï¼‰å‘ç”Ÿäº†å˜åŒ–ï¼Œä¸” Hidden Sizeï¼ˆåˆ—æ•°ï¼‰ä¸€è‡´
                    if ckpt_tensor.shape[1] == model_tensor.shape[1]:
                        old_rows = ckpt_tensor.shape[0]
                        new_rows = model_tensor.shape[0]

                        if new_rows > old_rows:
                            # ã€æƒ…å†µ1ï¼šæ–°æ•°æ®å‡ºç°äº†æ–°ç±»åˆ«ã€‘
                            # åˆ›å»ºä¸€ä¸ªä¸å½“å‰æ¨¡å‹å½¢çŠ¶ä¸€è‡´çš„ Tensor (åŒ…å«æ–°ç±»åˆ«çš„éšæœºåˆå§‹å€¼)
                            # æ³¨æ„ï¼šè¿™é‡Œç›´æ¥ä½¿ç”¨ model_tensor.clone() ä¼šå¤åˆ¶å½“å‰çš„éšæœºåˆå§‹åŒ–å€¼
                            new_ckpt_tensor = model_tensor.clone()

                            # å°†æ—§æƒé‡è¦†ç›–å›å» (ä¿ç•™è®­ç»ƒå¥½çš„çŸ¥è¯†)
                            new_ckpt_tensor[:old_rows] = ckpt_tensor

                            # æ›´æ–° checkpoint å­—å…¸ï¼Œéª—è¿‡ strict åŠ è½½æ£€æŸ¥
                            state_dict[key] = new_ckpt_tensor
                            logger.info(
                                f"âœ… è‡ªåŠ¨æ‰©å±•æƒé‡ {key}: ä¿ç•™å‰ {old_rows} è¡Œæ—§æƒé‡ï¼Œæ–°å¢ {new_rows - old_rows} è¡Œéšæœºæƒé‡ç”¨äºå¢é‡å­¦ä¹ ã€‚")

                        elif new_rows < old_rows:
                            # ã€æƒ…å†µ2ï¼šå½“å‰æ•°æ®ç±»åˆ«å°‘äºè®­ç»ƒé›†ã€‘(è¾ƒå°‘è§ï¼Œä½†ä¸ºäº†å¥å£®æ€§å¤„ç†)
                            # ç›´æ¥æˆªå–æ—§æƒé‡çš„å‰ N è¡Œ
                            state_dict[key] = ckpt_tensor[:new_rows]
                            logger.info(f"âš ï¸ è‡ªåŠ¨è£å‰ªæƒé‡ {key}: {old_rows} -> {new_rows}")


    @classmethod
    def load_from_checkpoint(cls, checkpoint_path, **kwargs):
        # å¼ºåˆ¶ä¼ é€’ dataset
        return super().load_from_checkpoint(checkpoint_path, **kwargs)

    def on_validation_epoch_end(self):
        """éªŒè¯é˜¶æ®µç»“æŸæ—¶æ£€æŸ¥æ¦‚å¿µæ¼‚ç§»"""
        super().on_validation_epoch_end()

        # å®šæœŸè¿›è¡Œæ¦‚å¿µæ¼‚ç§»æ£€æµ‹å’ŒçŠ¶æ€æŠ¥å‘Š
        if self.drift_detector is not None:
            # æ¯ä¸ªepochéƒ½æŠ¥å‘Šä¸€æ¬¡çŠ¶æ€
            stats = self.drift_detector.get_statistics()

            logger.info(f"ğŸ“ˆ æ¦‚å¿µæ¼‚ç§»æ£€æµ‹çŠ¶æ€ (Epoch {self.current_epoch}):")
            logger.info(f"  æ€»æ ·æœ¬æ•°: {stats['total_samples']}")
            logger.info(f"  æ£€æµ‹åˆ°æ¼‚ç§»æ¬¡æ•°: {stats['drift_count']}")
            logger.info(f"  å½“å‰çŠ¶æ€: {stats['status']}")
            logger.info(f"  çª—å£çŠ¶æ€: W={stats['window_W_size']}, R={stats['window_R_size']}")

            # å¦‚æœæ£€æµ‹åˆ°æ¼‚ç§»ï¼Œè®°å½•è¯¦ç»†ä¿¡æ¯
            if self.drift_detected:
                is_drift, B, info = self.drift_detector.detect_drift()
                if is_drift:
                    logger.warning(f"ğŸ”´ å½“å‰å­˜åœ¨æ¦‚å¿µæ¼‚ç§»!")
                    logger.warning(f"   è´å¶æ–¯å› å­B: {B:.6f}")
                    logger.warning(f"   æ¯”è¾ƒ: {info['comparison']}")

                self.drift_detected = False  # é‡ç½®æ£€æµ‹æ ‡å¿—

    def freeze_backbone(self):
        for param in self.flow_bert.parameters():
            param.requires_grad = False

    def unfreeze_backbone(self):
        for param in self.flow_bert.parameters():
            param.requires_grad = True

    def _load_bert_model(self, cfg: DictConfig) -> tuple:
        """
        åŠ è½½BERTæ¨¡å‹ã€é…ç½®å’Œtokenizer
        
        Args:
            cfg: é…ç½®å¯¹è±¡
            
        Returns:
            tuple: (bert_model, bert_config, tokenizer)
        """
        # æ·»åŠ å‚æ•°éªŒè¯
        if not hasattr(cfg.model.bert, 'model_name') or cfg.model.bert.model_name is None:
            raise ValueError("BERTæ¨¡å‹åç§°æœªåœ¨é…ç½®ä¸­è®¾ç½®ã€‚è¯·æ£€æŸ¥é…ç½®æ–‡ä»¶ä¸­çš„ bert.model_name å­—æ®µã€‚")
        
        logger.info(f"ä½¿ç”¨BERTæ¨¡å‹: {cfg.model.bert.model_name}")
        current_dir = os.path.dirname(os.path.abspath(__file__))
        project_root = os.path.abspath(os.path.join(current_dir, "../../../../"))
        model_path = os.path.join(project_root, 'models_hub', cfg.model.bert.model_name)
        
        try:
            # é¦–å…ˆå°è¯•ä»æœ¬åœ°ç¼“å­˜åŠ è½½
            logger.info("å°è¯•ä»æœ¬åœ°ç¼“å­˜åŠ è½½BERTæ¨¡å‹...")
            bert_config = BertConfig.from_pretrained(
                model_path,
                hidden_dropout_prob=cfg.model.bert.hidden_dropout_prob,
                attention_probs_dropout_prob=cfg.model.bert.attention_probs_dropout_prob)
            bert_model = BertModel.from_pretrained(model_path)
            tokenizer = BertTokenizer.from_pretrained(model_path)
            
            # ğŸ”´ ç¡®ä¿BERTæ¨¡å‹è®¾ç½®ä¸ºè®­ç»ƒæ¨¡å¼
            bert_model.train()
            logger.info(f"æˆåŠŸä»æœ¬åœ°ç¼“å­˜åŠ è½½BERTæ¨¡å‹ï¼Œè®¾ç½®ä¸ºè®­ç»ƒæ¨¡å¼")
            
        except (OSError, ValueError) as e:
            logger.warning(f"æœ¬åœ°æ¨¡å‹æœªæ‰¾åˆ°: {e}, å°è¯•åœ¨çº¿ä¸‹è½½...")
            logger.warning(f"å¦‚æœç½‘ç»œä¸å¯ç”¨ï¼Œè¯·æ‰‹åŠ¨ä¸‹è½½æ¨¡å‹å¹¶æ”¾ç½®åœ¨æœ¬åœ°è·¯å¾„: {model_path}")
            # å¦‚æœæœ¬åœ°æ²¡æœ‰ï¼Œä¸‹è½½å¹¶ä¿å­˜
            bert_config = BertConfig.from_pretrained(
                cfg.model.bert.model_name,
                hidden_dropout_prob=cfg.model.bert.hidden_dropout_prob,
                attention_probs_dropout_prob=cfg.model.bert.attention_probs_dropout_prob)
            bert_model = BertModel.from_pretrained(cfg.model.bert.model_name)
            tokenizer = BertTokenizer.from_pretrained(cfg.model.bert.model_name)
            
            # ğŸ”´ç¡®ä¿BERTæ¨¡å‹è®¾ç½®ä¸ºè®­ç»ƒæ¨¡å¼
            bert_model.train()

            # ä¿å­˜åˆ°æœ¬åœ°
            bert_config.save_pretrained(model_path)
            bert_model.save_pretrained(model_path)
            tokenizer.save_pretrained(model_path)
            logger.info("BERTæ¨¡å‹åœ¨çº¿ä¸‹è½½å¹¶ä¿å­˜åˆ°æœ¬åœ°å®Œæˆï¼Œè®¾ç½®ä¸ºè®­ç»ƒæ¨¡å¼")
        
        return bert_model, bert_config, tokenizer

    def _init_projection_layers(self, cfg: DictConfig):
        # ğŸ”´ 1. åˆå§‹åŒ–æ–‡æœ¬ç‰¹å¾æŠ•å½±å±‚ï¼ˆå¯é€‰ï¼‰
        if self.text_features_enabled:
            logger.info("åˆå§‹åŒ–æ–‡æœ¬ç‰¹å¾ç¼–ç å™¨")
        else:
            logger.info("è·³è¿‡æ–‡æœ¬ç‰¹å¾ç¼–ç å™¨åˆå§‹åŒ–")

        # ğŸ”´ 2. åˆå§‹åŒ–æ•°æ®åŒ…ç‰¹å¾æŠ•å½±å±‚ï¼ˆå¯é€‰ï¼‰
        if self.sequence_features_enabled:
            # å½“å‰çš„ä¸¤å±‚è®¾è®¡ï¼ˆåµŒå…¥å±‚ + æŠ•å½±å±‚ï¼‰
            self.sequence_encoder = SequenceEncoder(
                embedding_dim=cfg.model.sequence.embedding_dim, # embedding_dimå¯ä»¥ç‹¬ç«‹äº BERT çš„éšè—å±‚å¤§å°è¿›è¡Œè°ƒä¼˜
                num_layers=cfg.model.sequence.num_layers,
                num_heads=cfg.model.sequence.num_heads,
                dropout=cfg.model.sequence.dropout, 
                max_seq_length=cfg.data.max_seq_length
            )
            # sequence_projection æ˜¯æœ€è½»é‡çš„è·¨æ¨¡æ€ Adapter
            self.sequence_projection = nn.Linear(cfg.model.sequence.embedding_dim, self.bert_config.hidden_size)
            logger.info("åˆå§‹åŒ–æ•°æ®åŒ…åºåˆ—ç¼–ç å™¨")
        else:
            self.sequence_encoder = None
            self.sequence_projection = None
            logger.info("è·³è¿‡æ•°æ®åŒ…åºåˆ—ç¼–ç å™¨åˆå§‹åŒ–")

        # ğŸ”´ 3. åˆå§‹åŒ–åŸŸååµŒå…¥ç‰¹å¾ç»´åº¦ï¼ˆå¯é€‰ï¼‰ï¼Œä¸è€ƒè™‘
        if self.domain_embedding_enabled:
            label_id_map = ConfigManager.read_session_label_id_map(self.cfg.data.dataset)
            self.prob_list_length = len(label_id_map)
            logger.info(f"åŸŸååµŒå…¥ç‰¹å¾æ¦‚ç‡åˆ—è¡¨é•¿åº¦: {self.prob_list_length}")
            self.domain_feature_dim = len(cfg.data.domain_name_embedding_features.column_list) * self.prob_list_length
            logger.info(f"åŸŸååµŒå…¥ç‰¹å¾é•¿åº¦: {self.domain_feature_dim}")
        else:
            self.domain_feature_dim = 0
            logger.info(f"åŸŸååµŒå…¥ç‰¹å¾é•¿åº¦: {self.domain_feature_dim}")

        # 4. åˆå§‹åŒ–ç±»åˆ«å‹ç‰¹å¾åµŒå…¥å±‚ï¼ˆå¿…é€‰ï¼‰
        # ç±»åˆ«å‹ç‰¹å¾å§‹ç»ˆå¯ç”¨ï¼Œå› ä¸ºæ‰€æœ‰æµéƒ½æœ‰ conn.protoã€service ç­‰ç±»åˆ«è¯­ä¹‰
        self.categorical_embedding_layers = nn.ModuleDict()

        # â­ ä» dataset è¡¥å…… category â†’ index æ˜ å°„
        for col, mapping in self.categorical_val2idx_mappings.items():
            # num_classes = (æœ€å¤§ index) + 1
            # å› ä¸º index èŒƒå›´ä¸º [0 ... K]ï¼Œå…± K+1 ä¸ª embedding å‘é‡
            # å…¶ä¸­ index=0 ä¿ç•™ç»™ OOV ï¼ˆOut-Of-Vocabularyï¼‰
            num_classes = max(mapping.values()) + 1  
            self.categorical_embedding_layers[col] = nn.Embedding(
                num_embeddings=num_classes,
                embedding_dim=self.bert_config.hidden_size
            )
            logger.info(f"åˆå§‹åŒ– categorical embedding: {col} â†’ {num_classes} ç±»åˆ«")
        
        # ğŸ”¹ åˆå§‹åŒ– categorical LayerNormï¼ˆæ‹¼æ¥ååšå½’ä¸€åŒ–ï¼‰
        self.categorical_norm = nn.LayerNorm(
            normalized_shape=self.bert_config.hidden_size * len(self.categorical_columns_effective)
        )
        
        # 5. åˆå§‹åŒ–æ•°å€¼å‹ç‰¹å¾ï¼ˆå¿…é€‰ï¼‰+åŸŸååµŒå…¥ç‰¹å¾+ç±»åˆ«å‹ç‰¹å¾ï¼ˆå¿…é€‰ï¼‰çš„è¡¨æ ¼æ•°æ®æŠ•å½±å±‚
        # Delay to forward æ—¶è®¡ç®— tabular_feature_dim
        self.numeric_feature_dim = (
            len(cfg.data.tabular_features.numeric_features.flow_features)
            + len(cfg.data.tabular_features.numeric_features.x509_features)
            + len(cfg.data.tabular_features.numeric_features.dns_features)
        )
        logger.info(f"æ•°å€¼å‹æµç‰¹å¾æ•°: {self.numeric_feature_dim}")
        self.categorical_feature_dim = len(self.categorical_columns_effective) * self.bert_config.hidden_size
        logger.info(f"æ€»ç±»åˆ«å‹ç‰¹å¾æ•°: {len(self.categorical_columns_effective)}")
        logger.info(f"æ€»ç±»åˆ«å‹ç‰¹å¾ç»´åº¦: {self.bert_config.hidden_size} * {len(self.categorical_columns_effective)} = {self.bert_config.hidden_size * len(self.categorical_columns_effective)}")
        self.tabular_feature_dim = self.numeric_feature_dim + self.domain_feature_dim + self.categorical_feature_dim
        logger.info(f"è¡¨æ ¼æ•°æ®æ€»ç‰¹å¾ç»´åº¦: æ•°å€¼ç‰¹å¾({self.numeric_feature_dim}) + åŸŸååµŒå…¥({self.domain_feature_dim}) + ç±»åˆ«å‹ç‰¹å¾({self.categorical_feature_dim}) = {self.tabular_feature_dim}")

        self.tabular_projection = nn.Linear(
            self.tabular_feature_dim,
            self.bert_config.hidden_size
        )
        logger.info(f"åˆå§‹åŒ–è¡¨æ ¼ç‰¹å¾çº¿æ€§æŠ•å½±å±‚: è¾“å…¥ç»´åº¦={self.tabular_feature_dim}, è¾“å‡ºç»´åº¦={self.bert_config.hidden_size}")

    def _init_classifier(self, cfg: DictConfig):
        """åˆå§‹åŒ–åˆ†ç±»å™¨ - æ ¹æ®èåˆæ–¹æ³•è°ƒæ•´è¾“å…¥ç»´åº¦"""
        self.classifier_input_dim = self._get_classifier_input_dim(cfg)
        self._init_is_malicious_classifier(cfg)
        self._init_attack_family_classifier(cfg)

    def _get_classifier_input_dim(self, cfg: DictConfig) -> int:
        fusion_method = cfg.model.multiview.fusion.method

        if fusion_method == "concat":
            # å¯¹äºconcatæ–¹æ³•ï¼Œåˆ†ç±»å™¨è¾“å…¥ç»´åº¦æ˜¯æ‰€æœ‰å¯ç”¨è§†å›¾çš„ç»´åº¦ä¹‹å’Œ
            # classifier_input_dim = self.bert_config.hidden_size  # æ•°å€¼ç‰¹å¾æŠ•å½±åçš„ç»´åº¦ï¼ˆå¿…é€‰ï¼‰
            
            # if self.sequence_features_enabled:
            #     classifier_input_dim += self.bert_config.hidden_size
            # if self.text_features_enabled:
            #     classifier_input_dim += self.bert_config.hidden_size
            # if self.domain_embedding_enabled:
            #     classifier_input_dim += self.bert_config.hidden_size

            # logger.info(f"è§†å›¾å¯ç”¨çŠ¶æ€: æ•°å€¼ç‰¹å¾å‘é‡å¿…é€‰ï¼Œæ•°æ®åŒ…åºåˆ—={self.sequence_features_enabled}, æ–‡æœ¬={self.text_features_enabled}, åŸŸååµŒå…¥={self.domain_embedding_enabled}")
            # logger.info(f"æ‹¼æ¥èåˆæ€»ç»´åº¦: {classifier_input_dim}")

            # ğŸ”´ å››ä¸ªè§†å›¾çš„concatï¼Œæ”¹æˆäº†ConcatFusionï¼Œå…¶å†…éƒ¨åšäº†æ‹¼æ¥è§£é‡Šç‰¹å¾çš„çº¿æ€§æŠ•å½±åˆ° hidden_size
            classifier_input_dim = self.bert_config.hidden_size
            logger.warning(
                f"[Fusion Warning] 'concat' èåˆåŸå§‹ç‰¹å¾ç»´åº¦ = bert_config.hidden_size * num_views "
                f"ä½† ConcatFusion ä¼šè‡ªåŠ¨æŠ•å½±å› bert_config.hidden_sizeï¼Œå› æ­¤åˆ†ç±»å™¨è¾“å…¥ç»´åº¦å›ºå®šä¸º bert_config.hidden_size = {self.bert_config.hidden_size} "
                f"(num_views={self.num_views})"
            )
            return classifier_input_dim
        else:
            # å…¶ä»–æ–¹æ³•éƒ½è¾“å‡º hidden_size ç»´åº¦
            classifier_input_dim = self.bert_config.hidden_size
            logger.info(f"[Fusion Info] ä½¿ç”¨ {fusion_method} èåˆï¼Œè¾“å‡ºç»´åº¦ = bert_config.hidden_size = {self.bert_config.hidden_size}")
            return classifier_input_dim

    def _init_is_malicious_classifier(self, cfg: DictConfig):
        is_malicious_classifier_cfg = cfg.datasets.tasks.outputs.is_malicious.classifier

        logger.info(f"is_maliciouså–„æ„/æ¶æ„æµé‡åˆ†ç±»å™¨çš„è¾“å…¥ç»´åº¦: {self.classifier_input_dim} ï¼Œ"
                    f"hidden_dims={is_malicious_classifier_cfg.hidden_dims}")


        # æ·»åŠ  is_maliciousï¼ˆäºŒåˆ†ç±»ï¼Œå–„æ„ / æ¶æ„ï¼‰çš„ flow-level åˆ†ç±»å™¨éšè—å±‚
        is_malicious_classifier_layers = []
        current_dim = self.classifier_input_dim
        for i, hidden_dim in enumerate(is_malicious_classifier_cfg.hidden_dims):
            logger.info(f"is_maliciouså–„æ„/æ¶æ„åˆ†ç±»å™¨éšè—å±‚ {i+1}: {current_dim} -> {hidden_dim}")
            
            is_malicious_classifier_layers.extend([
                nn.Linear(current_dim, hidden_dim),
                nn.GELU() if is_malicious_classifier_cfg.activation == "gelu" else nn.ReLU(),
                nn.Dropout(is_malicious_classifier_cfg.dropout),
                nn.LayerNorm(hidden_dim)
            ])
            current_dim = hidden_dim  # æ›´æ–°å½“å‰ç»´åº¦
        
        # æ·»åŠ is_maliciouså–„æ„/æ¶æ„æµé‡åˆ†ç±»å™¨çš„è¾“å‡ºå±‚
        # è¾“å‡ºå±‚ï¼š1 ç»´ï¼Œå¯¹åº” BCEWithLogitsLoss
        is_malicious_classifier_layers.append(nn.Linear(current_dim, 1))

        self.is_malicious_classifier = nn.Sequential(*is_malicious_classifier_layers)

        logger.info(
            f"is_malicious åˆ†ç±»å™¨ç»“æ„: "
            f"input_dim={self.classifier_input_dim} -> "
            f"hidden_dims={is_malicious_classifier_cfg.hidden_dims} -> "
            f"output_dim=1 (binary classification)"
        )


    def _init_attack_family_classifier(self, cfg: DictConfig):
        """
        åˆå§‹åŒ– attack_family åˆ†ç±»å™¨ï¼ˆOVR å¤šåˆ†ç±»ï¼‰

        attack_family é‡‡ç”¨ One-vs-Rest (OVR) å½¢å¼ï¼š
        - æ¯ä¸ªæ”»å‡»å®¶æ—å¯¹åº”ä¸€ä¸ªäºŒåˆ†ç±»å™¨
        - è¾“å‡ºç»´åº¦ = æ”»å‡»å®¶æ—æ•°é‡
        - é…åˆ BCEWithLogitsLoss + multi-hot æ ‡ç­¾ä½¿ç”¨
        """

        # ---------- 1. é…ç½®æ£€æŸ¥ ----------
        if "attack_family" not in cfg.datasets.tasks.outputs:
            logger.info("æœªé…ç½® attack_family ä»»åŠ¡ï¼Œè·³è¿‡ attack_family åˆ†ç±»å™¨åˆå§‹åŒ–")
            self.attack_family_classifier = None
            return

        attack_family_cfg = cfg.datasets.tasks.outputs.attack_family
        assert attack_family_cfg.strategy == "ovr", \
            f"attack_family ç›®å‰ä»…æ”¯æŒ OVR ç­–ç•¥ï¼Œå½“å‰é…ç½®ä¸º {attack_family_cfg.strategy}"

        # ---------- 2. è§£ææ”»å‡»å®¶æ—ä¿¡æ¯ ----------
        assert "attack_family" in self.labels_cfg, \
            "attack_family å·²å¯ç”¨ï¼Œä½† labels_cfg.attack_family æœªå®šä¹‰"

        # 1ï¸âƒ£ ç±»åˆ«å®šä¹‰ï¼šå”¯ä¸€æ¥æº
        self.attack_family_classes = [
            c.strip() for c in self.labels_cfg.attack_family.classes
        ]

        # 2ï¸âƒ£ æ¨¡å‹ / loss / logits ç»Ÿä¸€ä½¿ç”¨åŒä¸€é¡ºåº
        self.attack_family_names = self.attack_family_classes
        num_fam = len(self.attack_family_names)

        logger.info(
            f"[attack_family] å¯ç”¨ OVR æ”»å‡»å®¶æ—åˆ†ç±»ä»»åŠ¡ï¼Œå…± {num_fam} ä¸ªæ”»å‡»å®¶æ—: "
            f"{self.attack_family_names}"
        )

        # 3ï¸âƒ£ æ ¡éªŒ class_weights æ˜¯å¦ä¸ labels_cfg ä¸€è‡´ï¼ˆfail-fastï¼‰
        weight_keys = list(attack_family_cfg.class_weights.keys())
        if set(weight_keys) != set(self.attack_family_names):
            raise ValueError(
                "[attack_family] labels_cfg.attack_family.classes ä¸ "
                "task.outputs.attack_family.class_weights é”®ä¸ä¸€è‡´!\n"
                f"labels_cfg: {self.attack_family_names}\n"
                f"class_weights: {weight_keys}"
            )

        # ---------- 3. åˆ†ç±»å™¨è¾“å…¥ç»´åº¦ ----------
        input_dim = self.classifier_input_dim
        logger.info(
            f"[attack_family] åˆ†ç±»å™¨è¾“å…¥ç»´åº¦ = {input_dim} "
            f"(æ¥è‡ªå¤šè§†å›¾èåˆè¾“å‡º)"
        )

        # ---------- 4. æ„å»º OVR åˆ†ç±»å™¨ç½‘ç»œ ----------
        layers = []
        cur = input_dim

        for i, h in enumerate(attack_family_cfg.classifier.hidden_dims):
            logger.info(
                f"[attack_family] åˆ†ç±»å™¨éšè—å±‚ {i+1}: {cur} -> {h}"
            )
            layers.extend([
                nn.Linear(cur, h),
                nn.GELU(),
                nn.Dropout(attack_family_cfg.classifier.dropout),
                nn.LayerNorm(h),
            ])
            cur = h

        # ---------- 5. è¾“å‡ºå±‚ ----------
        # è¾“å‡ºç»´åº¦ = æ”»å‡»å®¶æ—æ•°é‡ï¼ˆOVRï¼Œæ¯ä¸€ç»´å¯¹åº”ä¸€ä¸ªå®¶æ—ï¼‰
        layers.append(nn.Linear(cur, num_fam))

        self.attack_family_classifier = nn.Sequential(*layers)

        logger.info(
            f"[attack_family] OVR åˆ†ç±»å™¨ç»“æ„æ„å»ºå®Œæˆ: "
            f"input_dim={input_dim} -> "
            f"hidden_dims={attack_family_cfg.classifier.hidden_dims} -> "
            f"output_dim={num_fam} (OVR, multi-label)"
        )


    def _init_loss_function(self, cfg: DictConfig):
        self._init_is_malicious_loss_function(cfg)
        self._init_attack_family_loss_function(cfg)

    def _init_is_malicious_loss_function(self, cfg: DictConfig):
        """
        åˆå§‹åŒ– is_maliciousï¼ˆäºŒåˆ†ç±»ï¼Œå–„æ„/æ¶æ„ï¼‰çš„æŸå¤±å‡½æ•°
        ä½¿ç”¨ BCEWithLogitsLoss + ç±»åˆ«æƒé‡
        """
        # åˆå§‹åŒ–åˆ†ç±»æŸå¤±å‡½æ•°
        self.is_malicious_class_loss = nn.BCEWithLogitsLoss(reduction='none')

        # ğŸ”¹ é»˜è®¤æƒé‡ï¼ˆä¸åŠ æƒï¼‰
        class_weights = [1.0, 1.0]

        # ğŸ”¹ ä» task.outputs ä¸­è¯»å–ï¼ˆå¦‚æœé…ç½®äº†ï¼‰
        try:
            task_cfg = cfg.datasets.tasks.outputs.get("is_malicious", None)
            if task_cfg is not None and "class_weights" in task_cfg:
                cw = task_cfg.class_weights
                if isinstance(cw, (list, tuple)) and len(cw) == 2:
                    class_weights = list(map(float, cw))
        except Exception as e:
            logger.warning(f"è¯»å– task.outputs.is_malicious.class_weights å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤æƒé‡: {e}")

        if class_weights is None or len(class_weights) != 2:
            logger.warning("[is_malicious] class_weights éæ³•ï¼Œé‡ç½®ä¸º [1.0, 1.0]")
            # ä½¿ç”¨é»˜è®¤æƒé‡ [1.0, 1.0]
            class_weights = [1.0, 1.0]

        self.is_malicious_class_weights = torch.tensor(class_weights, dtype=torch.float32)
        logger.info(
            f"[is_malicious] æŸå¤±å‡½æ•°åˆå§‹åŒ–å®Œæˆ: "
            f"BCEWithLogitsLoss, class_weights={class_weights}"
        )

    def _init_attack_family_loss_function(self, cfg: DictConfig):
        """
        åˆå§‹åŒ– attack_family çš„ OVR å¤šåˆ†ç±»æŸå¤±å‡½æ•°
        - æ¯ä¸ªæ”»å‡»å®¶æ—ä¸€ä¸ª BCE loss
        - ä½¿ç”¨ per-family [neg, pos] æƒé‡
        """

        if not hasattr(self, "attack_family_classifier") or \
        self.attack_family_classifier is None:
            self.attack_family_loss_fn = None
            self.attack_family_class_weights = None
            logger.info("[attack_family] æœªå¯ç”¨ä»»åŠ¡ï¼Œè·³è¿‡æŸå¤±å‡½æ•°åˆå§‹åŒ–")
            return

        attack_family_cfg = cfg.datasets.tasks.outputs.attack_family

        self.attack_family_loss_fn = nn.BCEWithLogitsLoss(reduction="none")

        # æ„å»ºæƒé‡çŸ©é˜µ [num_families, 2]
        weights = []
        for fam in self.attack_family_names:
            w = attack_family_cfg.class_weights.get(fam, [1.0, 1.0])
            if len(w) != 2:
                logger.warning(
                    f"[attack_family] å®¶æ— {fam} çš„ class_weights éæ³•ï¼Œä½¿ç”¨é»˜è®¤ [1.0, 1.0]"
                )
                w = [1.0, 1.0]
            weights.append(w)

        self.attack_family_class_weights = torch.tensor(
            weights, dtype=torch.float32
        )

        logger.info(
            f"[attack_family] OVR æŸå¤±å‡½æ•°åˆå§‹åŒ–å®Œæˆ: "
            f"BCEWithLogitsLoss, "
            f"class_weights shape={self.attack_family_class_weights.shape}"
        )

    def _compute_losses(self, outputs, batch, stage: str):
        """è®¡ç®—åˆ†ç±»æŸå¤± + tabularç‰¹å¾é‡å»ºæŸå¤± + æ€»æŸå¤±"""

        # åˆå§‹åŒ–æ‰€æœ‰æŸå¤±å˜é‡
        total_loss = torch.tensor(0.0, device=self.device)

        # ===== is_malicious åˆ†ç±»æŸå¤± =====
        is_malicious_cls_logits = outputs['is_malicious_cls_logits']
        # is_malicious_prob = torch.sigmoid(is_malicious_cls_logits)
        # is_malicious_pred = (is_malicious_prob > 0.5).float()
        is_malicious_label = batch['is_malicious_label']
        is_malicious_class_loss = self._compute_is_malicious_class_loss(is_malicious_cls_logits, is_malicious_label)

        # æ£€æŸ¥is_maliciousåˆ†ç±»æŸå¤±å€¼
        if torch.isnan(is_malicious_class_loss) or torch.isinf(is_malicious_class_loss):
            logger.error(f"ğŸš¨ {stage}æŸå¤±å€¼ä¸ºNaNæˆ–Inf: {is_malicious_class_loss}")
            # å°è¯•ä½¿ç”¨å°æŸå¤±å€¼ç»§ç»­è®­ç»ƒ
            is_malicious_class_loss = torch.tensor(1.0, device=self.device, requires_grad=True)

        is_malicious_weight = getattr(self.cfg.datasets.tasks.outputs.is_malicious, "weight", 1.0)
        total_loss = total_loss + is_malicious_weight * is_malicious_class_loss

        # ===== attack_family åˆ†ç±»æŸå¤± =====
        attack_family_class_loss = None
        if self.attack_family_classifier is not None:
            attack_family_cls_logits = outputs["attack_family_cls_logits"]
            assert "attack_family_label" in batch, \
                f"attack_family_label not found in batch keys: {batch.keys()}"
            attack_family_label = batch["attack_family_label"]
            attack_family_class_loss = self._compute_attack_family_class_loss(attack_family_cls_logits, attack_family_label, is_malicious_label)
            # æ£€æŸ¥attack_familyåˆ†ç±»æŸå¤±å€¼
            if torch.isnan(attack_family_class_loss) or torch.isinf(attack_family_class_loss):
                logger.error(f"ğŸš¨ {stage} attack_family_loss ä¸º NaN/Inf: {attack_family_class_loss}")
                attack_family_class_loss = torch.tensor(
                    1.0, device=self.device, requires_grad=True
                )
            attack_family_weight = getattr(self.cfg.datasets.tasks.outputs.attack_family, "weight", 1.0)
            total_loss = total_loss + attack_family_weight * attack_family_class_loss

        return {
            "total_loss": total_loss,
            "is_malicious_class_loss": is_malicious_class_loss,
            "attack_family_class_loss": attack_family_class_loss,
        }

    def _compute_is_malicious_class_loss(self, logits, labels):
        """è®¡ç®—åˆ†ç±»æŸå¤±"""
        # ç¡®ä¿æ ‡ç­¾å½¢çŠ¶æ­£ç¡® [batch_size, 1]
        if labels.dim() == 1:
            labels = labels.unsqueeze(1)

        # åŸºæœ¬æŸå¤±è®¡ç®—
        base_loss = self.is_malicious_class_loss(logits, labels)

        # ç¡®ä¿ is_malicious_class_weights å·²åˆå§‹åŒ–
        if self.is_malicious_class_weights is not None:
            # åº”ç”¨ç±»åˆ«æƒé‡
            binary_weights = self.is_malicious_class_weights.to(logits.device) # âœ… ç¡®ä¿æƒé‡åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
            binary_weights = torch.where(
                labels == 1,
                binary_weights[1],  # is_malicious=1 çš„æƒé‡
                binary_weights[0],  # is_malicious=0 çš„æƒé‡
            )
            weighted_loss = base_loss * binary_weights

            return weighted_loss.mean()
        else:
            return base_loss.mean()

    def _compute_attack_family_class_loss(self, logits, label, is_malicious_label):
        """
        è®¡ç®— attack_family çš„ OVR å¤šåˆ†ç±»æŸå¤±ï¼ˆæ¡ä»¶æŸå¤±ï¼‰

        è¯´æ˜ï¼š
        - attack_family ä»…åœ¨çœŸå®æ¶æ„æ ·æœ¬ï¼ˆis_malicious_label == 1ï¼‰ä¸Šå…·æœ‰è¯­ä¹‰å®šä¹‰ï¼›
        - benign æ ·æœ¬åœ¨ attack_family ç»´åº¦ä¸Šå±äº not applicableï¼Œ
        ä¸å‚ä¸ attack_family çš„æŸå¤±è®¡ç®—ä¸æ¢¯åº¦åä¼ ï¼›
        - å› æ­¤ï¼Œè¯¥æŸå¤±å‡½æ•°åˆ»ç”»çš„æ˜¯æ¡ä»¶åˆ†å¸ƒä¸‹çš„åˆ†ç±»æ€§èƒ½ï¼š
            P(attack_family | is_malicious = 1)ã€‚

        å‚æ•°è¯´æ˜ï¼š
        - logits: [B, K]
            attack_family åˆ†ç±»å™¨çš„åŸå§‹è¾“å‡ºï¼ˆæ¯ä¸ªæ”»å‡»å®¶æ—ä¸€ä¸ª OVR logitï¼‰
        - label: [B, K]ï¼ˆmulti-hotï¼‰
            attack_family çš„çœŸå®æ ‡ç­¾ï¼ˆOVR / multi-label è¡¨ç¤ºï¼‰
        - is_malicious_label: [B]ï¼ˆ0/1ï¼‰
            is_malicious çš„çœŸå®æ ‡ç­¾ï¼Œç”¨äºç­›é€‰å…·æœ‰ attack_family è¯­ä¹‰çš„æ ·æœ¬
        """
        assert logits.shape == label.shape, \
            f"attack_family logits/labels shape mismatch: {logits.shape} vs {label.shape}"

        # ä»…ä¿ç•™çœŸå®æ¶æ„æ ·æœ¬ï¼ˆground truthï¼‰ï¼Œåœ¨è¯¥å­é›†ä¸Šè®¡ç®— attack_family æŸå¤±
        # æ³¨æ„ï¼šè¿™é‡Œä½¿ç”¨çš„æ˜¯ is_malicious çš„çœŸå®æ ‡ç­¾ï¼Œè€Œä¸æ˜¯æ¨¡å‹é¢„æµ‹ç»“æœï¼Œ
        # ä»¥é¿å…çº§è”è¯¯å·®ï¼ˆcascaded errorï¼‰å¯¹ family å­¦ä¹ çš„å½±å“ã€‚
        mask = is_malicious_label.view(-1) == 1
        if mask.sum() == 0:
            # å½“å‰ batch ä¸­æ²¡æœ‰çœŸå®æ¶æ„æ ·æœ¬ï¼š
            # attack_family åœ¨è¯¥ batch ä¸Šä¸å®šä¹‰ï¼Œè¿”å› 0 lossï¼ˆä¸äº§ç”Ÿæ¢¯åº¦ï¼‰
            return torch.tensor(0.0, device=logits.device, requires_grad=True)

        logits = logits[mask]
        label = label[mask].float()  # BCE loss è¦æ±‚ label ä¸º float

        # åŸºç¡€ BCE æŸå¤±ï¼ˆOVRï¼‰ï¼š[B_malicious, K]
        base_loss = self.attack_family_loss_fn(logits, label)

        if self.attack_family_class_weights is not None:
            # class_weights å½¢çŠ¶ä¸º [K, 2]ï¼Œè¡¨ç¤ºæ¯ä¸ªæ”»å‡»å®¶æ—çš„ [neg, pos] æƒé‡
            # ç”¨äºç¼“è§£ä¸åŒ attack_family ä¹‹é—´çš„ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜
            weights = self.attack_family_class_weights.to(logits.device)

            # å¯¹æ¯ä¸ª OVR ç»´åº¦ï¼š
            # - label == 1 æ—¶ä½¿ç”¨æ­£ç±»æƒé‡ï¼ˆposï¼‰
            # - label == 0 æ—¶ä½¿ç”¨è´Ÿç±»æƒé‡ï¼ˆnegï¼‰
            # é€šè¿‡ broadcasting æ‰©å±•åˆ° [B_malicious, K]
            weighted = torch.where(
                label == 1,
                weights[:, 1],
                weights[:, 0]
            )

            loss = base_loss * weighted
            return loss.mean()
        else:
            return base_loss.mean()

    def _process_text_features(self, batch: Dict[str, Any]) -> torch.Tensor:
        """å¤„ç†æ–‡æœ¬ç‰¹å¾ï¼Œè¿”å›[CLS] tokenåµŒå…¥"""
        if not self.text_features_enabled:
            # è¿”å›é›¶å‘é‡å ä½ç¬¦
            batch_size = batch['numeric_features'].shape[0] # åˆ©ç”¨å¿…å¤‡çš„numeric_featuresç»´åº¦
            return torch.zeros(batch_size, self.bert_config.hidden_size if self.bert_config else 512, 
                            device=self.device)

        combined_texts = batch['combined_text']
        
        encoding = self.tokenizer(
            combined_texts,
            padding=True,
            truncation=True,
            max_length=self.cfg.data.max_seq_length,
            return_tensors='pt'
        )
        
        input_ids = encoding['input_ids'].to(self.device)
        attention_mask = encoding['attention_mask'].to(self.device)
        
        text_outputs = self.bert(
            input_ids=input_ids,
            attention_mask=attention_mask,
            return_dict=True
        )
        
        return text_outputs.last_hidden_state[:, 0]  # è¿”å›[CLS] tokenåµŒå…¥

    # ---------------- å®‰å…¨è®­ç»ƒæ¨¡å¼ ----------------
    def on_train_start(self) -> None:
        """ç¡®ä¿æ‰€æœ‰å­æ¨¡å—éƒ½åœ¨ train æ¨¡å¼"""
        super().on_train_start()

        # è®¾ç½®æ‰€æœ‰æ¨¡å—ä¸ºè®­ç»ƒæ¨¡å¼
        self.train()

        # è®¾ç½®åºåˆ—ç¼–ç å™¨è®­ç»ƒæ¨¡å¼ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if hasattr(self, 'sequence_encoder') and self.sequence_encoder is not None:
            self.sequence_encoder.train()
            if self.debug_mode:
                logger.info("åºåˆ—ç¼–ç å™¨è®¾ç½®ä¸ºè®­ç»ƒæ¨¡å¼")

        # è®¾ç½®ç”¨äºæ–‡æœ¬ç‰¹å¾è¡¨å¾çš„BERTï¼ˆå¦‚æœå­˜åœ¨ï¼‰ï¼Œè¿›å…¥è®­ç»ƒæ¨¡å¼
        if self.bert is not None:
            self.bert.train()

        # è®¾ç½®æ•°å€¼æŠ•å½±å±‚è®­ç»ƒæ¨¡å¼ï¼ˆå¿…é€‰ï¼‰
        if hasattr(self, 'tabular_projection') and self.tabular_projection is not None:
            self.tabular_projection.train()
            if self.debug_mode:
                logger.info("è¡¨æ ¼ç‰¹å¾çš„æŠ•å½±å±‚è®¾ç½®ä¸ºè®­ç»ƒæ¨¡å¼")
        
        # è®¾ç½®åˆ†ç±»å™¨è®­ç»ƒæ¨¡å¼
        if hasattr(self, 'is_malicious_classifier') and self.is_malicious_classifier is not None:
            self.is_malicious_classifier.train()

        if hasattr(self, 'attack_family_classifier') and self.attack_family_classifier is not None:
            self.attack_family_classifier.train()

        # è®¾ç½®æ‰€æœ‰æŠ•å½±å±‚ä¸ºè®­ç»ƒæ¨¡å¼
        for name, module in self.named_modules():
            if isinstance(module, (nn.Linear, nn.LayerNorm, nn.Dropout)):
                module.train()

        # é¢å¤–æ£€æŸ¥ï¼šåˆ—å‡ºæ‰€æœ‰æ¨¡å—çš„è®­ç»ƒçŠ¶æ€
        if self.debug_mode:
            eval_modules = []
            for name, module in self.named_modules():
                if not module.training:
                    eval_modules.append(name)

            if eval_modules:
                logger.warning(f"ä»¥ä¸‹æ¨¡å—ä»åœ¨è¯„ä¼°æ¨¡å¼: {eval_modules}")
            else:
                logger.info("âœ… æ‰€æœ‰æ¨¡å—éƒ½å·²æ­£ç¡®è®¾ç½®ä¸ºè®­ç»ƒæ¨¡å¼")

        if self.drift_enabled and self.drift_manager is not None:
            logger.info("â„¹ï¸ è®­ç»ƒé˜¶æ®µç¦ç”¨æ¦‚å¿µæ¼‚ç§»æ£€æµ‹ (Manager will be ignored during training)")


    # ---------------- NaN æ£€æŸ¥ ----------------
    def _safe_check_nan(self, x: torch.Tensor, name: str, operation: str = "") -> torch.Tensor:
        """å®‰å…¨çš„NaNæ£€æŸ¥ï¼Œä¸ä¼šå¯¼è‡´è®­ç»ƒå´©æºƒ"""
        if not self.debug_mode:
            return x
            
        if torch.isnan(x).any():
            nan_count = torch.isnan(x).sum().item()
            logger.warning(f"âš ï¸ {name} åœ¨ {operation} ååŒ…å« {nan_count} ä¸ªNaNå€¼")
            # å°è¯•ä¿®å¤ï¼šå°†NaNæ›¿æ¢ä¸º0
            x = torch.where(torch.isnan(x), torch.zeros_like(x), x)
            
        if torch.isinf(x).any():
            inf_count = torch.isinf(x).sum().item()
            logger.warning(f"âš ï¸ {name} åœ¨ {operation} ååŒ…å« {inf_count} ä¸ªInfå€¼")
            # å°è¯•ä¿®å¤ï¼šå°†Infæ›¿æ¢ä¸ºæœ‰é™å¤§å€¼
            x = torch.where(torch.isinf(x), torch.finfo(x.dtype).max * torch.ones_like(x), x)
            
        return x
    
    def _debug_tensor(self, x: torch.Tensor, name: str):
        """è°ƒè¯•å¼ é‡ä¿¡æ¯"""
        if not self.nan_check_enabled:
            return

        stats = {
            "shape": x.shape,
            "min": x.min().item(),
            "max": x.max().item(), 
            "mean": x.mean().item() if x.dtype.is_floating_point else 0.0,
            "std": x.std(unbiased=False).item() if (x.dtype.is_floating_point and x.numel() >= 2) else 0.0,
            "nan_count": torch.isnan(x).sum().item(),
            "inf_count": torch.isinf(x).sum().item()
        }
        
        # åªæœ‰å½“æœ‰NaNæˆ–Infæ—¶æ‰è¾“å‡ºè­¦å‘Š
        if stats["nan_count"] > 0 or stats["inf_count"] > 0:
            msg = f"ğŸ”ğŸ” {name}: " + " | ".join([f"{k}: {v:.6f}" if isinstance(v, float) else f"{k}: {v}" 
                                            for k, v in stats.items()])
            logger.warning(msg)


    def _check_inputs_for_nan(self, batch: Dict[str, Any]):
        """æ›´è¯¦ç»†çš„è¾“å…¥æ•°æ®æ£€æŸ¥"""
        if not self.debug_mode:
            return
            
        for key, value in batch.items():
            if torch.is_tensor(value):
                if torch.isnan(value).any():
                    nan_count = torch.isnan(value).sum().item()
                    logger.error(f"ğŸš¨ğŸš¨ğŸš¨ğŸš¨ ä¸¥é‡é”™è¯¯: è¾“å…¥æ•°æ® {key} åŒ…å« {nan_count} ä¸ªNaNå€¼")
                    raise ValueError(f"è¾“å…¥æ•°æ® {key} åŒ…å«NaNå€¼ï¼Œè¯·æ£€æŸ¥æ•°æ®é¢„å¤„ç†")
                    
                if torch.isinf(value).any():
                    inf_count = torch.isinf(value).sum().item()
                    logger.error(f"ğŸš¨ğŸš¨ğŸš¨ğŸš¨ ä¸¥é‡é”™è¯¯: è¾“å…¥æ•°æ® {key} åŒ…å« {inf_count} ä¸ªInfå€¼")
                    raise ValueError(f"è¾“å…¥æ•°æ® {key} åŒ…å«Infå€¼ï¼Œè¯·æ£€æŸ¥æ•°æ®é¢„å¤„ç†")
                    
                # åªå¯¹æ•°å€¼ç±»å‹ï¼ˆæµ®ç‚¹æ•°ã€æ•´æ•°ï¼‰çš„å¼ é‡è¿›è¡Œç»Ÿè®¡è®¡ç®—ï¼Œè·³è¿‡å¸ƒå°”ç±»å‹
                if value.numel() > 0 and value.dtype in [torch.float16, torch.float32, torch.float64, torch.int16, torch.int32, torch.int64]:
                    try:
                        stats = {
                            'min': value.min().item(),
                            'max': value.max().item(),
                        }
                        
                        # åªæœ‰æµ®ç‚¹ç±»å‹æ‰èƒ½è®¡ç®—meanå’Œstd
                        if value.dtype in [torch.float16, torch.float32, torch.float64]:
                            stats['mean'] = value.mean().item()
                            if value.numel() >= 2 and value.dtype.is_floating_point:
                                stats["std"] = value.std(unbiased=False).item()
                            else:
                                stats["std"] = 0.0
                        else:
                            # å¯¹äºæ•´æ•°ç±»å‹ï¼Œè®¡ç®—æ€»å’Œä½œä¸ºæ›¿ä»£
                            stats['sum'] = value.sum().item()
                        
                        # logger.debug(f"è¾“å…¥æ•°æ® {key} ç»Ÿè®¡: {stats}")
                        
                    except Exception as e:
                        # å¦‚æœç»Ÿè®¡è®¡ç®—å¤±è´¥ï¼Œåªè®°å½•åŸºæœ¬ä¿¡æ¯
                        logger.debug(f"æ— æ³•è®¡ç®— {key} çš„ç»Ÿè®¡ä¿¡æ¯: {e}, dtype: {value.dtype}")


    def _build_tabular_features(self, batch):
        """æ„å»ºè¡¨æ ¼æ•°æ®ç‰¹å¾å‘é‡"""
        # 1. æ•°å€¼å‹ç‰¹å¾å¤„ç†ï¼ˆå¿…é€‰ï¼‰
        numeric_features = batch['numeric_features'].to(self.device)
        # print("DEBUG numeric_features shape:", numeric_features.shape)

        # 2. ç±»åˆ«å‹ç‰¹å¾å¤„ç†ï¼ˆå¿…é€‰ï¼‰
        categorical_ids = batch["categorical_features"].to(self.device)   # [B, C]
        batch_size, num_cat_cols = categorical_ids.shape
        expected_num_cat_cols = len(self.categorical_columns_effective)

        # categorical_ids å½¢çŠ¶åº”ä¸º [B, num_effective_cols]
        assert num_cat_cols == expected_num_cat_cols, \
            f"âš  categorical_features åˆ—æ•°ä¸åŒ¹é…ï¼šbatch ä¸­ {num_cat_cols}ï¼Œdataset ä¸­{expected_num_cat_cols}"
                
        categorical_embedded_list = []
        for i, cat_col in enumerate(self.categorical_columns_effective):
            cat_emb_layer = self.categorical_embedding_layers[cat_col]
            assert cat_emb_layer is not None, f"âš  categorical_embedding_layer for column={cat_col} æ‰¾ä¸åˆ°!"
            cat_col_ids = categorical_ids[:, i]       # [B]
            cat_col_emb = cat_emb_layer(cat_col_ids)  # [B, H]
            categorical_embedded_list.append(cat_col_emb)

        categorical_features = torch.cat(categorical_embedded_list, dim=1) # [B, C*H]
        self._debug_tensor(categorical_features, "categorical_features (before_norm)")
        # print("DEBUG categorical_features shape (before_norm):", categorical_features.shape)

        # â­ è§„èŒƒåŒ–ç±»åˆ«ç‰¹å¾
        categorical_features = self.categorical_norm(categorical_features)
        self._debug_tensor(categorical_features, "categorical_features (after_norm)")
        # print("DEBUG categorical_features shape (after_norm):", categorical_features.shape)
        
        # 3. åŸŸååµŒå…¥ç‰¹å¾å¤„ç†ï¼ˆå¯é€‰ï¼‰
        if self.domain_embedding_enabled:
            domain_embeddings = batch['domain_embedding_features'].to(self.device)
            # print("DEBUG domain_embeddings shape:", domain_embeddings.shape)
        else:
            domain_embeddings = None

        # 4. è¡¨æ ¼æ•°æ®ç‰¹å¾èåˆ
        if self.domain_embedding_enabled:
            tabular_features = torch.cat([numeric_features, categorical_features, domain_embeddings], dim=1)
        else:
            tabular_features = torch.cat([numeric_features, categorical_features], dim=1)

        # print("DEBUG tabular_features shape:", tabular_features.shape)
        return tabular_features

    # ---------------- forward ----------------        
    def forward(self, batch: Dict[str, Any]) -> Dict[str, torch.Tensor]:
        """å‰å‘ä¼ æ’­"""
        
        # æ£€æŸ¥è¾“å…¥æ•°æ®æ˜¯å¦æœ‰NaN/Inf
        self._check_inputs_for_nan(batch)        

        # æ·»åŠ ç»´åº¦è°ƒè¯•
        # logger.debug(f"æ•°å€¼ç‰¹å¾ç»´åº¦: {batch['numeric_features'].shape}")
        # logger.debug(f"åŸŸååµŒå…¥ç‰¹å¾ç»´åº¦: {batch['domain_embedding_features'].shape}")
                
        # 1. æ•°æ®åŒ…åºåˆ—ç‰¹å¾å¤„ç†ï¼ˆå¯é€‰ï¼‰
        if self.sequence_features_enabled and self.sequence_encoder is not None:
            sequence_data = {
                'directions': batch['directions'],
                'iat_times': batch['iat_times'],
                'payload_sizes': batch['payload_sizes'], 
                'packet_numbers': batch['packet_numbers'],
                'avg_payload_sizes': batch['avg_payload_sizes'],
                'durations': batch['durations'],
                'sequence_mask': batch['sequence_mask'],  # æœ‰æ•ˆtokenæ©ç 
            }
            
            # æ£€æŸ¥è¾“å…¥æ•°æ®
            self._debug_tensor(sequence_data['directions'], "è¾“å…¥_directions")            
            self._debug_tensor(sequence_data['payload_sizes'], "è¾“å…¥_payload_sizes")
            self._debug_tensor(sequence_data['iat_times'], "è¾“å…¥_iat_times")
            self._debug_tensor(sequence_data['packet_numbers'], "è¾“å…¥_packet_numbers")
            self._debug_tensor(sequence_data['avg_payload_sizes'], "è¾“å…¥_avg_payload_sizes")
            self._debug_tensor(sequence_data['durations'], "è¾“å…¥_durations")
            self._debug_tensor(sequence_data['sequence_mask'], "è¾“å…¥_sequence_mask")

            seq_outputs = self.sequence_encoder(sequence_data)
            sequence_emb = seq_outputs["sequence_embedding"]
            self._debug_tensor(sequence_emb, "sequence_encoderè¾“å‡º")
            sequence_emb = self._safe_check_nan(sequence_emb, "sequence_emb", "åºåˆ—ç¼–ç ")
            
            sequence_outputs = self.sequence_projection(sequence_emb)
            self._debug_tensor(sequence_outputs, "sequence_projectionè¾“å‡º")
            sequence_outputs = self._safe_check_nan(sequence_outputs, "sequence_outputs", "åºåˆ—æŠ•å½±")

        else:
            # åˆ›å»ºç©ºçš„åºåˆ—ç‰¹å¾è¾“å‡º
            batch_size = batch['numeric_features'].shape[0]  # å€Ÿç”¨å¿…é€‰çš„numeric_featuresï¼Œç”Ÿæˆç‰¹å¾ç»´åº¦
            sequence_outputs = torch.zeros(batch_size, self.bert_config.hidden_size, device=self.device)

        # 2. æ–‡æœ¬ç‰¹å¾å¤„ç†ï¼ˆå¯é€‰ï¼‰
        if self.text_features_enabled:
            text_outputs = self._process_text_features(batch)
            self._debug_tensor(text_outputs, "text_outputs")
            text_outputs = self._safe_check_nan(text_outputs, "text_outputs", "BERTå¤„ç†")
        else:
            # åˆ›å»ºç©ºçš„æ–‡æœ¬ç‰¹å¾è¾“å‡º
            batch_size = batch['numeric_features'].shape[0]  # å€Ÿç”¨å¿…é€‰çš„numeric_featuresï¼Œç”Ÿæˆç‰¹å¾ç»´åº¦
            text_outputs = torch.zeros(batch_size, self.bert_config.hidden_size, device=self.device)

        # 3. è¡¨æ ¼æ•°æ®ç‰¹å¾æ„å»º
        tabular_features = self._build_tabular_features(batch)
        self._debug_tensor(tabular_features, "è¾“å…¥_tabular_features")
        
        # 4. è¡¨æ ¼æ•°æ®ç‰¹å¾æŠ•å½±
        tabular_outputs = self.tabular_projection(tabular_features)
        self._debug_tensor(tabular_outputs, "tabular_outputsè¾“å‡º")
        tabular_outputs = self._safe_check_nan(tabular_outputs, "tabular_outputs", "è¡¨æ ¼æ•°æ®æŠ•å½±")

        # 5. å¤šè§†å›¾ç‰¹å¾èåˆï¼šæ•°æ®åŒ…åºåˆ—ç‰¹å¾+æ–‡æœ¬ç‰¹å¾+è¡¨æ ¼æ•°æ®ç‰¹å¾
        multiview_outputs = self._fuse_multi_views(sequence_outputs, text_outputs, tabular_outputs)
        self._debug_tensor(multiview_outputs, "èåˆå_multiview_outputs")
        multiview_outputs = self._safe_check_nan(multiview_outputs, "multiview_outputs", "å¤šè§†å›¾èåˆ")
        
        # 6. åˆ†ç±»å™¨
        is_malicious_cls_logits = self.is_malicious_classifier(multiview_outputs)
        self._debug_tensor(is_malicious_cls_logits, "is_malicious_classifierè¾“å‡º")
        is_malicious_cls_logits = self._safe_check_nan(is_malicious_cls_logits, "is_malicious_cls_logits", "is_malicious_åˆ†ç±»å™¨")

        attack_family_cls_logits = None
        if self.attack_family_classifier is not None:
            attack_family_cls_logits = self.attack_family_classifier(multiview_outputs)
            self._debug_tensor(attack_family_cls_logits, "attack_family_cls_logitsè¾“å‡º")
            attack_family_cls_logits = self._safe_check_nan(attack_family_cls_logits, "attack_family_cls_logits", "is_malicious_åˆ†ç±»å™¨")

        return {
            'sequence_embeddings': sequence_outputs,
            'text_embeddings': text_outputs if self.text_features_enabled else None,
            'tabular_embeddings': tabular_outputs,
            'multiview_embeddings': multiview_outputs,
            'is_malicious_cls_logits': is_malicious_cls_logits,
            'attack_family_cls_logits': attack_family_cls_logits
        }

    def _on_drift_detected(self, drift_info: Dict):
        """
        æ£€æµ‹åˆ°æ¦‚å¿µæ¼‚ç§»æ—¶çš„å“åº”ç­–ç•¥ (è‡ªå®šä¹‰ä¼˜åŒ–ç‰ˆ)
        """
        import math  # ç¡®ä¿å¯¼å…¥math

        logger.info("\n" + "!" * 60)
        logger.info(f"ğŸš¨ æ¦‚å¿µæ¼‚ç§»æ£€æµ‹è§¦å‘ (ç¬¬ {self.drift_count} æ¬¡)")
        logger.info("!" * 60)

        # 1. åŸºç¡€ä¿¡æ¯ (ç§»é™¤ Log Bï¼Œä»…ä¿ç•™ B å€¼)
        bayes_factor = drift_info.get('bayes_factor', 0.0)
        threshold = self.drift_detector.detection_threshold if self.drift_detector else 0

        # è·å–é…ç½®çš„æ¼‚ç§»ç±»å‹
        drift_type_config = getattr(self.drift_detector, 'drift_type', 'sudden')

        logger.info(f"ğŸ“Š [æŒ‡æ ‡åˆ†æ]")
        # å°†ç§‘å­¦è®¡æ•°æ³•æ”¹ä¸ºå°æ•°å½¢å¼
        logger.info(f"   å½“å‰ B å€¼ : {bayes_factor:.10f} (é˜ˆå€¼ Ï„ = {threshold})")

        # 2. å†å²è¶‹åŠ¿
        if 'history' in drift_info and drift_info['history']:
            history = drift_info['history']
            # å°†è¶‹åŠ¿ä¸­çš„å€¼ä¹Ÿæ”¹ä¸ºå°æ•°å½¢å¼
            history_str = " -> ".join([f"{b:.8f}" for b in history[-5:]])
            logger.info(f"   Bå€¼å˜åŒ–è¶‹åŠ¿ (è¿‘5æ¬¡): {history_str}")

            # 3. æ¼‚ç§»åˆ¤å®š (æ›¿ä»£çªå˜æ¯”ç‡)
            # æ ¹æ® B å€¼å¤§å°åˆ¤æ–­æ¼‚ç§»è¯æ®çš„å¼ºå¼±
            evidence_str = ""
            if bayes_factor < 1e-10:
                evidence_str = "æå¼º (Extreme)"
            elif bayes_factor < 1e-4:
                evidence_str = "å¼º (Strong)"
            else:
                evidence_str = "ä¸­ç­‰ (Moderate)"

            logger.info(f"   æ¼‚ç§»ç±»å‹åˆ¤å®š: {drift_type_config} drift (è¯æ®å¼ºåº¦: {evidence_str})")

        # 4. æ¼‚ç§»è¯Šæ–­ (ä¼˜åŒ–å±‚çº§æ˜¾ç¤º)
        if 'diagnosis' in drift_info and drift_info['diagnosis']:
            logger.info(f"ğŸ” [çª—å£åˆ†å¸ƒå·®å¼‚è¯Šæ–­ (Top-3 å·®å¼‚èŠ‚ç‚¹)]")
            logger.info(f"   è¯´æ˜: 'Ref'ä¸ºå‚è€ƒçª—å£åˆ†å¸ƒï¼Œ'Cur'ä¸ºå½“å‰çª—å£åˆ†å¸ƒ")

            for i, node_info in enumerate(drift_info['diagnosis']):
                level = node_info['level']
                code = node_info['code']
                log_b = node_info['log_B_s']
                # å°†èŠ‚ç‚¹çš„ log contribution è½¬ä¸º B å€¼ contribution
                node_b_val = math.exp(log_b) if log_b > -700 else 0.0

                ref_cnt = node_info['ref_counts']  # (L, R)
                cur_cnt = node_info['cur_counts']  # (L, R)

                # æ„å»ºå±‚çº§è·¯å¾„å­—ç¬¦ä¸²ï¼šRoot -> 0 -> 01
                path_visual = "Root"
                if code and code != "ROOT":
                    current_path = ""
                    steps = []
                    for char in code:
                        current_path += char
                        steps.append(current_path)
                    # åªæ˜¾ç¤ºæœ€åå‡ å±‚ä»¥ä¿æŒç®€æ´ï¼Œæˆ–è€…æ˜¾ç¤ºå…¨è·¯å¾„
                    path_visual = " -> ".join(["Root"] + steps)

                # è®¡ç®—ç®€å•çš„æ¯”ä¾‹ä»¥è¾…åŠ©è§‚å¯Ÿ
                ref_total = sum(ref_cnt) + 1e-9
                cur_total = sum(cur_cnt) + 1e-9
                ref_ratio = f"{ref_cnt[0] / ref_total:.1%} vs {ref_cnt[1] / ref_total:.1%}"
                cur_ratio = f"{cur_cnt[0] / cur_total:.1%} vs {cur_cnt[1] / cur_total:.1%}"

                # å°†èŠ‚ç‚¹è´¡çŒ®çš„ B å€¼ä¹Ÿæ”¹ä¸ºå°æ•°å½¢å¼
                logger.info(f"   ğŸ“ èŠ‚ç‚¹å±‚çº§ {level} | è·¯å¾„: [{path_visual}] | è´¡çŒ® Bå€¼: {node_b_val:.8f}")
                logger.info(
                    f"      å‚è€ƒ R (N={int(ref_total)}): L {ref_cnt[0]} - R {ref_cnt[1]} ({ref_ratio})")
                logger.info(
                    f"      å½“å‰ W (N={int(cur_total)}): L {cur_cnt[0]} - R {cur_cnt[1]} ({cur_ratio})")

        # 5. å“åº”ç­–ç•¥
        logger.info(f"ğŸ›¡ï¸ [å“åº”ç­–ç•¥ - è§‚å¯Ÿæ¨¡å¼]")
        # if hasattr(self, 'trainer') and self.trainer is not None:
        #     opt = self.trainer.optimizers[0]
        #     current_lr = opt.param_groups[0]['lr']
        #
        #     # ğŸŸ¢ ä¿®æ”¹ç‚¹ï¼šè®¾ç½®æœ€å°å­¦ä¹ ç‡ (ä¾‹å¦‚ 1e-6)
        #     min_lr = 1e-6
        #     # ğŸŸ¢ ä¿®æ”¹ç‚¹ï¼šè¡°å‡ç³»æ•°æ”¹ä¸º 0.8 (æ¯” 0.5 æ¸©å’Œ)
        #     decay_factor = 0.8
        #
        #     new_lr = max(current_lr * decay_factor, min_lr)
        #
        #     if new_lr < current_lr:
        #         for param_group in opt.param_groups:
        #             param_group['lr'] = new_lr
        #         # å­¦ä¹ ç‡ä½¿ç”¨å°æ•°å½¢å¼æ˜¾ç¤º
        #         logger.info(f"   ğŸ“‰ å­¦ä¹ ç‡ä¸‹è°ƒ: {current_lr:.8f} -> {new_lr:.8f} (ä¸‹é™: {min_lr})")
        #     else:
        #         logger.info(f"   ğŸ›‘ å­¦ä¹ ç‡å·²è¾¾ä¸‹é™ ({min_lr})ï¼Œä¸å†ä¸‹è°ƒ")
    
    def training_step(self, batch, batch_idx):
        """è®­ç»ƒæ­¥éª¤"""
        return self._shared_step(batch, batch_idx, stage = "train")

    def _shared_step(self, batch, batch_idx, stage: str, return_outputs: bool = False):
        """å…±äº«çš„è®­ç»ƒ/éªŒè¯/æµ‹è¯•æ­¥éª¤"""
        # ======== Forward ========
        outputs = self(batch)  # å‰å‘ä¼ æ’­

        # æ£€æŸ¥æ¢¯åº¦ç›¸å…³çš„NaN
        if stage == "train":
            self._check_gradients("å‰å‘ä¼ æ’­å")

        # è®¡ç®—æŸå¤±å‡½æ•°
        losses = self._compute_losses(outputs, batch, stage)

        # è·å–batch_size
        batch_size = batch['numeric_features'].shape[0] if 'numeric_features' in batch else 1

        is_malicious_class_loss = losses.get("is_malicious_class_loss")
        # ä¸ºäº†åœ¨ tensorboard/logging ä¸­è®°å½•æŒ‡æ ‡
        if stage == "train":
            self.log(f"{stage}_is_malicious_class_loss", is_malicious_class_loss, on_step=False, on_epoch=True, sync_dist=True, batch_size=batch_size)

        attack_family_class_loss = losses.get("attack_family_class_loss")
        if stage == "train" and attack_family_class_loss is not None:
            self.log(f"{stage}_attack_family_class_loss", attack_family_class_loss, on_step=False, on_epoch=True, sync_dist=True, batch_size=batch_size)

        total_loss = losses["total_loss"]
        if stage == "train":
            self.log(f"{stage}_total_loss", total_loss, on_step=False, on_epoch=True, sync_dist=True, batch_size=batch_size)

        # ä¸ºis_maliciousä»»åŠ¡è®¡ç®—å’Œè®°å½•æ€§èƒ½Metrics
        self._compute_and_log_is_malicious_batch_metrics(stage, outputs, batch, batch_size)

        # ä¸ºattack_familyå¤šåˆ†ç±»ä»»åŠ¡è®¡ç®—å’Œè®°å½•æ€§èƒ½Metrics
        self._compute_and_log_attack_family_batch_metrics(stage, outputs, batch, batch_size)

        # åå‘ä¼ æ’­å‰æ£€æŸ¥
        if stage == "train":
            self._check_gradients("åå‘ä¼ æ’­å‰")

        if return_outputs:
            return outputs

        return total_loss
    
    def _compute_and_log_is_malicious_batch_metrics(self, stage, outputs, batch, batch_size):
        is_malicious_cls_logits = outputs['is_malicious_cls_logits']
        is_malicious_probs = torch.sigmoid(is_malicious_cls_logits)
        is_malicious_preds = (is_malicious_probs > 0.5).float()

        is_malicious_labels = batch['is_malicious_label'].to(is_malicious_cls_logits.device).float()
        if is_malicious_labels.dim() == 1:
            is_malicious_labels = is_malicious_labels.unsqueeze(1)

        if stage == "train":
            # è®¡ç®— accuracy / precision / recall / f1
            try:
                is_malicious_trues_np = is_malicious_labels.squeeze(1).cpu().numpy()
                is_malicious_preds_np = is_malicious_preds.squeeze(1).cpu().numpy()

                accuracy = accuracy_score(is_malicious_trues_np, is_malicious_preds_np)
                precision = precision_score(is_malicious_trues_np, is_malicious_preds_np, zero_division=0)
                recall = recall_score(is_malicious_trues_np, is_malicious_preds_np, zero_division=0)
                f1 = f1_score(is_malicious_trues_np, is_malicious_preds_np, zero_division=0)
            except Exception as e:
                logger.warning(f"è®¡ç®—æŒ‡æ ‡æ—¶å‡ºé”™: {e}")
                accuracy = precision = recall = f1 = 0.0

            self.log(f"{stage}_is_malicious_accuracy", accuracy, prog_bar=True, sync_dist=True, batch_size=batch_size)
            self.log(f"{stage}_is_malicious_precision", precision, prog_bar=True, sync_dist=True, batch_size=batch_size)
            self.log(f"{stage}_is_malicious_recall", recall, prog_bar=True, sync_dist=True, batch_size=batch_size)
            self.log(f"{stage}_is_malicious_f1", f1, prog_bar=True, sync_dist=True, batch_size=batch_size)
        elif stage == "val":
            # ä¿å­˜æ¯ä¸ª batch çš„ç»“æœï¼ˆå¿…é¡» detach + cpuï¼‰
            self.val_is_malicious_labels.append(is_malicious_labels.detach().cpu())
            self.val_is_malicious_probs.append(is_malicious_probs.detach().cpu())
            self.val_is_malicious_preds.append(is_malicious_preds.detach().cpu())
        elif stage == "test":
            # ä¿å­˜æ¯ä¸ª batch çš„ç»“æœï¼ˆå¿…é¡» detach + cpuï¼‰
            self.test_is_malicious_labels.append(is_malicious_labels.detach().cpu())
            self.test_is_malicious_probs.append(is_malicious_probs.detach().cpu())
            self.test_is_malicious_preds.append(is_malicious_preds.detach().cpu())
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„stageå­—ç¬¦ä¸²: {stage}")

        return

    def _compute_and_log_attack_family_batch_metrics(self, stage, outputs, batch, batch_size):
        """
        è®¡ç®—å¹¶è®°å½• attack_family çš„ batch çº§æŒ‡æ ‡
        - ä»…åœ¨ malicious æ ·æœ¬å­é›†ä¸Šè¯„ä¼°
        - ä½¿ç”¨ OVR + macro-F1
        """
        if "attack_family_cls_logits" not in outputs:
            return

        # logits / labels
        attack_family_logits = outputs["attack_family_cls_logits"]          # [B, K]
        attack_family_labels = batch["attack_family_label"].to(attack_family_logits.device)  # [B, K]

        # åªåœ¨ malicious å­é›†ä¸Šè¯„ä¼°
        is_malicious_label = batch["is_malicious_label"].to(attack_family_logits.device).view(-1) == 1
        if is_malicious_label.sum() == 0:
            return

        # ä»…åœ¨ã€ŒçœŸå®æ¶æ„æµé‡ã€æ ·æœ¬ä¸Šè¯„ä¼° attack_family åˆ†ç±»ç»“æœã€‚
        # è¿™é‡Œä½¿ç”¨çš„æ˜¯ is_malicious çš„çœŸå®æ ‡ç­¾ï¼ˆground truthï¼‰ï¼Œè€Œä¸æ˜¯æ¨¡å‹é¢„æµ‹ç»“æœã€‚
        # å› æ­¤è¯„ä¼°çš„æ˜¯æ¡ä»¶åˆ†ç±»æ€§èƒ½ï¼š
        #   P(attack_family | is_malicious = 1)ï¼Œ
        # è€Œä¸æ˜¯â€œå…ˆé¢„æµ‹æ˜¯å¦æ¶æ„ï¼Œå†é¢„æµ‹æ”»å‡»å®¶æ—â€çš„çº§è”é¢„æµ‹æµç¨‹ã€‚
        attack_family_logits = attack_family_logits[is_malicious_label]
        attack_family_labels = attack_family_labels[is_malicious_label]

        # OVR threshold
        attack_family_probs = torch.sigmoid(attack_family_logits)
        attack_family_preds = (attack_family_probs > 0.5).int()

        if stage == "train":
            try:
                # è½¬ numpy
                labels_np = attack_family_labels.cpu().numpy()
                preds_np = attack_family_preds.cpu().numpy()

                macro_f1 = f1_score(labels_np, preds_np, average="macro", zero_division=0)
                micro_f1 = f1_score(labels_np, preds_np, average="micro", zero_division=0)
            except Exception as e:
                logger.warning(f"è®¡ç®— attack_family æŒ‡æ ‡æ—¶å‡ºé”™: {e}")
                macro_f1 = micro_f1 = 0.0

            self.log(f"{stage}_attack_family_macro_f1", macro_f1, prog_bar=True, sync_dist=True, batch_size=batch_size)
            self.log(f"{stage}_attack_family_micro_f1", micro_f1, prog_bar=False, sync_dist=True, batch_size=batch_size)
        elif stage == "val":
            self.val_attack_family_labels.append(attack_family_labels.detach().cpu())
            self.val_attack_family_probs.append(attack_family_probs.detach().cpu())
            self.val_attack_family_preds.append(attack_family_preds.detach().cpu())
        elif stage == "test":
            self.test_attack_family_labels.append(attack_family_labels.detach().cpu())
            self.test_attack_family_probs.append(attack_family_probs.detach().cpu())
            self.test_attack_family_preds.append(attack_family_preds.detach().cpu())
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„stageå­—ç¬¦ä¸²: {stage}")


    def _check_gradients(self, stage: str):
        """æ£€æŸ¥æ¢¯åº¦çŠ¶æ€"""
        if not self.debug_mode:
            return
            
        for name, param in self.named_parameters():
            if param.grad is not None:
                if torch.isnan(param.grad).any():
                    nan_count = torch.isnan(param.grad).sum().item()
                    logger.warning(f"âš ï¸ æ¢¯åº¦NaN: {name} åœ¨ {stage} æœ‰ {nan_count} ä¸ªNaNæ¢¯åº¦")
                if torch.isinf(param.grad).any():
                    inf_count = torch.isinf(param.grad).sum().item()
                    logger.warning(f"âš ï¸ æ¢¯åº¦Inf: {name} åœ¨ {stage} æœ‰ {inf_count} ä¸ªInfæ¢¯åº¦")

    def on_after_backward(self):
        """åå‘ä¼ æ’­åçš„é’©å­"""
        if self.debug_mode:
            self._check_gradients("åå‘ä¼ æ’­å")
            
            # æ¢¯åº¦è£å‰ªï¼ˆé¢„é˜²æ¢¯åº¦çˆ†ç‚¸ï¼‰
            torch.nn.utils.clip_grad_norm_(self.parameters(), max_norm=1.0)

    def on_validation_epoch_start(self):
        """éªŒè¯é˜¶æ®µå¼€å§‹ï¼šæ¸…ç©ºç¼“å­˜"""
        self.val_is_malicious_labels = []
        self.val_is_malicious_preds = []
        self.val_is_malicious_probs = []

        self.val_attack_family_labels = []
        self.val_attack_family_preds = []
        self.val_attack_family_probs = []

    def validation_step(self, batch, batch_idx):
        # è°ƒç”¨ _shared_step è¿”å› outputsï¼Œä¸é‡å¤è®¡ç®— loss
        self._shared_step(batch, batch_idx, stage="val", return_outputs=False)

        # ä½¿ç”¨æ–°çš„é€šç”¨SHAPåˆ†ææ¡†æ¶ del by qinyf 2012-12-02
        # if self.cfg.shap.enable_shap:
        #     if self.should_run_shap_analysis(self.current_epoch, batch_idx):
        #         logger.info(f"å¼€å§‹é€šç”¨SHAPåˆ†æï¼Œepoch: {self.current_epoch}")
                
        #         try:
        #             # æ‰§è¡ŒSHAPåˆ†æ
        #             shap_results = self.perform_shap_analysis(batch)
                    
        #             if shap_results and not shap_results.get('error'):
        #                 # è°ƒç”¨é’©å­æ–¹æ³•ï¼Œå¯ä»¥åœ¨å­ç±»ä¸­é‡å†™
        #                 self.on_shap_analysis_completed(shap_results)
                        
        #         except Exception as e:
        #             logger.error(f"é€šç”¨SHAPåˆ†æå¤±è´¥: {e}")

        return None
        
    def on_validation_epoch_end(self):
        """éªŒè¯é›† epoch ç»“æŸæ—¶ç»Ÿä¸€è®¡ç®— F1 / precision / recall"""
        self._compute_and_log_is_malicious_epoch_metrics(
            stage="val",
            labels_list=self.val_is_malicious_labels,
            preds_list=self.val_is_malicious_preds,
            probs_list=self.val_is_malicious_probs
        )

        # æ¸…ç©ºç¼“å­˜
        self.val_is_malicious_labels.clear()
        self.val_is_malicious_preds.clear()
        self.val_is_malicious_probs.clear()

        # attack_family çš„ epoch-level é€»è¾‘ï¼ˆå…¬å…±å‡½æ•°ï¼‰
        self._compute_and_log_attack_family_epoch_metrics(
            stage="val",
            labels_list=self.val_attack_family_labels,
            preds_list=self.val_attack_family_preds,
        )

        # æ¸…ç©ºç¼“å­˜
        self.val_attack_family_labels.clear()
        self.val_attack_family_preds.clear()
        self.val_attack_family_probs.clear()
                
    def on_test_epoch_start(self):
        """æµ‹è¯•é˜¶æ®µå¼€å§‹æ—¶åˆå§‹åŒ–å…¨å±€å­˜å‚¨"""
        self.test_is_malicious_labels = []
        self.test_is_malicious_preds = []
        self.test_is_malicious_probs = []

        self.test_attack_family_labels = []
        self.test_attack_family_preds = []
        self.test_attack_family_probs = []

        # [æ–°å¢] åˆå§‹åŒ–ç”¨äºå¯è§†åŒ–çš„è®°å½•åˆ—è¡¨
        self.test_step_accuracies = []  # å­˜å‚¨æ¯ä¸€æ­¥çš„å‡†ç¡®ç‡
        self.test_drift_steps = []  # å­˜å‚¨å‘ç”Ÿæ¼‚ç§»çš„ step ç´¢å¼•

        # [æ–°å¢] åœ¨æµ‹è¯•å¼€å§‹æ—¶åˆå§‹åŒ–æ¦‚å¿µæ¼‚ç§»ç®¡ç†å™¨
        if self.drift_enabled and DRIFT_MANAGER_AVAILABLE and self.drift_manager is None:
            logger.info("ğŸ”§ [Test Phase] åˆå§‹åŒ–æ¦‚å¿µæ¼‚ç§»ç®¡ç†å™¨...")
            try:
                self.drift_manager = ConceptDriftManager(self.cfg, self)

                # [å…³é”®ä¿®æ”¹] å¼ºåˆ¶æ¸…ç©ºé€‚åº”ç­–ç•¥é…ç½®ï¼Œå®ç°â€œé€‚åº”å…ˆä¸åšâ€
                # è¿™ä¼šé˜²æ­¢ adapter.adapt() æ‰§è¡Œä»»ä½• LR è¡°å‡æˆ–ä¼˜åŒ–å™¨é‡ç½®æ“ä½œ
                if hasattr(self.drift_manager, 'adapter'):
                    logger.info("ğŸ›‘ [Test Phase] å¼ºåˆ¶ç¦ç”¨æ¼‚ç§»é€‚åº”ç­–ç•¥ (Adaptation Disabled)")
                    self.drift_manager.adapter.config = {}

                logger.info(f"âœ… ä¸»æ£€æµ‹å™¨: {self.drift_manager.main_name.upper()}")
            except Exception as e:
                logger.error(f"âŒ æ¦‚å¿µæ¼‚ç§»ç®¡ç†å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
                self.drift_enabled = False

        # 3. ç»„ä»¶é‡ç½® added by qinyf 2025-12-02
        if self.cfg.shap.enable_shap:
            self.shap_analyzer.reset()

    def test_step(self, batch, batch_idx):
        """æµ‹è¯•æ­¥éª¤ - å®Œå…¨å¤ç”¨ _shared_step"""
        # [ä¿®æ”¹] è°ƒç”¨ _shared_step å¹¶è·å– outputs (return_outputs=True)
        # è¿™æ ·æˆ‘ä»¬æ‰èƒ½æ‹¿åˆ° multiview_embeddings ç”¨äºæ£€æµ‹
        outputs = self._shared_step(batch, batch_idx, stage="test", return_outputs=True)

        if batch_idx % 10 == 0:  # é˜²æ­¢åˆ·å±ï¼Œæ¯10ä¸ªbatchæ‰“å°ä¸€æ¬¡ï¼Œæˆ–è€…æ‚¨å¯ä»¥å»æ‰è¿™ä¸ªif
            try:
                # === A. è§£æ Is Malicious (äºŒåˆ†ç±») ===
                mal_labels = batch['is_malicious_label'].cpu().numpy()
                mal_names = ["Benign(0)", "Malicious(1)"]
                # ç»Ÿè®¡æ•°é‡
                mal_counts = {name: np.sum(mal_labels == i) for i, name in enumerate(mal_names)}

                # === B. è§£æ Attack Family (å¤šåˆ†ç±») ===
                if 'attack_family_label' in batch:
                    family_labels = batch['attack_family_label']  # [B, Num_Classes] (Multi-hot or One-hot)

                    # å¦‚æœæ˜¯ One-Hot/Multi-Hotï¼Œè½¬ä¸ºç´¢å¼•
                    if family_labels.dim() > 1:
                        family_indices = torch.argmax(family_labels, dim=1).cpu().numpy()
                    else:
                        family_indices = family_labels.cpu().numpy()

                    # è·å–ç±»åˆ«åç§°æ˜ å°„
                    # self.attack_family_names æ¥è‡ªåˆå§‹åŒ–æ—¶çš„ labels_cfg
                    family_map = {i: name for i, name in enumerate(self.attack_family_names)}

                    # ç»Ÿè®¡å½“å‰ Batch é‡Œæœ‰å“ªäº›æ”»å‡»
                    unique_idxs, counts = np.unique(family_indices, return_counts=True)
                    family_stats = []
                    for idx, count in zip(unique_idxs, counts):
                        name = family_map.get(idx, f"Unknown({idx})")
                        family_stats.append(f"{name}: {count}")

                    # === æ‰“å°æ—¥å¿— ===
                    logger.info(f"ğŸ” [Step {batch_idx}] æ•°æ®åˆ†å¸ƒ:")
                    logger.info(f"   â–º äºŒåˆ†ç±» (Is Malicious): {mal_counts}")
                    logger.info(f"   â–º æ”»å‡»å®¶æ— (Ground Truth): {', '.join(family_stats)}")

                    # === C. (å¯é€‰) æŸ¥çœ‹æ¨¡å‹é¢„æµ‹äº†ä»€ä¹ˆ ===
                    # è·å–é¢„æµ‹ logits -> probabilities -> predictions
                    pred_logits = outputs['attack_family_cls_logits']
                    pred_indices = torch.argmax(pred_logits, dim=1).cpu().numpy()

                    unique_pred_idxs, pred_counts = np.unique(pred_indices, return_counts=True)
                    pred_stats = []
                    for idx, count in zip(unique_pred_idxs, pred_counts):
                        name = family_map.get(idx, f"Unknown({idx})")
                        pred_stats.append(f"{name}: {count}")

                    logger.info(f"   â–º æ¨¡å‹é¢„æµ‹ (Prediction)  : {', '.join(pred_stats)}")
                    logger.info("-" * 40)

            except Exception as e:
                logger.warning(f"æ‰“å°æ ‡ç­¾åˆ†å¸ƒå‡ºé”™: {e}")

            # 3. æ¼‚ç§»æ£€æµ‹ (ä¿æŒåŸæœ‰é€»è¾‘)
        if self.drift_enabled and self.drift_manager:
            try:
                features = outputs['multiview_embeddings'].detach()
                result = self.drift_manager.process_batch(
                    features=features,
                    global_step=batch_idx,
                    current_epoch=self.current_epoch
                )
            except Exception as e:
                if self.debug_mode:
                    logger.warning(f"Drift detection error in test_step: {e}")

        # 4. ç»„ä»¶æ•°æ®æ”¶é›† (SHAP)
        if self.cfg.shap.enable_shap:
            self.shap_analyzer.collect_batch(batch)

        return None

    def on_test_epoch_end(self):
        """æµ‹è¯•é˜¶æ®µç»“æŸï¼Œæ±‡æ€»å…¨å±€æŒ‡æ ‡ï¼Œå¤š GPU ä¸‹æ”¯æŒåŒæ­¥"""
        self._compute_and_log_is_malicious_epoch_metrics(
            stage="test",
            labels_list=self.test_is_malicious_labels,
            preds_list=self.test_is_malicious_preds,
            probs_list=self.test_is_malicious_probs,
        )
        # æ¸…ç©ºå­˜å‚¨
        self.test_is_malicious_labels.clear()
        self.test_is_malicious_preds.clear()
        self.test_is_malicious_probs.clear()

        # attack_family çš„ epoch-level é€»è¾‘ï¼ˆå…¬å…±å‡½æ•°ï¼‰
        self._compute_and_log_attack_family_epoch_metrics(
            stage="test",
            labels_list=self.test_attack_family_labels,
            preds_list=self.test_attack_family_preds,
        )
        # [æ–°å¢] ç»˜åˆ¶ ç²¾åº¦è¶‹åŠ¿ä¸æ¦‚å¿µæ¼‚ç§»ç‚¹ å›¾
        if len(self.test_step_accuracies) > 0 and self.trainer.is_global_zero:
            self._plot_drift_accuracy_curve()

        # æ¸…ç©ºå­˜å‚¨
        self.test_attack_family_labels.clear()
        self.test_attack_family_preds.clear()
        self.test_attack_family_probs.clear()

        # 5. ç»„ä»¶æ‰§è¡Œåˆ†æ  added by qinyf 2025-12-02
        if self.cfg.shap.enable_shap:
            self.shap_analyzer.finalize()

    def _plot_drift_accuracy_curve(self):
        """ç»˜åˆ¶æµ‹è¯•é˜¶æ®µçš„ç²¾åº¦å˜åŒ–ä¸æ¦‚å¿µæ¼‚ç§»ç‚¹"""
        try:
            steps = range(len(self.test_step_accuracies))
            accuracies = self.test_step_accuracies

            plt.figure(figsize=(12, 6))

            # 1. ç»˜åˆ¶ç²¾åº¦æ›²çº¿
            plt.plot(steps, accuracies, label='Batch Accuracy', color='blue', alpha=0.6, linewidth=1)

            # (å¯é€‰) ç»˜åˆ¶ç§»åŠ¨å¹³å‡çº¿ä½¿å…¶æ›´å¹³æ»‘
            if len(accuracies) > 10:
                window_size = 10
                moving_avg = np.convolve(accuracies, np.ones(window_size) / window_size, mode='valid')
                # å¯¹é½ x è½´
                plt.plot(range(window_size - 1, len(accuracies)), moving_avg,
                         label=f'Moving Avg (w={window_size})', color='darkblue', linewidth=2)

            # 2. æ ‡è®°æ¼‚ç§»ç‚¹
            if self.test_drift_steps:
                for drift_step in self.test_drift_steps:
                    plt.axvline(x=drift_step, color='red', linestyle='--', alpha=0.8)

                # ä»…åœ¨å›¾ä¾‹ä¸­æ·»åŠ ä¸€æ¬¡ Drift æ ‡ç­¾
                plt.axvline(x=self.test_drift_steps[0], color='red', linestyle='--', alpha=0.8, label='Drift Detected')

            plt.title('Test Phase: Accuracy Trend & Concept Drift Detection')
            plt.xlabel('Test Step (Batch Index)')
            plt.ylabel('Accuracy')
            plt.ylim(0, 1.05)
            plt.legend(loc='lower right')
            plt.grid(True, alpha=0.3)

            # 3. ä¿å­˜å›¾ç‰‡
            save_dir = self.cfg.logging.save_dir
            if not os.path.exists(save_dir):
                os.makedirs(save_dir)

            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            save_path = os.path.join(save_dir, f"test_drift_accuracy_{timestamp}.png")

            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            plt.close()

            logger.info(f"ğŸ“Š æ¦‚å¿µæ¼‚ç§»ä¸ç²¾åº¦åˆ†æå›¾å·²ä¿å­˜è‡³: {save_path}")

        except Exception as e:
            logger.warning(f"ç»˜åˆ¶æ¼‚ç§»åˆ†æå›¾å¤±è´¥: {e}")

    def _compute_and_log_is_malicious_epoch_metrics(
        self,
        stage: str,
        labels_list: List[torch.Tensor],
        preds_list:  List[torch.Tensor],
        probs_list:  List[torch.Tensor],
    ):
        if len(labels_list) == 0:
            logger.warning(f"[{stage}] is_malicious labels empty")
            return

        labels = torch.cat(labels_list, dim=0)
        preds  = torch.cat(preds_list, dim=0)
        probs  = torch.cat(probs_list, dim=0)

        # ---- DDP gather ----
        if self.trainer.world_size > 1:
            labels = self.all_gather(labels).view(-1)
            preds  = self.all_gather(preds).view(-1)
            probs  = self.all_gather(probs).view(-1)

        labels_np = labels.cpu().numpy()
        preds_np  = preds.cpu().numpy()
        probs_np  = probs.cpu().numpy()

        accuracy  = accuracy_score(labels_np, preds_np)
        precision = precision_score(labels_np, preds_np, zero_division=0)
        recall  = recall_score(labels_np, preds_np, zero_division=0)
        f1   = f1_score(labels_np, preds_np, zero_division=0)

        self.log(f"{stage}_is_malicious_accuracy", accuracy, on_epoch=True, prog_bar=False, sync_dist=True)
        self.log(f"{stage}_is_malicious_precision", precision, on_epoch=True, prog_bar=False, sync_dist=True)
        self.log(f"{stage}_is_malicious_recall", recall, on_epoch=True, prog_bar=False, sync_dist=True)
        self.log(f"{stage}_is_malicious_f1", f1, on_epoch=True, prog_bar=False, sync_dist=True)

        # ğŸ”´ğŸ”´ åªåœ¨ä¸»è¿›ç¨‹ä¸Šè¾“å‡ºis_maliciousä»»åŠ¡çš„æµ‹è¯•é˜¶æ®µçš„å®Œæ•´æŠ¥å‘Š
        if stage == "test" and self.trainer.is_global_zero:
            logger.info("=" * 60)
            logger.info("ğŸ¤– æœ€ä½³æ¨¡å‹çš„is_maliciousä»»åŠ¡çš„æµ‹è¯•æŠ¥å‘Š")
            logger.info("=" * 60)

            # ---- æ•´ç†è¯¦ç»†çš„æ¨¡å‹æ€§èƒ½æŠ¥å‘Š ----
            logger.info(f"ğŸ“Š is_maliciousä»»åŠ¡çš„åŸºç¡€æŒ‡æ ‡:")
            logger.info(f"   å‡†ç¡®ç‡: {accuracy:.4f}")
            logger.info(f"   ç²¾ç¡®ç‡: {precision:.4f}")
            logger.info(f"   å¬å›ç‡: {recall:.4f}")
            logger.info(f"   F1åˆ†æ•°: {f1:.4f}")
            
            # åˆ†ç±»æŠ¥å‘Š
            try:
                report = classification_report(labels_np, preds_np, digits=4, target_names=['æ­£å¸¸', 'æ¶æ„'])
                logger.info("ğŸ“‹ is_maliciousä»»åŠ¡çš„è¯¦ç»†åˆ†ç±»æŠ¥å‘Š:")
                logger.info("\n" + report)
            except Exception as e:
                logger.warning(f"is_maliciousä»»åŠ¡çš„åˆ†ç±»æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {e}")
            
            # æ··æ·†çŸ©é˜µ
            try:
                cm = confusion_matrix(labels_np, preds_np)
                logger.info("ğŸ¯ is_maliciousä»»åŠ¡çš„æ··æ·†çŸ©é˜µ:")
                logger.info(f"\n{cm}")
            except Exception as e:
                logger.warning(f"is_maliciousä»»åŠ¡çš„æ··æ·†çŸ©é˜µç”Ÿæˆå¤±è´¥: {e}")
            
            # ROC-AUC å’Œ PRæ›²çº¿
            try:
                auc = roc_auc_score(labels_np, probs_np)
                avg_precision = average_precision_score(labels_np, probs_np)
                logger.info(f"ğŸ“ˆ is_maliciousä»»åŠ¡çš„é«˜çº§æŒ‡æ ‡:")
                logger.info(f"   ROC-AUC: {auc:.4f}")
                logger.info(f"   Average Precision: {avg_precision:.4f}")
            except Exception as e:
                logger.warning(f"é«˜çº§æŒ‡æ ‡è®¡ç®—å¤±è´¥: {e}")
            
            # æ ·æœ¬ç»Ÿè®¡
            logger.info(f"ğŸ“Š æ ·æœ¬çš„is_maliciousæ ‡ç­¾æ•°é‡ç»Ÿè®¡:")
            logger.info(f"   æ€»æ ·æœ¬æ•°: {len(labels)}")
            logger.info(f"   æ­£æ ·æœ¬æ•°: {labels_np.sum()}")
            logger.info(f"   è´Ÿæ ·æœ¬æ•°: {len(labels) - labels_np.sum()}")
            logger.info(f"   æ­£æ ·æœ¬æ¯”ä¾‹: {labels_np.mean():.2%}")
            
            logger.info("=" * 60)


    def _compute_and_log_attack_family_epoch_metrics(self, stage: str, labels_list, preds_list):
        """
        åœ¨ epoch ç»“æŸæ—¶ç»Ÿä¸€è®¡ç®—å¹¶ log attack_family æŒ‡æ ‡
        ç”¨äº val / test é˜¶æ®µï¼ˆtrain ä¸èµ°è¿™é‡Œï¼‰
        """
        assert stage in ("val", "test"), f"éæ³• stage={stage}"

        if not labels_list or len(labels_list) == 0:
            logger.warning(f"[{stage}] attack_family: æ— å¯ç”¨æ ·æœ¬")
            return

        # æ‹¼æ¥ï¼Œè½¬æ¢ä¸ºnumpyæ•°ç»„
        try:
            labels_np = torch.cat(labels_list, dim=0).cpu().numpy()
            preds_np = torch.cat(preds_list, dim=0).cpu().numpy()
        except Exception as e:
            logger.error(f"æ•°æ®æ‹¼æ¥ã€è½¬æ¢å¤±è´¥: {e}")
            return

        # è®¡ç®—æŒ‡æ ‡
        macro_f1 = f1_score(labels_np, preds_np, average="macro", zero_division=0)
        micro_f1 = f1_score(labels_np, preds_np, average="micro", zero_division=0)

        # logï¼ˆepoch-levelï¼Œä¸è¦ batch_sizeï¼‰
        self.log(f"{stage}_attack_family_macro_f1", macro_f1, on_epoch=True, prog_bar=True, sync_dist=True)
        self.log(f"{stage}_attack_family_micro_f1", micro_f1, on_epoch=True, prog_bar=False, sync_dist=True)

        # ğŸ”´ test é˜¶æ®µçš„ç®€è¦æœ€ç»ˆæŠ¥å‘Š
        if stage == "test" and self.trainer.is_global_zero:
            logger.info("=" * 60)
            logger.info("ğŸ¤– attack_family ä»»åŠ¡æµ‹è¯•æŠ¥å‘Šï¼ˆç®€è¦ï¼‰")
            logger.info(f"macro_f1={macro_f1:.4f}, micro_f1={micro_f1:.4f}")

            try:
                report = classification_report(labels_np, preds_np, digits=4, zero_division=0,
                    target_names=self.attack_family_names)
                logger.info("ğŸ“‹ attack_family ä»»åŠ¡çš„åˆ†ç±»æŠ¥å‘Š:")
                logger.info("\n" + report)
            except Exception as e:
                logger.warning(f"attack_family åˆ†ç±»æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {e}")

            per_attack_family_f1 = f1_score(labels_np, preds_np, average=None, zero_division=0)
            logger.info("ğŸ“Š attack_family per-class F1:")

            for name, f1v in zip(self.attack_family_names, per_attack_family_f1):
                logger.info(f"  {name:20s}: F1={f1v:.4f}")

            logger.info("=" * 60)


    def configure_optimizers(self):
        """é…ç½®ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨"""
        # ä½¿ç”¨ datamodule è·å–è®­ç»ƒé›†é•¿åº¦
        if self.trainer and hasattr(self.trainer, "datamodule") and self.trainer.datamodule is not None:
            train_loader = self.trainer.datamodule.train_dataloader()
            total_steps = len(train_loader) * self.trainer.max_epochs
        else:
            # å¦‚æœ trainer è¿˜æœªåˆå§‹åŒ–ï¼Œä½¿ç”¨é»˜è®¤æ­¥æ•°ï¼ˆå¯æ ¹æ® cfg è®¾ç½®ï¼‰
            total_steps = self.cfg.optimizer.default_total_steps

        warmup_steps = int(total_steps * self.cfg.optimizer.warmup_ratio)

        optimizer = AdamW(
            self.parameters(),
            lr=self.cfg.optimizer.lr,
            weight_decay=self.cfg.optimizer.weight_decay,
            eps=getattr(self.cfg.optimizer, 'eps', 1e-8)
        )

        scheduler_type = self.cfg.scheduler.type
        if scheduler_type == "cosine":
            scheduler = get_cosine_schedule_with_warmup(
                optimizer,
                num_warmup_steps=warmup_steps,
                num_training_steps=total_steps
            )
        elif scheduler_type == "constant":
            scheduler = get_constant_schedule_with_warmup(
                optimizer,
                num_warmup_steps=warmup_steps,
                num_training_steps=total_steps
            )
        else:  # linear
            scheduler = get_linear_schedule_with_warmup(
                optimizer,
                num_warmup_steps=warmup_steps,
                num_training_steps=total_steps
            )

        return {
            "optimizer": optimizer,
            "lr_scheduler": {
                "scheduler": scheduler,
                "interval": "step",
            }
        }
    

    def _fuse_multi_views(self, sequence_outputs, text_outputs, tabular_outputs):
        """ä½¿ç”¨é…ç½®çš„èåˆæ–¹æ³•èåˆå¤šè§†å›¾ç‰¹å¾"""
        view_embeddings = []
        
        # æ”¶é›†æ‰€æœ‰å¯ç”¨çš„è§†å›¾
        view_embeddings.append(tabular_outputs)  # æ•°å€¼ç‰¹å¾å¿…é€‰ + åŸŸåç‰¹å¾å¯é€‰
        
        if self.text_features_enabled:
            view_embeddings.append(text_outputs)
        
        if self.sequence_features_enabled:
            view_embeddings.append(sequence_outputs)
        
        # ä½¿ç”¨é…ç½®çš„èåˆæ–¹æ³•
        if len(view_embeddings) > 1:
            fused_embedding = self.fusion_layer(view_embeddings)
        else:
            # å•è§†å›¾æƒ…å†µï¼Œç›´æ¥ä½¿ç”¨
            fused_embedding = view_embeddings[0]
        
        return fused_embedding

    # ================== æ–°å¢ï¼šè®­ç»ƒç»“æŸå›è°ƒ ==================
    def on_train_end(self):
            """è®­ç»ƒç»“æŸæ—¶æ‰“å°æ¼‚ç§»å¯¹æ¯”æŠ¥å‘Š"""
            super().on_train_end()
            if self.drift_manager:
                report = self.drift_manager.generate_report()
                logger.info("\n" + report)

    # ================== æ–°å¢ï¼šæµ‹è¯•ç»“æŸå›è°ƒ ==================
    def on_test_end(self):
        super().on_test_end()
        # å¦‚æœéœ€è¦åœ¨æµ‹è¯•ç»“æŸä¹Ÿæ‰“å°ï¼Œå¯ä»¥å¤ç”¨é€»è¾‘ï¼Œä½†é€šå¸¸æ¼‚ç§»ä¸»è¦å…³æ³¨è®­ç»ƒæ—¶çš„é€‚åº”
        pass