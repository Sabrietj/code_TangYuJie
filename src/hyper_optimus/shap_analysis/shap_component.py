# models/shap_component.py

import torch
import torch.nn as nn
import numpy as np
import pandas as pd
import os
import logging

logger = logging.getLogger(__name__)

# ==============================================================================
# 1. ä»£ç†æ¨¡å‹ Wrapper (ç”¨äº DeepExplainer æ±‚å¯¼)
# ==============================================================================
class ShapFusionWrapper(nn.Module):
    """
    SHAP ä¸“ç”¨åŒ…è£…å™¨ã€‚
    è¾“å…¥ï¼šEmbedding å‘é‡ (Numeric, Domain, Seq, Text)
    è¾“å‡ºï¼šLogits
    åŠŸèƒ½ï¼šè·³è¿‡ Tokenizer å’Œ Encodersï¼Œç›´æ¥è®¡ç®—èåˆå±‚å’Œåˆ†ç±»å™¨çš„æ¢¯åº¦ã€‚
    """
    def __init__(self, model):
        super().__init__()
        self.model = model

    def forward(self, numeric_feats, domain_feats, seq_emb, text_emb):
        # 1. è¡¨æ ¼ç‰¹å¾è·¯å¾„
        if self.model.domain_embedding_enabled:
            tabular_input = torch.cat([numeric_feats, domain_feats], dim=1)
        else:
            tabular_input = numeric_feats
        
        tabular_out = self.model.tabular_projection(tabular_input)

        # 2. åºåˆ—ç‰¹å¾è·¯å¾„ (seq_emb å·²ç»æ˜¯ SequenceEncoder è¾“å‡º)
        if self.model.sequence_features_enabled:
            seq_out = self.model.sequence_projection(seq_emb)
        else:
            seq_out = torch.zeros_like(tabular_out)

        # 3. æ–‡æœ¬ç‰¹å¾è·¯å¾„ (text_emb å·²ç»æ˜¯ BERT CLS è¾“å‡º)
        if self.model.text_features_enabled:
            text_out = text_emb
        else:
            text_out = torch.zeros_like(tabular_out)

        # 4. å¤šè§†å›¾èåˆ
        multiview_out = self.model._fuse_multi_views(seq_out, text_out, tabular_out)

        # 5. åˆ†ç±»å™¨
        logits = self.model.classifier(multiview_out)
        return logits

# ==============================================================================
# 2. SHAP åˆ†æå™¨ç»„ä»¶ (ä¸»é€»è¾‘ç±») - å¢å¼ºç‰ˆæ”¯æŒæ–°æ¶æ„
# ==============================================================================
class ShapAnalyzer:
    def __init__(self, model, use_enhanced_mode: bool = True):
        """
        åˆå§‹åŒ– SHAP åˆ†æå™¨
        :param model: FlowBertMultiview æ¨¡å‹å®ä¾‹ (éœ€è¦è®¿é—®å…¶ config å’Œ encoders)
        :param use_enhanced_mode: æ˜¯å¦ä½¿ç”¨å¢å¼ºç‰ˆäº”å¤§ç‰¹å¾ç±»åˆ«åˆ†ææ¶æ„
        """
        self.model = model
        self.cfg = model.cfg
        self.buffer = []
        self.sample_limit = 100  # é™åˆ¶åˆ†ææ ·æœ¬æ•°ï¼Œé˜²æ­¢ OOM
        self.collected_count = 0
        self.enabled = True # å¯ä»¥é€šè¿‡é…ç½®å…³é—­
        
        # å¢å¼ºæ¨¡å¼é…ç½®
        self.use_enhanced_mode = use_enhanced_mode
        self.enhanced_analyzer = None
        
        if use_enhanced_mode:
            try:
                from .enhanced_shap_component import EnhancedShapAnalyzer
                self.enhanced_analyzer = EnhancedShapAnalyzer(
                    model=model, 
                    enable_five_tier_analysis=True
                )
                logger.info("ShapAnalyzerå·²å¯ç”¨å¢å¼ºç‰ˆäº”å¤§ç‰¹å¾ç±»åˆ«åˆ†ææ¨¡å¼")
            except ImportError as e:
                logger.warning(f"æ— æ³•å¯¼å…¥å¢å¼ºç‰ˆåˆ†æå™¨ï¼Œä½¿ç”¨ä¼ ç»Ÿæ¨¡å¼: {e}")
                self.use_enhanced_mode = False
        else:
            logger.info("ShapAnalyzerä½¿ç”¨ä¼ ç»Ÿåˆ†ææ¨¡å¼")

    def reset(self):
        """æ¯ä¸ª epoch å¼€å§‹æ—¶é‡ç½®ç¼“å†²åŒº"""
        self.buffer = []
        self.collected_count = 0
        
        # åŒæ­¥é‡ç½®å¢å¼ºç‰ˆåˆ†æå™¨
        if self.enhanced_analyzer is not None:
            self.enhanced_analyzer.reset()

    def collect_batch(self, batch):
        """æ”¶é›†æµ‹è¯•é˜¶æ®µçš„ Batch æ•°æ® (CPU ç¼“å­˜)"""
        if not self.enabled or self.collected_count >= self.sample_limit:
            return

        # ç§»åŠ¨åˆ° CPU å¹¶åˆ†ç¦»è®¡ç®—å›¾ï¼Œåªä¿ç•™æ•°æ®
        batch_cpu = {}
        batch_size = 0
        
        with torch.no_grad():
            for k, v in batch.items():
                if isinstance(v, torch.Tensor):
                    batch_cpu[k] = v.detach().cpu()
                    if batch_size == 0: batch_size = v.shape[0]
                elif isinstance(v, list):
                    batch_cpu[k] = v
                else:
                    batch_cpu[k] = v # å…ƒæ•°æ®ç­‰
        
        self.buffer.append(batch_cpu)
        self.collected_count += batch_size

    def finalize(self):
        """æµ‹è¯•ç»“æŸæ—¶æ‰§è¡Œ SHAP åˆ†æ"""
        # åªåœ¨ä¸»è¿›ç¨‹æ‰§è¡Œ
        if hasattr(self.model, 'trainer') and not self.model.trainer.is_global_zero:
            return

        if not self.buffer:
            logger.warning("[ShapComponent] æœªæ”¶é›†åˆ°æ ·æœ¬ï¼Œè·³è¿‡åˆ†æ")
            return

        logger.info("=" * 60)
        logger.info(f"ğŸ” [ShapComponent] å¼€å§‹æ‰§è¡Œç‰¹å¾å½’å› åˆ†æ (æ ·æœ¬æ•°: {self.collected_count})...")
        
        try:
            if self.use_enhanced_mode and self.enhanced_analyzer is not None:
                # ä½¿ç”¨å¢å¼ºç‰ˆäº”å¤§ç‰¹å¾ç±»åˆ«åˆ†æ
                logger.info("ğŸš€ ä½¿ç”¨å¢å¼ºç‰ˆäº”å¤§ç‰¹å¾ç±»åˆ«åˆ†ææ¶æ„")
                self.enhanced_analyzer.buffer = self.buffer
                self.enhanced_analyzer.collected_count = self.collected_count
                self.enhanced_analyzer.finalize()
            else:
                # ä½¿ç”¨ä¼ ç»Ÿåˆ†æ
                self._run_analysis()
        except Exception as e:
            logger.error(f"âŒ [ShapComponent] åˆ†æå¤±è´¥: {e}", exc_info=True)
        
        logger.info("=" * 60)

    def _run_analysis(self):
        import shap
        import matplotlib.pyplot as plt
        import seaborn as sns

        # ======================================================================
        # ğŸ”´ ç»ˆæä¿®å¤: é€€å‡º Inference Mode + å¼€å¯ Grad
        # PyTorch Lightning çš„ test é˜¶æ®µé»˜è®¤å¤„äº inference_mode (æ¯” no_grad æ›´å¼º)
        # å¿…é¡»æ˜¾å¼é€€å‡º inference_mode æ‰èƒ½æ„å»ºè®¡ç®—å›¾
        # ======================================================================
        with torch.inference_mode(False):  # 1. é€€å‡ºæ¨ç†æ¨¡å¼
            with torch.enable_grad():      # 2. å¼€å¯æ¢¯åº¦è®¡ç®—
                
                try:
                    # 3. å†æ¬¡ç¡®ä¿æ¨¡å‹å‚æ•°å…è®¸æ±‚å¯¼ (åŒé‡ä¿é™©)
                    for param in self.model.parameters():
                        param.requires_grad = True
                        
                    # 1. åˆå¹¶ Buffer æ•°æ®
                    combined_batch = self._merge_buffer()
                    
                    # 2. åˆ’åˆ†èƒŒæ™¯é›† (Background) å’Œ è§£é‡Šé›† (Eval)
                    total_samples = combined_batch['numeric_features'].shape[0]
                    bg_size = min(50, int(total_samples * 0.7))
                    eval_size = min(20, total_samples - bg_size)
                    
                    # 3. é¢„è®¡ç®— Embeddings
                    device = self.model.device
                    self.model.eval() # ä¿æŒ eval æ¨¡å¼ (Dropout ä¸éšæœº)

                    bg_inputs = self._precompute_embeddings(combined_batch, 0, bg_size, device)
                    eval_inputs = self._precompute_embeddings(combined_batch, bg_size, bg_size + eval_size, device)

                    # åˆå§‹åŒ– Wrapper
                    wrapper = ShapFusionWrapper(self.model)

                    # ==========================================================
                    # 4. è®¡ç®—å›¾è¿é€šæ€§è‡ªæ£€ (Sanity Check)
                    # ==========================================================
                    logger.info("[ShapComponent] æ‰§è¡Œè®¡ç®—å›¾è¿é€šæ€§è‡ªæ£€...")
                    
                    # æ‰‹åŠ¨å‰å‘ä¼ æ’­
                    test_out = wrapper(*eval_inputs)
                    
                    # æ£€æŸ¥è¾“å‡ºæ˜¯å¦ä¾èµ–äºè¾“å…¥
                    if test_out.grad_fn is None:
                        logger.error("âŒ [Fatal] Wrapper è¾“å‡ºæ²¡æœ‰ grad_fnï¼è®¡ç®—å›¾ä¾ç„¶æ–­è£‚ã€‚")
                        logger.error(f"å½“å‰æ¢¯åº¦çŠ¶æ€: is_grad_enabled={torch.is_grad_enabled()}, is_inference_mode={torch.is_inference_mode_enabled()}")
                        raise RuntimeError("æ— æ³•æ„å»ºè®¡ç®—å›¾ï¼Œè¯·æ£€æŸ¥ PyTorch ç‰ˆæœ¬æˆ– Lightning é…ç½®ã€‚")
                    
                    logger.info(f"âœ… è®¡ç®—å›¾æ£€æŸ¥é€šè¿‡! Output grad_fn: {test_out.grad_fn}")

                    # 5. åˆå§‹åŒ– DeepExplainer
                    explainer = shap.DeepExplainer(wrapper, bg_inputs)

                    logger.info("   æ­£åœ¨è®¡ç®— SHAP å€¼ (DeepLIFT)...")
                    # ==================================================================
                    # ğŸ”´ å…³é”®ä¿®å¤: å…³é—­åŠ æ€§æ£€æŸ¥ (check_additivity=False)
                    # åŸå› : æ¨¡å‹åŒ…å« LayerNorm/GELU ç­‰éçº¿æ€§å±‚ï¼ŒDeepLIFT æ— æ³•å®Œç¾åˆ†è§£ï¼Œ
                    # å¯¼è‡´è´¡çŒ®å€¼ä¹‹å’Œä¸æ¨¡å‹è¾“å‡ºå­˜åœ¨åå·®ã€‚è¿™æ˜¯ Transformer æ¨¡å‹çš„å¸¸è§ç°è±¡ã€‚
                    # ==================================================================
                    shap_values = explainer.shap_values(eval_inputs, check_additivity=False)
                    
                    # 6. å¤„ç†è¾“å‡ºæ ¼å¼
                    target_shap = shap_values
                    if isinstance(shap_values, list) and not isinstance(shap_values[0], np.ndarray) and not torch.is_tensor(shap_values[0]):
                        if len(shap_values) > 1:
                            target_shap = shap_values[1]

                    # 7. èšåˆä¸ç»˜å›¾
                    feature_importance = self._aggregate_importance(target_shap)
                    self._plot_results(feature_importance)

                except Exception as e:
                    logger.error(f"SHAP åˆ†æè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)

    def _merge_buffer(self):
        combined = {}
        keys = self.buffer[0].keys()
        for k in keys:
            val = self.buffer[0][k]
            if isinstance(val, torch.Tensor):
                combined[k] = torch.cat([b[k] for b in self.buffer], dim=0)
            elif isinstance(val, list):
                combined[k] = [item for b in self.buffer for item in b[k]]
        return combined

    def _precompute_embeddings(self, batch, start, end, device):
        """é¢„è®¡ç®— Embeddingï¼Œè¿”å› [Num, Dom, Seq, Text] çš„ Tensor åˆ—è¡¨"""
        # æ„å»ºåˆ‡ç‰‡å¹¶ç§»è‡³ GPU
        slice_batch = {}
        for k, v in batch.items():
            if isinstance(v, torch.Tensor):
                slice_batch[k] = v[start:end].to(device)
            elif isinstance(v, list):
                slice_batch[k] = v[start:end]
        
        # 1. æå– Embedding (åœ¨ no_grad ä¸‹è¿›è¡Œï¼Œé¿å…è®¡ç®—å›¾è¿‡æ·±)
        with torch.no_grad():
            # A. Numeric
            num_feats = slice_batch['numeric_features']
            
            # B. Domain
            if self.model.domain_embedding_enabled:
                dom_feats = slice_batch['domain_embedding_features']
            else:
                dom_feats = torch.zeros(num_feats.shape[0], 0, device=device)
            
            # C. Sequence
            if self.model.sequence_features_enabled:
                seq_data = {
                    'iat_times': slice_batch['iat_times'],
                    'payload_sizes': slice_batch['payload_sizes'],
                    'sequence_mask': slice_batch['sequence_mask']
                }
                # æ³¨æ„ï¼šè¿™é‡Œè°ƒç”¨çš„æ˜¯ encoderï¼Œä¸æ˜¯ projection
                seq_emb = self.model.sequence_encoder(seq_data)["sequence_embedding"]
            else:
                seq_emb = torch.zeros(num_feats.shape[0], self.cfg.model.sequence.embedding_dim, device=device)
            
            # D. Text
            if self.model.text_features_enabled:
                text_emb = self.model._process_text_features(slice_batch)
            else:
                text_emb = torch.zeros(num_feats.shape[0], self.model.bert_config.hidden_size, device=device)
        
        # ======================================================================
        # ğŸ”´ ç»ˆæä¿®å¤ï¼šå¼ºåˆ¶ç±»å‹è½¬æ¢ + æ˜¾å¼å¼€å¯æ¢¯åº¦
        # ======================================================================
        
        # ======================================================================
        # ğŸ”´ ç»ˆæä¿®å¤ï¼šå¼ºåˆ¶ç±»å‹è½¬æ¢ + æ˜¾å¼å¼€å¯æ¢¯åº¦
        # ======================================================================
        raw_inputs = [num_feats, dom_feats, seq_emb, text_emb]
        final_inputs = []
        
        for t in raw_inputs:
            t = t.detach().clone()
            if not t.is_floating_point(): # ç¡®ä¿æ˜¯æµ®ç‚¹æ•°
                t = t.float()
            t.requires_grad_(True) # æ ‡è®°éœ€è¦æ¢¯åº¦
            final_inputs.append(t)

        # éªŒè¯æ‰€æœ‰è¾“å…¥å¼ é‡éƒ½å¯ç”¨äº†æ¢¯åº¦
        for i, t in enumerate(final_inputs):
            logger.debug(f"è¾“å…¥ {i} (shape: {t.shape}) requires_grad: {t.requires_grad}")

        return final_inputs

    def _aggregate_importance(self, target_shap):
        """èšåˆ SHAP å€¼åˆ°ç‰¹å¾åå’Œè§†å›¾"""
        importance = {}
        
        # æ•°å€¼ç‰¹å¾ (1å¯¹1)
        shap_num = target_shap[0]
        if torch.is_tensor(shap_num): shap_num = shap_num.cpu().numpy()
        mean_shap_num = np.abs(shap_num).mean(axis=0)
        
        feat_names = self.cfg.data.numeric_features.flow_features
        for i, val in enumerate(mean_shap_num):
            if i < len(feat_names):
                importance[feat_names[i]] = float(val)

        # è§†å›¾èšåˆå‡½æ•°
        def agg_view(tensor, name):
            if torch.is_tensor(tensor): tensor = tensor.cpu().numpy()
            score = np.abs(tensor).sum(axis=1).mean() # Sum dim -> Mean batch
            importance[name] = float(score)

        if self.model.domain_embedding_enabled:
            agg_view(target_shap[1], 'View: Domain')
        if self.model.sequence_features_enabled:
            agg_view(target_shap[2], 'View: Packet Seq')
        if self.model.text_features_enabled:
            agg_view(target_shap[3], 'View: Text Metadata')
            
        return importance

    def _plot_results(self, importance):
        import matplotlib.pyplot as plt
        import seaborn as sns
        
        # è®¾ç½®matplotlibä¸ºé™é»˜æ¨¡å¼
        import logging
        mpl_logger = logging.getLogger('matplotlib')
        font_logger = logging.getLogger('matplotlib.font_manager')
        original_level = mpl_logger.level
        original_font_level = font_logger.level
        mpl_logger.setLevel(logging.WARNING)
        font_logger.setLevel(logging.WARNING)
        
        try:
            # è®¾ç½®Times New Romanå­—ä½“
            plt.rcParams['font.family'] = 'Times New Roman'
            plt.rcParams['font.serif'] = ['Times New Roman', 'Times', 'DejaVu Serif']
            plt.rcParams['font.sans-serif'] = ['DejaVu Sans']
            plt.rcParams['axes.unicode_minus'] = False
            plt.rcParams['font.size'] = 12
            plt.rcParams['axes.titlesize'] = 16
            plt.rcParams['axes.labelsize'] = 14
            plt.rcParams['xtick.labelsize'] = 12
            plt.rcParams['ytick.labelsize'] = 12
            plt.rcParams['legend.fontsize'] = 12
            logger.info("ShapComponentå­—ä½“å·²è®¾ç½®ä¸ºTimes New Roman")
        except Exception as e:
            logger.debug(f"ShapComponentå­—ä½“è®¾ç½®å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å­—ä½“: {e}")
            plt.rcParams['font.family'] = 'serif'
        
        # è·¯å¾„è®¾ç½®
        if hasattr(self.model.logger, 'log_dir'):
            save_dir = os.path.join(self.model.logger.log_dir, "shap_analysis")
        else:
            save_dir = "shap_analysis"
        os.makedirs(save_dir, exist_ok=True)

        # è½¬æ¢ä¸º DataFrame
        df = pd.DataFrame(list(importance.items()), columns=['Feature', 'Importance'])
        
        # ======================================================================
        # 1. Top 20 æ£’å›¾ (Bar Chart) - ç°åº¦+çº¹ç†ç‰ˆæœ¬
        # ç›®æ ‡: åªå±•ç¤ºå…·ä½“çš„ç‰¹å¾ï¼Œæ’é™¤å®è§‚çš„ "View: ..." èšåˆç‰¹å¾
        # ======================================================================
        
        # è¿‡æ»¤æ‰æ‰€æœ‰ä»¥ "View:" å¼€å¤´çš„ç‰¹å¾å
        df_bar = df[~df['Feature'].str.startswith('View:')]
        
        # æ’åºå¹¶å–å‰ 20
        df_bar = df_bar.sort_values(by='Importance', ascending=False).head(20)

        plt.figure(figsize=(12, 10))
        if len(df_bar) > 0:
            # åˆ›å»ºç°åº¦è‰²å½©
            colors = ['#404040'] * len(df_bar)  # æ·±ç°è‰²
            
            # åˆ›å»ºä¸åŒçš„çº¹ç†æ¨¡å¼
            hatches = ['///', '\\\\\\\\', '|||', '---', '+++', '...', 'xxx', 'ooo', '///', '\\\\\\\\', 
                      '|||', '---', '+++', '...', 'xxx', 'ooo', '///', '\\\\\\\\', '|||', '---']
            hatches = hatches[:len(df_bar)]
            
            # ç»˜åˆ¶æ¡å½¢å›¾
            ax = sns.barplot(x='Importance', y='Feature', data=df_bar, palette=colors, 
                           edgecolor='black', linewidth=1.0)
            
            # æ·»åŠ çº¹ç†åˆ°æ¯ä¸ªæ¡å½¢
            for i, bar in enumerate(ax.patches):
                bar.set_hatch(hatches[i])
                bar.set_alpha(0.9)
            
            # æ·»åŠ æ•°å­—æ ‡ç­¾
            for i, (imp, _) in enumerate(zip(df_bar['Importance'], df_bar['Feature'])):
                ax.text(imp + max(df_bar['Importance']) * 0.01, i, f'{imp:.2f}', 
                       ha='left', va='center', fontsize=10, fontweight='bold')
            
            # plt.title("SHAP Feature Importance (Top 20 Features)", fontsize=16, fontweight='bold', pad=20)
            plt.xlabel("mean(|SHAP value|)", fontsize=14, fontweight='bold')
            plt.ylabel("Feature Name", fontsize=14, fontweight='bold')
            plt.tight_layout()
            plt.savefig(os.path.join(save_dir, "shap_top20.png"), dpi=300, bbox_inches='tight', 
                       facecolor='white', edgecolor='none')
        else:
            logger.warning("[ShapComponent] æ²¡æœ‰å…·ä½“çš„æ•°å€¼ç‰¹å¾å¯ä¾›ç»˜åˆ¶ Top 20 æ£’å›¾")
        plt.close()

        # ======================================================================
        # 2. è§†å›¾é¥¼å›¾ (Pie Chart) - ç°åº¦+å¯†é›†çº¹ç†ç‰ˆæœ¬
        # ç›®æ ‡: å±•ç¤ºå®è§‚è§†å›¾çš„è´¡çŒ® (Numeric vs Sequence vs Text vs Domain)
        # ======================================================================
        
        view_scores = {'Numeric': 0.0, 'Sequence': 0.0, 'Text': 0.0, 'Domain': 0.0}
        
        # ä½¿ç”¨åŸå§‹çš„ df (åŒ…å« View ç‰¹å¾) è¿›è¡Œç»Ÿè®¡
        for idx, row in df.iterrows():
            name = row['Feature']
            val = row['Importance']
            
            # å¦‚æœæ˜¯èšåˆç‰¹å¾ï¼Œç›´æ¥åŠ åˆ°å¯¹åº”çš„è§†å›¾åˆ†æ•°ä¸­
            if 'View: Packet Seq' in name: 
                view_scores['Sequence'] += val
            elif 'View: Text Metadata' in name: 
                view_scores['Text'] += val
            elif 'View: Domain' in name: 
                view_scores['Domain'] += val
            # å¦‚æœä¸æ˜¯èšåˆç‰¹å¾ï¼ˆå³å…·ä½“çš„æ•°å€¼ç‰¹å¾ï¼‰ï¼ŒåŠ åˆ° Numeric åˆ†æ•°ä¸­
            elif not name.startswith('View:'): 
                view_scores['Numeric'] += val
        
        # è¿‡æ»¤æ‰è´¡çŒ®æå°çš„è§†å›¾
        view_scores = {k: v for k, v in view_scores.items() if v > 1e-6}
        
        if view_scores:
            plt.figure(figsize=(10, 8))
            
            # ç°åº¦è‰²å½©æ–¹æ¡ˆ
            colors = ['#404040', '#606060', '#808080', '#A0A0A0']
            colors = colors[:len(view_scores)]
            
            # å¯†é›†çº¹ç†æ¨¡å¼
            hatches = ['///', '\\\\\\\\', '|||', '---']
            hatches = hatches[:len(view_scores)]
            
            # ç”Ÿæˆé¥¼å›¾
            wedges, texts, autotexts = plt.pie(view_scores.values(), labels=view_scores.keys(), 
                                             colors=colors, autopct='%1.1f%%', 
                                             startangle=90, shadow=False,
                                             textprops={'fontsize': 14, 'fontweight': 'bold'})
            
            # ä¸ºæ¯ä¸ªé¥¼å›¾å—æ·»åŠ å¯†é›†çº¹ç†
            for i, wedge in enumerate(wedges):
                wedge.set_hatch(hatches[i])
                wedge.set_edgecolor('black')
                wedge.set_linewidth(1.0)
            
            # plt.title("Global View Contribution", fontsize=18, fontweight='bold', pad=30)
            plt.tight_layout()
            plt.savefig(os.path.join(save_dir, "shap_view_pie.png"), dpi=300, bbox_inches='tight', 
                       facecolor='white', edgecolor='none')
            plt.close()
        
        logger.info(f"âœ… [ShapComponent] å›¾è¡¨å·²ä¿å­˜è‡³: {save_dir}")
        
        # æ¢å¤åŸå§‹æ—¥å¿—çº§åˆ«
        mpl_logger.setLevel(original_level)
        font_logger.setLevel(original_font_level)