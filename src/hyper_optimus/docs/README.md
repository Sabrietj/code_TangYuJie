# æ¶ˆèå®éªŒæ¡†æ¶

ç»Ÿä¸€çš„æ¶ˆèå®éªŒæ‰§è¡Œæ¡†æ¶ï¼Œæ”¯æŒå¤šè§†å›¾ç½‘ç»œæµé‡åˆ†ç±»æ¨¡å‹çš„ç³»ç»Ÿæ€§æ¶ˆèç ”ç©¶ã€‚

## ğŸš€ åŠŸèƒ½ç‰¹æ€§

- **ç»Ÿä¸€é…ç½®ç®¡ç†**: åŸºäºYAMLçš„å®éªŒé…ç½®ï¼Œæ”¯æŒç‰¹å¾æ¶ˆèã€èåˆæ¶ˆèã€æŸå¤±æ¶ˆè
- **æ™ºèƒ½é…ç½®è½¬æ¢**: åŠ¨æ€å‚æ•°è¦†ç›–ï¼Œæ”¯æŒå‚æ•°åˆ é™¤å’ŒåŠ¨æ€è¦†ç›–ï¼Œæ”¯æŒå¤–éƒ¨é…ç½®æ˜ å°„æ–‡ä»¶
- **W&Bé›†æˆ**: å®Œæ•´çš„å®éªŒè·Ÿè¸ªï¼Œæ”¯æŒæ¯ä¸ªepochçš„å®æ—¶æŒ‡æ ‡é‡‡é›†å’ŒSHAPåˆ†æç»“æœä¸Šä¼ 
- **å¤šç§æ‰§è¡Œæ¨¡å¼**: æ”¯æŒä¸²è¡Œ/å¹¶è¡Œæ‰§è¡Œã€æ ‡å‡†å®éªŒæ¨¡å¼å’Œæ¶ˆèå®éªŒæ¨¡å¼
- **ç»“æœç”Ÿæˆå™¨**: ç‹¬ç«‹çš„ç»“æœç”Ÿæˆå’Œè§£æå·¥å…·ï¼Œæ”¯æŒä»å·²æœ‰æ—¥å¿—ç”Ÿæˆfinal_results.json
- **å®Œå–„çš„CLIå·¥å…·**: ç»Ÿä¸€å‘½ä»¤è¡Œæ¥å£ï¼Œæ”¯æŒç”Ÿæˆã€ä¸Šä¼ ã€è¿è¡Œç­‰å¤šç§æ“ä½œ
- **è‡ªåŠ¨åŒ–æŠ¥å‘Š**: è‡ªåŠ¨ç”Ÿæˆå®éªŒæŠ¥å‘Šå’Œç»“æœæ±‡æ€»
- **é”™è¯¯æ¢å¤**: å®Œå–„çš„é”™è¯¯å¤„ç†å’Œæ¢å¤æœºåˆ¶
- **å˜ä½“æ ‡è¯†ç³»ç»Ÿ**: æ™ºèƒ½çš„æ¶ˆèå˜ä½“IDç”Ÿæˆå’Œè§£æ
- **æ¨¡å—åŒ–è®¾è®¡**: æ¸…æ™°çš„æ¨¡å—åˆ†ç¦»ï¼Œä¾¿äºæ‰©å±•å’Œç»´æŠ¤

## ğŸ“ ç›®å½•ç»“æ„

```
src/hyper_optimus/
â”œâ”€â”€ configs/                 # é…ç½®æ–‡ä»¶ç›®å½•
â”œâ”€â”€ docs/                   # æ–‡æ¡£ç›®å½•  
â”œâ”€â”€ experiment/              # å®éªŒæ‰§è¡Œæ ¸å¿ƒ
â”‚   â”œâ”€â”€ __main__.py        # æ¨¡å—ä¸»å…¥å£
â”‚   â”œâ”€â”€ cli_tools.py        # ç»Ÿä¸€CLIå·¥å…·æ¥å£
â”‚   â”œâ”€â”€ experiment_executor.py  # å®éªŒæ‰§è¡Œå™¨
â”‚   â”œâ”€â”€ config_converter.py     # é…ç½®è½¬æ¢å™¨
â”‚   â”œâ”€â”€ wandb_integration.py    # W&Bé›†æˆ
â”‚   â”œâ”€â”€ result_generator.py     # ç»“æœç”Ÿæˆå™¨
â”‚   â”œâ”€â”€ variant_identifier.py    # å˜ä½“æ ‡è¯†ç³»ç»Ÿ
â”‚   â””â”€â”€ run_ablation_exp.py   # ä¸»æ‰§è¡Œè„šæœ¬
â””â”€â”€ shap_analysis/          # SHAPåˆ†ææ¨¡å—
    â”œâ”€â”€ universal_analyzer.py    # é€šç”¨SHAPåˆ†æå™¨
    â”œâ”€â”€ analysis_strategies.py  # åˆ†æç­–ç•¥
    â”œâ”€â”€ feature_classifier.py   # ç‰¹å¾åˆ†ç±»å™¨
    â””â”€â”€ ...
```
### å®Œæ•´æ•°æ®æµç¨‹

  è¾“å…¥é…ç½®æ–‡ä»¶ (exp_config.yaml)
         â†“
   é…ç½®éªŒè¯å’Œè½¬æ¢
         â†“  
   å®éªŒå˜ä½“è§£æ (VariantIdentifier)
         â†“
   è®­ç»ƒå‘½ä»¤æ„å»º (ConfigConverter)
         â†“
   æ¨¡å‹è®­ç»ƒæ‰§è¡Œ (ExperimentExecutor)
         â†“
   å®æ—¶æŒ‡æ ‡ä¸Šä¼  (WandBIntegration) â†â”€â”
         â†“                          â”‚
   æ—¥å¿—è§£æ (ResultGenerator)        â”‚
         â†“                          â”‚
   SHAPåˆ†æ (UniversalSHAPAnalyzer) â”€â”˜
         â†“
   ç»“æœæ±‡æ€»å’ŒæŠ¥å‘Šç”Ÿæˆ
         â†“
   æœ€ç»ˆè¾“å‡º (JSON + Markdown + W&B)


## ğŸ› ï¸ å®‰è£…ä¾èµ–

```bash
pip install pyyaml wandb torch pytorch-lightning transformers hydra omegaconf psutil GPUtil tensorboard
```

### å¯é€‰ä¾èµ–

```bash
# GPUç›‘æ§
pip install GPUtil

# TensorBoardæ—¥å¿—è§£æ
pip install tensorboard
```

## ğŸ“– ä½¿ç”¨æ–¹æ³•

### Python è™šæ‹ŸåŒ–ç¯å¢ƒ

cd /data/qinyf/code-multiview-network-traffic-classification-model/
source myvenv/bin/activate

### è¿è¡Œå®éªŒ

```bash
# ä»…éªŒè¯é…ç½®ï¼Œä¸æ‰§è¡Œå®éªŒ(æŒ‡å®šé…ç½®æ–‡ä»¶exp_config.yaml)
python src/hyper_optimus/experiment/run_ablation_exp.py --config src/hyper_optimus/configs/exp_config.yaml --dry-run

# å¹¶è¡Œæ‰§è¡Œï¼ˆ2ä¸ªå®éªŒåŒæ—¶è¿è¡Œï¼‰ é»˜è®¤å•ä¸ªå®éªŒè¿è¡Œ
python src/hyper_optimus/experiment/run_ablation_exp.py --config src/hyper_optimus/configs/exp_config.yaml --parallel 2

# è¿è¡Œå®éªŒå¹¶æŒ‡å®šè¾“å‡ºç›®å½•ï¼Œé»˜è®¤è¾“å‡ºåˆ°æ ¹ç›®å½• ./ablation_results
python src/hyper_optimus/experiment/run_ablation_exp.py --config src/hyper_optimus/configs/exp_config.yaml --output-dir ./my_results

# æ£€æŸ¥ä¾èµ–åŒ…
python src/hyper_optimus/experiment/run_ablation_exp.py --check-deps

# è‡ªå®šä¹‰W&Bé¡¹ç›®
python src/hyper_optimus/experiment/run_ablation_exp.py --config src/hyper_optimus/configs/exp_config.yaml --wandb-project my-project --wandb-entity my-entity

# è®¾ç½®æ—¥å¿—çº§åˆ«å’Œæ—¥å¿—æ–‡ä»¶
python src/hyper_optimus/experiment/run_ablation_exp.py --config src/hyper_optimus/configs/exp_config.yaml --log-level DEBUG --log-file experiment.log

# å¯ç”¨æ‰¹é‡W&Bä¸Šä¼ 
python src/hyper_optimus/experiment/run_ablation_exp.py --config src/hyper_optimus/configs/exp_config.yaml --batch-wandb-upload

# å¯ç”¨å»¶è¿ŸæŠ¥å‘Šç”Ÿæˆ
python src/hyper_optimus/experiment/run_ablation_exp.py --config src/hyper_optimus/configs/exp_config.yaml --enable-delayed-report
```

### å®éªŒè¿è¡Œç»“æœå¤„ç†
# æ‰‹åŠ¨ç”Ÿæˆæœ€ç»ˆç»“æœæ–‡ä»¶(final_results.jsonï¼Œ experiment_report.md)
# æ‰‹åŠ¨ä¸Šä¼ W&B

```bash
# ç”Ÿæˆæœ€æ–°å®éªŒçš„ç»“æœæ–‡ä»¶(ablation_results)
python -m src.hyper_optimus.experiment generate --latest

# ç”ŸæˆæŒ‡å®šç›®å½•çš„ç»“æœæ–‡ä»¶
python -m src.hyper_optimus.experiment generate --results-dir ablation_results/suite_20251127_123456

# ä»…éªŒè¯ç›®å½•ç»“æ„
python -m src.hyper_optimus.experiment generate --results-dir suite_XXX --validate-only

# ä¸Šä¼ æœ€æ–°å®éªŒç»“æœåˆ°W&B
python -m src.hyper_optimus.experiment upload --latest

# ä¸Šä¼ æŒ‡å®šç›®å½•ç»“æœ
python -m src.hyper_optimus.experiment upload --results-dir ablation_results/suite_20251127_123456

# Dry-runæ¨¡å¼éªŒè¯æ–‡ä»¶æ ¼å¼
python -m src.hyper_optimus.experiment upload --results-file final_results.json --dry-run

# æŒ‡å®šW&Bé¡¹ç›®
python -m src.hyper_optimus.experiment run --config src/hyper_optimus/configs/exp_config.yaml --wandb-project my-project --wandb-entity my-entity

# æŸ¥çœ‹å¸®åŠ©
python -m src.hyper_optimus.experiment --help
```


## âš™ï¸ é…ç½®æ–‡ä»¶æ ¼å¼

### åŸºç¡€é…ç½®ç»“æ„

###  åŸºäºhydra çš„ æ¨¡å‹é…ç½®æ–‡ä»¶ä½“ç³» 

# æ¨¡å‹ä¸»å‡½æ•°å¢åŠ è£…é¥°å™¨,é€šè¿‡hydra.main è£…é¥°å™¨ä¼ å…¥é…ç½®æ–‡ä»¶è·¯å¾„å’Œåç§°
@hydra.main(config_path="./config", config_name="flow_bert_multiview_config", version_base=None)

# å®éªŒé…ç½®æ–‡ä»¶
é€šè¿‡å‘½ä»¤è¡Œå‚æ•°ï¼Œä¼ é€’éœ€è¦è¦†ç›–ä¿®æ”¹çš„é…ç½®å‚æ•°ï¼Œä»¥æ»¡è¶³è‡ªåŠ¨è¶…å‚æœç´¢ï¼Œæ¶ˆèå®éªŒçš„éœ€è¦ã€‚
ä½†è¿è¡Œè¿™äº›å®éªŒçš„åŸºç¡€é…ç½®å‚æ•°ï¼Œè¿˜æ˜¯ä½¿ç”¨æ¨¡å‹è‡ªå·±å®šä¹‰çš„é…ç½®æ–‡ä»¶ï¼Œä¸€èˆ¬å®åœ¨æ¨¡å‹çš„configæ–‡ä»¶å¤¹ä¸‹ã€‚

- å®éªŒç”¨çš„é…ç½®æ–‡ä»¶çš„æ ¼å¼ï¼š

```yaml
# é€šç”¨é…ç½®ï¼Œä¼šè¦†ç›–æ¨¡å‹é…ç½®æ–‡ä»¶ä¸­çš„åŒåå‚æ•°
data:
  data_path: "/path/to/dataset.csv"
  batch_size: 1024
  num_workers: 8

training:
  max_epochs: 50
  patience: 8

# å®éªŒé…ç½®
experiment:
  model_name: flow_bert_multiview        # æ¨¡å‹åç§°, åˆ°å¯¹åº”çš„æ¨¡å‹ç›®å½•ä¸‹(models),æ‰¾åˆ°å¯¹åº”çš„config.yaml,æ•°æ®å¤„ç†è„šæœ¬,è®­ç»ƒè„šæœ¬(train.py)
  ablation_strategy: "full"               # æ¶ˆèç­–ç•¥: standard(ä»…åŸºçº¿), full(å…¨éƒ¨ä½¿èƒ½å®éªŒ), selective(é€‰æ‹©æ€§)
  enable:                                 # å®éªŒé…ç½®, å®éªŒç±»å‹ä½¿èƒ½å¼€å…³
    feature_ablation: true                # å®éªŒé…ç½®, æ˜¯å¦å¼€å¯ç‰¹å¾æ¶ˆè 
    fusion_ablation: true                 # å®éªŒé…ç½®, æ˜¯å¦å¼€å¯èåˆæ¶ˆè
    loss_ablation:   true                 # å®éªŒé…ç½®, æ˜¯å¦å¼€å¯æŸå¤±æ¶ˆè
```

### æ¶ˆèå®éªŒå˜ä½“

#### ç‰¹å¾æ¶ˆè (feature_ablation)

```yaml
ablation_variants:
  FT1
    name: "æµåŸºçº¿æ¨¡å‹"
    description: "æ•°å€¼ç‰¹å¾é…ç½®"
    type: "feature_ablation"
    section: "data"
    config:
      enabled_features:
        sequence_features: false
        domain_name_embedding_features: false
        text_features: false
    baseline: true
```

#### èåˆæ¶ˆè (fusion_ablation)

```yaml
  FU1
    name: "æ‹¼æ¥"
    description: "å¤šè§†å›¾æ‹¼æ¥èåˆ"
    type: "fusion_ablation"
    section: "fusion"
    method: "concat"  # concat, cross_attention, weighted_sum
```

#### æŸå¤±æ¶ˆè (loss_ablation)

```yaml
  LS1
    name: "é¢„æµ‹æŸå¤±"
    description: "ä»…é¢„æµ‹æŸå¤±"
    type: "loss_ablation"
    model_name: "flow_bert_multiview_ssl"
```

## ğŸ”„ é…ç½®è½¬æ¢æœºåˆ¶

### å¤–éƒ¨é…ç½®æ˜ å°„æ–‡ä»¶

æ¡†æ¶ä½¿ç”¨ `config_mapping.yaml` æ–‡ä»¶å®šä¹‰é…ç½®æ˜ å°„å…³ç³»ï¼ˆå®éªŒé…ç½®->æ¨¡å‹é…ç½®ï¼‰ï¼š

```yaml
# æ”¯æŒæ¨¡å‹ç‰¹å®šçš„æ˜ å°„
data:
  data_path:
    flow_bert_multiview: "data.flow_data_path"
    flow_autoencoder: "data.data_path"
    default: "data.data_path"

training:
  max_epochs:
    default: "training.max_epochs"
  
  patience:
    flow_bert_multiview: "training.early_stopping.patience"
    default: "training.patience"

multiview:
  sequence_features:
    default: "data.sequence_features"
  text_features:
    default: "data.text_features"
```

### å‚æ•°è¦†ç›–è·¯å¾„()
## æ³¨æ„ï¼š å¦‚æœæ–°å¢åŠ æ¨¡å‹ï¼Œå°½é‡ç¡®ä¿æ¨¡å‹é…ç½®æ–‡ä»¶ä¸­ï¼Œå‚æ•°å®šä¹‰çš„ç¨³å®šï¼Œè¦ä¸ç„¶ä¼šå¯¼è‡´æ˜ å°„å…³ç³»ä¸ç¨³å®šï¼Œéœ€è¦æ‰‹åŠ¨ç»´æŠ¤æ˜ å°„å…³ç³»ã€‚
```python
# æ”¯æŒå¤šç§é…ç½®è·¯å¾„æ˜ å°„
'enabled_features.text_features' -> 'data.text_features.enabled'
'enabled_features.sequence_features' -> 'data.sequence_features.enabled'
'fusion.method' -> 'model.fusion.method'
'training.max_epochs' -> 'training.max_epochs' (flow_bert_multiview)
'training.max_epochs' -> 'training.early_stopping.patience' (flow_autoencoder)
```


## ğŸ“Š W&Bé›†æˆ

### å®æ—¶æŒ‡æ ‡é‡‡é›†

æ¯ä¸ªè®­ç»ƒepochçš„æŒ‡æ ‡éƒ½ä¼šå®æ—¶ä¸Šä¼ åˆ°W&Bï¼š

- **è®­ç»ƒæŒ‡æ ‡**: `train/loss`, `train/accuracy`, `train/f1_score`, `train/precision`, `train/recall`, `train/learning_rate`
- **éªŒè¯æŒ‡æ ‡**: `val/loss`, `val/accuracy`, `val/f1_score`, `val/precision`, `val/recall`, `val/auc`
- **æµ‹è¯•æŒ‡æ ‡**: `test/loss`, `test/accuracy`, `test/f1_score`, `test_precision`, `test_recall`, `test_roc_auc`
- **ç³»ç»Ÿèµ„æº**: `system/cpu_percent`, `system/memory_percent`, `system/gpu_utilization`
- **SHAPåˆ†æ**: `shap/feature_importance`, `shap/summary_plot`
- **å®éªŒä¸Šä¸‹æ–‡**: `experiment_name`, `model_type`, `ablation_config`, `variant_id`

### å®éªŒç»„ç»‡

```
W&B Project: multiview-ablation-studies
â”œâ”€â”€ suite_20251127_143022_FT1_baseline_143025/
â”œâ”€â”€ suite_20251127_143022_FT2_text_features_143045/
â””â”€â”€ suite_20251127_143022_FU1_concat_fusion_143108/
```

### æ ‡ç­¾ç³»ç»Ÿ

- `feature_ablation`: ç‰¹å¾æ¶ˆèå®éªŒ
- `fusion_ablation`: èåˆæ¶ˆèå®éªŒ
- `loss_ablation`: æŸå¤±æ¶ˆèå®éªŒ
- `baseline`: åŸºçº¿å®éªŒ
- `{model_name}`: æ¨¡å‹ç±»å‹
- `{variant_id}`: å˜ä½“æ ‡è¯†ç¬¦

### SHAPç»“æœä¸Šä¼ 

æ¡†æ¶è‡ªåŠ¨å¤„ç†SHAPåˆ†æç»“æœå¹¶ä¸Šä¼ åˆ°W&Bï¼š

```python
# è‡ªåŠ¨ä¸Šä¼ SHAPå›¾åƒ
wandb_run.log({
    "shap/summary_plot": wandb.Image("shap_results/summary_plot.png"),
    "shap/feature_importance": wandb.Image("shap_results/feature_importance.png")
})

# è‡ªåŠ¨ä¸Šä¼ SHAPæ•°æ®
with open("shap_results/shap_values.json", 'r') as f:
    shap_data = json.load(f)
    wandb_run.log({"shap/values": shap_data})
```

## ğŸ“ˆ è¾“å‡ºç»“æœ

### å®éªŒè¾“å‡ºç›®å½•

```
ablation_results/
â”œâ”€â”€ suite_20251127_143022/
â”‚   â”œâ”€â”€ FT1_baseline/
â”‚   â”‚   â”œâ”€â”€ .hydra/
â”‚   â”‚   â”œâ”€â”€ training.log
â”‚   â”‚   â”œâ”€â”€ shap_results/
â”‚   â”‚   â”‚   â”œâ”€â”€ summary_plot.png
â”‚   â”‚   â”‚   â””â”€â”€ shap_values.json
â”‚   â”‚   â””â”€â”€ tensorboard/
â”‚   â”œâ”€â”€ FU1_concat_fusion/
â”‚   â”œâ”€â”€ LS1_prediction_loss/
â”‚   â”œâ”€â”€ experiment_config.yaml
â”‚   â”œâ”€â”€ experiment_report.md   # å®éªŒæŠ¥å‘Š,åŒ…å«å®éªŒID,çŠ¶æ€,æœ€ç»ˆå‡†ç¡®ç‡,æœ€ç»ˆF1åˆ†æ•°,è®­ç»ƒæ—¶é—´
â”‚   â”œâ”€â”€ intermediate_results.json
â”‚   â””â”€â”€ final_results.json     # æœ€ç»ˆç»“æœ,epoch_results, test_results, shap_results
```

### è‡ªåŠ¨ç”ŸæˆæŠ¥å‘Š

å®éªŒå®Œæˆåè‡ªåŠ¨ç”ŸæˆMarkdownæŠ¥å‘Šï¼š

```markdown
# æ¶ˆèå®éªŒæŠ¥å‘Š

**ç”Ÿæˆæ—¶é—´**: 2025-11-27 14:30:22
**å®éªŒæ€»æ•°**: 3
**æˆåŠŸå®éªŒ**: 3
**å¤±è´¥å®éªŒ**: 0

## å®éªŒç»“æœæ±‡æ€»

| å®éªŒID             | çŠ¶æ€          | æœ€ç»ˆå‡†ç¡®ç‡    | æœ€ç»ˆF1åˆ†æ•°    | è®­ç»ƒæ—¶é—´  |
|-------------------|--------------|--------------|--------------|----------|
| FT1               | âœ… completed | 0.8234       | 0.8012       | 45.2s    |
| FT2               | âœ… completed | 0.8567       | 0.8423       | 52.1s    |
| FU1               | âœ… completed | 0.8345       | 0.8198       | 48.7s    |
```

### è§£æçš„æŒ‡æ ‡ç±»å‹

- **è®­ç»ƒæŒ‡æ ‡**: è®­ç»ƒæŸå¤±ã€å‡†ç¡®ç‡ã€ç²¾ç¡®ç‡ã€å¬å›ç‡ã€F1åˆ†æ•°
- **éªŒè¯æŒ‡æ ‡**: éªŒè¯æŸå¤±ã€å‡†ç¡®ç‡ã€ç²¾ç¡®ç‡ã€å¬å›ç‡ã€F1åˆ†æ•°ã€AUC
- **æµ‹è¯•æŒ‡æ ‡**: æµ‹è¯•å‡†ç¡®ç‡ã€F1åˆ†æ•°ã€ç²¾ç¡®ç‡ã€å¬å›ç‡ã€ROC-AUC
- **ç³»ç»ŸæŒ‡æ ‡**: CPUä½¿ç”¨ç‡ã€å†…å­˜ä½¿ç”¨ã€GPUåˆ©ç”¨ç‡ã€è®­ç»ƒæ—¶é•¿
- **ä¸šåŠ¡æŒ‡æ ‡**: ååé‡ã€å»¶è¿Ÿã€æ¨ç†æ—¶é—´ã€æ¨¡å‹å¤§å°
- **SHAPåˆ†æ**: ç‰¹å¾é‡è¦æ€§ã€å¯è§†åŒ–å›¾è¡¨ã€è§£é‡Šæ€§æ•°æ®


```

## ğŸ”§ æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

#### 1. é…ç½®éªŒè¯å¤±è´¥

```
Configuration validation failed:
  - Missing required field: experiment.model_name
  - Type mismatch at data.batch_size: expected int, got str
```

**è§£å†³æ–¹æ¡ˆ**: 
- æ£€æŸ¥é…ç½®æ–‡ä»¶æ ¼å¼ï¼Œç¡®ä¿å¿…éœ€å­—æ®µå­˜åœ¨ä¸”ç±»å‹æ­£ç¡®
- éªŒè¯ `config_mapping.yaml` ä¸­çš„æ˜ å°„å…³ç³»æ˜¯å¦æ­£ç¡®
- ä½¿ç”¨ `--dry-run` å‚æ•°ä»…éªŒè¯é…ç½®ä¸æ‰§è¡Œå®éªŒ

#### 2. ä¾èµ–åŒ…ç¼ºå¤±

```
Missing required packages: wandb, pytorch_lightning
```

**è§£å†³æ–¹æ¡ˆ**: 
```bash
pip install wandb pytorch_lightning GPUtil tensorboard
```

#### 3. W&Bè¿æ¥å¤±è´¥

```
W&B integration error: Failed to initialize wandb run
```

**è§£å†³æ–¹æ¡ˆ**: 
```bash
wandb login
# æˆ–è®¾ç½®ç¯å¢ƒå˜é‡
export WANDB_API_KEY="your-api-key"
```

#### 4. è®­ç»ƒè„šæœ¬æœªæ‰¾åˆ°

```
ModelNotFoundError: No training script found for model: unknown_model
```

**è§£å†³æ–¹æ¡ˆ**: 
- ç¡®ä¿æ¨¡å‹åç§°åœ¨`experiment_executor.py`çš„`model_script_mapping`ä¸­å­˜åœ¨
- æ£€æŸ¥è®­ç»ƒè„šæœ¬è·¯å¾„æ˜¯å¦æ­£ç¡®ä¸”æ–‡ä»¶å­˜åœ¨

#### 5. SHAPåˆ†æå¤±è´¥

```
SHAPåˆ†æç›®å½•ä¸å­˜åœ¨: /path/to/output/shap_results
```

**è§£å†³æ–¹æ¡ˆ**: 
- ç¡®ä¿æ¨¡å‹é…ç½®ä¸­å¯ç”¨äº†SHAPåˆ†æ
- æ£€æŸ¥è¾“å‡ºæƒé™å’Œç£ç›˜ç©ºé—´
- æŸ¥çœ‹è®­ç»ƒæ—¥å¿—ä¸­çš„SHAPç›¸å…³é”™è¯¯ä¿¡æ¯

#### 6. ç»“æœè§£æå¤±è´¥

```
è§£æå®éªŒç»“æœå¤±è´¥: æ— æ³•è§£ætraining.log
```

**è§£å†³æ–¹æ¡ˆ**: 
- æ£€æŸ¥è®­ç»ƒæ—¥å¿—æ–‡ä»¶æ˜¯å¦å­˜åœ¨ä¸”å¯è¯»
- éªŒè¯æ—¥å¿—æ ¼å¼æ˜¯å¦ç¬¦åˆæ¡†æ¶æœŸæœ›çš„æ ¼å¼
- ä½¿ç”¨ `ResultGenerator` ç±»çš„éªŒè¯åŠŸèƒ½

#### 7. å¹¶è¡Œæ‰§è¡Œå†²çª

```
å¹¶è¡Œæ‰§è¡Œæ—¶èµ„æºå†²çª: GPUå†…å­˜ä¸è¶³
```

**è§£å†³æ–¹æ¡ˆ**: 
- å‡å°‘å¹¶è¡Œä½œä¸šæ•°é‡
- æ£€æŸ¥ç³»ç»Ÿèµ„æºï¼ˆGPUå†…å­˜ã€ç£ç›˜ç©ºé—´ï¼‰
- ä½¿ç”¨ä¸²è¡Œæ¨¡å¼ `--parallel 1`

### è°ƒè¯•æ¨¡å¼

å¯ç”¨è¯¦ç»†æ—¥å¿—è¾“å‡ºï¼š

```bash
# ä¼ ç»Ÿæ–¹å¼
python src/hyper_optimus/experiment/run_ablation_exp.py --config exp_config.yaml --debug --log-file debug.log

# CLIæ–¹å¼
python -m src.hyper_optimus.experiment run --config exp_config.yaml --log-level DEBUG
```

### é…ç½®éªŒè¯

ä»…éªŒè¯é…ç½®ä¸æ‰§è¡Œå®éªŒï¼š

```bash
# éªŒè¯é…ç½®æ–‡ä»¶
python src/hyper_optimus/experiment/run_ablation_exp.py --config exp_config.yaml --dry-run

# éªŒè¯å®éªŒç›®å½•
python -m src.hyper_optimus.experiment generate --results-dir suite_XXX --validate-only

# éªŒè¯ç»“æœæ–‡ä»¶æ ¼å¼
python -m src.hyper_optimus.experiment upload --results-file final_results.json --dry-run
```

### æ—¥å¿—åˆ†æ

```python
from src.hyper_optimus.experiment import ExperimentExecutor

executor = ExperimentExecutor(workspace_root='/path/to/project')
parsed_data = executor._parse_log_files('/path/to/experiment/output')

print(f"è§£æåˆ° {len(parsed_data['epoch_metrics'])} ä¸ªepochæŒ‡æ ‡")
print(f"æµ‹è¯•ç»“æœ: {parsed_data['test_results']}")
print(f"è®­ç»ƒæ—¶é•¿: {parsed_data['duration']}ç§’")
```

## ğŸ“š æ‰©å±•å¼€å‘

### æ·»åŠ æ–°æ¨¡å‹æ”¯æŒ

1. åœ¨`experiment_executor.py`ä¸­æ·»åŠ æ¨¡å‹æ˜ å°„ï¼š

```python
self.model_script_mapping = {
    'your_new_model': 'path/to/your/model/train.py',
    # ... å…¶ä»–æ¨¡å‹
}
```

2. åœ¨`config_mapping.yaml`ä¸­æ·»åŠ é…ç½®æ˜ å°„ï¼š

```yaml
data:
  data_path:
    your_new_model: "data.custom_data_path"
  batch_size:
    your_new_model: "data.batch_size"

training:
  max_epochs:
    your_new_model: "training.max_epochs"
  learning_rate:
    your_new_model: "optimizer.lr"

# å…¶ä»–é…ç½®æ®µ...
```

3. åœ¨`config_converter.py`ä¸­æ›´æ–°æ˜ å°„é€»è¾‘ï¼ˆå¦‚æœéœ€è¦ç‰¹æ®Šå¤„ç†ï¼‰

### è‡ªå®šä¹‰æŒ‡æ ‡é‡‡é›†

æ‰©å±•æ—¥å¿—è§£ææ¨¡å¼ï¼š

```python
# åœ¨ ExperimentExecutor._parse_epoch_patterns() ä¸­æ·»åŠ æ–°æ¨¡å¼
def _parse_epoch_patterns(self, content: str, epoch_dict: dict):
    epoch_patterns = {
        # ç°æœ‰æ¨¡å¼...
        'custom_format': r'Epoch\s+(\d+).*custom_metric[=\s]\s*([\d.]+)',
    }
    # è§£æé€»è¾‘...
```

### è‡ªå®šä¹‰W&Bé›†æˆ

ç»§æ‰¿W&Bé›†æˆç±»ï¼š

```python
from src.hyper_optimus.experiment import WandBIntegration

class CustomWandBIntegration(WandBIntegration):
    def init_experiment_run(self, experiment_name, experiment_config, variant_id, ablation_variant):
        # è‡ªå®šä¹‰åˆå§‹åŒ–é€»è¾‘
        run = super().init_experiment_run(experiment_name, experiment_config, variant_id, ablation_variant)
        
        # æ·»åŠ è‡ªå®šä¹‰æ ‡ç­¾æˆ–é…ç½®
        run.tags.extend(['custom_tag'])
        
        return run
```

### è‡ªå®šä¹‰é…ç½®è½¬æ¢

ç»§æ‰¿é…ç½®è½¬æ¢å™¨ï¼š

```python
from src.hyper_optimus.experiment import AblationConfigConverter

class CustomConfigConverter(AblationConfigConverter):
    def convert_custom_ablation(self, ablation_config, model_name):
        """è‡ªå®šä¹‰æ¶ˆèç±»å‹è½¬æ¢"""
        override_config = {}
        # è‡ªå®šä¹‰è½¬æ¢é€»è¾‘
        return override_config
    
    def convert_ablation_config(self, ablation_variant, model_name):
        override_config = super().convert_ablation_config(ablation_variant, model_name)
        
        if ablation_variant.get('type') == 'custom_ablation':
            custom_config = self.convert_custom_ablation(ablation_variant.get('config', {}), model_name)
            override_config.update(custom_config)
        
        return override_config
```

### æ·»åŠ æ–°çš„æ¶ˆèç±»å‹

1. åœ¨`config_mapping.yaml`ä¸­æ·»åŠ æ–°çš„é…ç½®æ®µ

2. åœ¨`variant_identifier.py`ä¸­æ·»åŠ æ–°çš„ç±»å‹æ ‡è¯†ï¼š

```python
class VariantIdentifier:
    def __init__(self):
        self.type_prefixes = {
            # ç°æœ‰ç±»å‹...
            'custom_ablation': 'CU'
        }
```

3. æ›´æ–°é…ç½®è½¬æ¢å™¨æ”¯æŒæ–°ç±»å‹

## ğŸ¯ æœ€ä½³å®è·µ

### 1. å®éªŒè®¾è®¡

```yaml
# æ¨èçš„å®éªŒé…ç½®ç»“æ„
experiment:
  model_name: flow_bert_multiview
  ablation_strategy: ablation  # æˆ– 'standard'
  enable:
    feature_ablation: true
    fusion_ablation: true
    loss_ablation: false

# æ˜ç¡®å®šä¹‰åŸºçº¿å®éªŒ
ablation_variants:
  BASE:
    name: "åŸºçº¿æ¨¡å‹"
    type: "feature_ablation"
    baseline: true
    config:
      enabled_features:
        sequence_features: true
        text_features: true
```

### 2. èµ„æºç®¡ç†

```python
# åˆç†è®¾ç½®å¹¶è¡Œæ•°é‡
executor = BatchExperimentExecutor(
    workspace_root='/path/to/project',
    max_parallel_jobs=min(4, psutil.cpu_count() // 2)  # æ ¹æ®CPUèµ„æºåŠ¨æ€è°ƒæ•´
)
```


