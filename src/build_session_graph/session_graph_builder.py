# -*- coding: utf-8 -*-
import os
import pandas as pd
import dgl
import torch
import numpy as np
from dgl.data.utils import save_graphs, save_info
import tqdm
import ast
import re
from typing import List
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed
from session_parser import SessionParser
import ipaddress
import logging
import sys
import threading
import copy
import pickle
import math
import gc

# æ·»åŠ ../utilsç›®å½•åˆ°Pythonæœç´¢è·¯å¾„
utils_path = os.path.join(os.path.dirname(__file__), '..', 'utils')
sys.path.insert(0, utils_path)
# å¯¼å…¥é…ç½®ç®¡ç†æ¨¡å—
from logging_config import setup_preset_logging
# ä½¿ç”¨ç»Ÿä¸€çš„æ—¥å¿—é…ç½®
logger = setup_preset_logging(log_level=logging.DEBUG)    

verbose = False
class SessionGraphBuilder():
    def __init__(self, flow_node_builder, session_csv, dumpFilename, 
                concurrent_flow_iat_threshold=1.0, sequential_flow_iat_threshold=10.0,
                session_label_id_map=None, thread_count=1):
        self.flow_node_builder = flow_node_builder
        self.session_parser = SessionParser(flow_node_builder=flow_node_builder, 
                                            session_label_id_map=session_label_id_map)        
        self.concurrent_flow_iat_threshold = concurrent_flow_iat_threshold
        self.sequential_flow_iat_threshold = sequential_flow_iat_threshold
        
        if verbose:
            logger.debug(" Flowæ•°æ®ä¸­çš„å‰5ä¸ªUID:")
            logger.info(str(list(flow_node_builder.flow_dict.keys())[:5]))  # æ£€æŸ¥å®é™…å­˜å‚¨çš„UIDæ ¼å¼

        # é…ç½®å‚æ•°
        # self.gc_interval = 1000  # ä¿å­˜åƒåœ¾å›æ”¶é—´éš”å‚æ•°
        self.thread_count = thread_count
        self._graph_list_lock = threading.Lock()  # ä¿æŠ¤å…±äº«å˜é‡çš„çº¿ç¨‹é”
        self.dumpFilename = dumpFilename

        # æ•°æ®å­˜å‚¨
        self.graph_list = []
        self.graph_node_uids_list = []  # æ–°å¢ï¼šå­˜å‚¨æ¯ä¸ªå›¾çš„èŠ‚ç‚¹uidåˆ—è¡¨ï¼ˆä¸graphsä¸€ä¸€å¯¹åº”ï¼‰
        self.label_name_list = []
        self.label_id_list = []
        self.is_malicious_list = []
        self.stats = {
            "total_flows": 0,
            "total_bursts": 0,
            "total_edges": 0,
            "total_graphs": 0,
            "processed_session_count": 0  # å·²å¤„ç†çš„sessionè®¡æ•°
        }

        """æ‰§è¡Œå®Œæ•´çš„å›¾æ„å»ºæµç¨‹"""
        # 1. åŠ è½½ä¼šè¯æ•°æ®
        logger.info("Loading session CSV...")
        self.session_df = pd.read_csv(session_csv, low_memory=False)

        # æ·»åŠ è°ƒè¯•
        logger.debug(f" Session DataFrameå½¢çŠ¶: {self.session_df.shape}")
        logger.debug(f" Sessionåˆ—å: {self.session_df.columns.tolist()}")
        
        if verbose and len(self.session_df) > 0:
            sample_session = self.session_df.iloc[0]
            logger.debug(f" ç¤ºä¾‹Sessionæ•°æ®:")
            logger.debug(f"  session_index: {sample_session.get('session_index')}")
            logger.debug(f"  flow_uid_list: {sample_session.get('flow_uid_list')}")
            logger.debug(f"  split: {sample_session.get('split')}")

        self.stats.update({
            "mixed_session_count": 0,
        })
        self._stats_lock = threading.Lock()
        self.mixed_sessions = []   # debug / verbose æ¨¡å¼ä¸‹æ‰ç”¨

        # 2. å¤„ç†æ‰€æœ‰ä¼šè¯
        self.process_sessions_sequentially(self.session_df)   # é¡ºåºå¤„ç†
        # self.process_sessions_parallel(self.session_df)
        
        # 3. # åˆ’åˆ†æ•°æ®é›†ï¼ˆåŸºäºall_split_session.csvä¸­çš„splitåˆ—ï¼‰
        self.split_datasets()

        # 4. ä¿å­˜ç»“æœ
        self.save_results()

        logger.info(
            f"[SUMMARY] Mixed sessions skipped: "
            f"{self.stats['mixed_session_count']} / {len(self.session_df)} "
            f"({self.stats['mixed_session_count'] / len(self.session_df) * 100:.2f}%)"
        )
        logger.debug(f"First 100 mixed sessions: {self.mixed_sessions[:100]}")


    def process_sessions_sequentially(self, session_df: pd.DataFrame):
        """å•çº¿ç¨‹å¤„ç†æ‰€æœ‰ä¼šè¯"""
        logger.info(f"Building graphs from sessions (single-threaded)...")
        
        if not hasattr(self, 'graph_split_list_by_session'):
            self.graph_split_list_by_session = []

        # å•çº¿ç¨‹é¡ºåºå¤„ç†æ‰€æœ‰session
        for _, session_row in tqdm.tqdm(session_df.iterrows(), total=len(session_df), desc="Processing sessions"):
            self.process_single_session(session_row)
        
        # åç»­çš„ç»Ÿè®¡å’Œæ•°æ®åˆ’åˆ†é€»è¾‘ä¿æŒä¸å˜
        if not self.graph_list:
            logger.info("No graphs were built!")
            return
        
        logger.info(f"[SUMMARY] Total sessions: {len(self.session_df)}, \
                total graphs: {self.stats['total_graphs']}, \
                avg flows per graph: {self.stats['total_flows']/self.stats['total_graphs']:.2f}, \
                avg bursts per graph: {self.stats['total_bursts']/self.stats['total_graphs']:.2f}, \
                avg edges per graph: {self.stats['total_edges']/len(self.graph_list):.2f}")

    def process_sessions_parallel(self, session_df: pd.DataFrame):
        """
        å¤šçº¿ç¨‹å¤„ç†æ‰€æœ‰ä¼šè¯ï¼ˆä¿®æ­£ç‰ˆï¼šä¸¥æ ¼ä¿è¯æ—¶åºé¡ºåºï¼‰
        """
        logger.info(f"Building graphs from sessions (chunked parallel, thread_count={self.thread_count})...")

        if not hasattr(self, 'graph_split_list_by_session'):
            self.graph_split_list_by_session = []

        session_count = len(session_df)
        chunk_size = min(100, session_count // (self.thread_count * 4))
        chunk_size = max(1, chunk_size)  # é˜²æ­¢ä¸º0

        session_idx_ranges = [
            (i, min(i + chunk_size, session_count))
            for i in range(0, session_count, chunk_size)
        ]

        # 1. æäº¤ä»»åŠ¡ï¼Œå¹¶ä¿å­˜ futures åˆ—è¡¨ï¼ˆæŒ‰æäº¤é¡ºåºï¼Œå³æ—¶é—´é¡ºåºï¼‰
        futures = []
        with ThreadPoolExecutor(max_workers=self.thread_count) as executor:
            for start_idx, end_idx in session_idx_ranges:
                # æäº¤çº¯å‡½æ•°ä»»åŠ¡ï¼Œè¿”å›æ•°æ®è€Œéä¿®æ”¹self
                futures.append(executor.submit(
                    self._process_session_range_pure,
                    session_df,
                    start_idx,
                    end_idx
                ))

            # 2. æŒ‰é¡ºåºæ”¶é›†ç»“æœ (Crucial for Temporal Order)
            # å³ä½¿åé¢çš„ä»»åŠ¡å…ˆå®Œæˆï¼Œè¿™é‡Œä¹Ÿä¼šç­‰å¾…å‰é¢çš„ä»»åŠ¡result()è¿”å›ï¼Œä»è€Œä¿è¯åˆå¹¶é¡ºåº
            for future in tqdm.tqdm(futures, desc="Processing chunks (Ordered Merge)"):
                try:
                    # è·å–è¯¥ chunk çš„æ‰€æœ‰ç»“æœ
                    chunk_results = future.result()

                    if not chunk_results:
                        continue

                    # 3. å°†ç»“æœæŒ‰é¡ºåºåˆå¹¶åˆ°ä¸»åˆ—è¡¨
                    # chunk_results æ˜¯ä¸€ä¸ªåˆ—è¡¨ï¼ŒåŒ…å«: (graph, node_uids, label_name, label_id, is_mal, split)
                    for res in chunk_results:
                        (g, uids, lname, lid, mal, split) = res

                        self.graph_list.append(g)
                        self.graph_node_uids_list.append(uids)
                        self.label_name_list.append(lname)
                        self.label_id_list.append(lid)
                        self.is_malicious_list.append(mal)
                        self.graph_split_list_by_session.append(split)

                        # æ›´æ–°ç»Ÿè®¡ (ä½¿ç”¨é”æˆ–è€…æ˜¯ç´¯åŠ ï¼Œè¿™é‡Œå•çº¿ç¨‹åˆå¹¶æ˜¯å®‰å…¨çš„ï¼Œä¸éœ€è¦é”)
                        # ä½†ä¸ºäº†ä¿æŒå…¼å®¹æ€§ï¼Œç›´æ¥æ›´æ–°statså­—å…¸
                        # æ³¨æ„ï¼šstatsåœ¨å¹¶è¡Œä¸­å¾ˆéš¾ç²¾ç¡®è¿½è¸ª burst/edges ç»†èŠ‚é™¤éä» res è¿”å›ï¼Œ
                        # è¿™é‡Œä¸ºäº†ç®€åŒ–ï¼Œä»…åœ¨æœ€ååšç®€å•ç»Ÿè®¡ï¼Œæˆ–è€…ä½ å¯ä»¥è®© pure å‡½æ•°ä¹Ÿè¿”å› statsã€‚
                        self.stats["total_graphs"] += 1
                        self.stats["processed_session_count"] += 1

                except Exception as e:
                    logger.exception("Future crashed or merge failed")

        # æœ€ç»ˆç»Ÿè®¡æ—¥å¿—...
        if not self.graph_list:
            logger.info("No graphs were built!")
            return

        logger.info(f"[SUMMARY] Ordered parallel processing finished. Total graphs: {len(self.graph_list)}")

    def _process_session_range_pure(self, session_df, start_idx, end_idx):
        """
        [æ–°å¢è¾…åŠ©æ–¹æ³•]
        å¤„ç†ä¸€ä¸ªèŒƒå›´çš„ sessionï¼Œè¿”å›ç»“æœåˆ—è¡¨ï¼Œä¸äº§ç”Ÿå‰¯ä½œç”¨ï¼ˆä¸ä¿®æ”¹ self.graph_listï¼‰ã€‚
        """
        local_results = []  # å­˜å‚¨ (graph, uids, label_name, label_id, is_mal, split)

        for idx in range(start_idx, end_idx):
            session_row = session_df.iloc[idx]
            try:
                # 1. å¹¿æ’­åœ°å€è¿‡æ»¤
                if self.contains_broadcast_ip_address(session_row.get('session_index')):
                    continue

                session_split = str(session_row.get('split', 'train')).strip().lower()

                # 2. æ„å»ºå›¾ (å¤ç”¨ç°æœ‰é€»è¾‘)
                g_list, node_uids_list, stats = self.build_graphs_from_session(session_row)

                if len(g_list) != len(node_uids_list):
                    continue

                # 3. éå†ç”Ÿæˆçš„å›¾ (ä¸€ä¸ªsessionå¯èƒ½ç”Ÿæˆå¤šä¸ªå›¾)
                for i in range(len(g_list)):
                    current_graph = g_list[i]
                    current_node_uids = node_uids_list[i]

                    # èšåˆæ ‡ç­¾
                    label_name, label_id, is_malicious = self.session_parser.aggregate_session_label(current_node_uids)

                    if label_name == "mixed":
                        continue  # è·³è¿‡ mixed

                    # 4. å­˜å…¥å±€éƒ¨ç»“æœåˆ—è¡¨
                    local_results.append((
                        current_graph,
                        current_node_uids,
                        label_name,
                        label_id,
                        is_malicious,
                        session_split
                    ))

            except Exception as e:
                # ä»…è®°å½•é”™è¯¯ï¼Œä¸ä¸­æ–­
                pass

        return local_results


    @staticmethod
    def _extract_ip_candidates(session_index) -> List[str]:
        """
        ç²—ç²’åº¦æå– IP å€™é€‰ï¼š
        - è‹¥æ˜¯ tuple / listï¼šè¿”å›å…¶ä¸­æ‰€æœ‰å­—æ®µçš„å­—ç¬¦ä¸²å½¢å¼
        - è‹¥æ˜¯å­—ç¬¦ä¸²å½¢å¼çš„ tuple / listï¼šliteral_eval ååŒæ ·å¤„ç†
        - è‹¥æ˜¯å•å­—ç¬¦ä¸²ï¼šç›´æ¥ä½œä¸ºå”¯ä¸€å€™é€‰
        åç»­ç”± _is_single_ip_broadcast ä¸¥æ ¼åˆ¤å®š
        """
        if session_index is None:
            return []

        # å·²ç»æ˜¯ tuple / list
        if isinstance(session_index, (tuple, list)):
            return [str(x).strip() for x in session_index if x is not None]

        # å­—ç¬¦ä¸²
        if isinstance(session_index, str):
            s = session_index.strip()
            if not s:
                return []

            # å°è¯•è§£æå­—ç¬¦ä¸²å½¢å¼çš„ tuple / list
            if (s.startswith("(") and s.endswith(")")) or (s.startswith("[") and s.endswith("]")):
                try:
                    obj = ast.literal_eval(s)
                    if isinstance(obj, (tuple, list)):
                        return [str(x).strip() for x in obj if x is not None]
                except Exception:
                    pass

            # å¦åˆ™å½“ä½œå•å­—æ®µ
            return [s]

        return []

    @staticmethod
    def _is_single_ip_broadcast(ip_str: str) -> bool:
        ip_str = ip_str.strip()

        # ---------- IPv4 ----------
        try:
            ipv4 = ipaddress.IPv4Address(ip_str)

            # å—é™å¹¿æ’­
            if ip_str == "255.255.255.255":
                return True

        except ipaddress.AddressValueError:
            pass

        # ---------- IPv6 ----------
        try:
            ipv6 = ipaddress.IPv6Address(ip_str)
            return ipv6.is_multicast or ipv6.is_unspecified
        except ipaddress.AddressValueError:
            pass

        return False

    @staticmethod
    def contains_broadcast_ip_address(session_index) -> bool:
        """
        åˆ¤æ–­ session_index ä¸­æ˜¯å¦åŒ…å«å¹¿æ’­ / ç»„æ’­ / æœªæŒ‡å®š IPã€‚
        å…¼å®¹ï¼š
        - å• IP
        - ('ip1','ip2')
        - å­—ç¬¦ä¸²å½¢å¼ tuple/list
        """
        ip_list = SessionGraphBuilder._extract_ip_candidates(session_index)

        for ip_str in ip_list:
            if SessionGraphBuilder._is_single_ip_broadcast(ip_str):
                return True

        return False

    def process_session_range(self, session_df: pd.DataFrame, start_idx: int, end_idx: int):
        """å¤„ç† session_df[start:end] è¿™ä¸€æ®µ"""
        for idx in range(start_idx, end_idx):
            session_row = session_df.iloc[idx]
            try:
                self.process_single_session(session_row)
            except Exception:
                logger.exception(f"Error processing session index={idx}")


    def process_single_session(self, session_row):
        """å•çº¿ç¨‹å¤„ç†å•ä¸ªsession_rowçš„å‡½æ•°"""
        try:
            session_index = session_row.get('session_index')  # æ ¹æ®å®é™…åˆ—åè°ƒæ•´
            # æ£€æµ‹session_indexæ˜¯å¦ä¸ºå¹¿æ’­åœ°å€ï¼Œæ˜¯åˆ™ç›´æ¥è·³è¿‡åç»­å¤„ç†
            if self.contains_broadcast_ip_address(session_row['session_index']):
                if verbose:
                    logger.debug(f" Session {session_row['session_index']} è¢«å¹¿æ’­åœ°å€è¿‡æ»¤")
                with self._stats_lock:
                    self.stats.setdefault("broadcast_session_count", 0)
                    self.stats["broadcast_session_count"] += 1                    
                return None

            # è·å–å½“å‰sessionçš„splitä¿¡æ¯
            session_split = str(session_row.get('split', 'unknown')).strip().lower()   # [ä¿®æ”¹] ä¸è¦é»˜è®¤ 'train'ï¼Œå¦‚æœç¼ºå¤±åˆ™è®¾ä¸º 'unknown' ä»¥ä¾¿åç»­è¿‡æ»¤
            
            # å¤„ç†å½“å‰session_row
            g_list, node_uids_list, stats = self.build_graphs_from_session(session_row)
            
            # éªŒè¯g_listä¸node_uids_listé•¿åº¦å¿…é¡»ä¸€è‡´ï¼Œä¸ä¸€è‡´åˆ™æŠ›å‡ºæ˜ç¡®å¼‚å¸¸
            if len(g_list) != len(node_uids_list):
                raise ValueError(
                    f"g_listä¸node_uids_listé•¿åº¦ä¸åŒ¹é…ï¼"
                    f"g_listé•¿åº¦: {len(g_list)}, node_uids_listé•¿åº¦: {len(node_uids_list)}"
                )

            # æŒ‰ç´¢å¼•éå†ï¼ˆè¦†ç›–0åˆ°len(g_list)-1çš„æ‰€æœ‰æœ‰æ•ˆç´¢å¼•ï¼Œç”¨æˆ·"0æˆ–1"è¡¨è¿°åº”ä¸ºç¬”è¯¯ï¼Œå®é™…éœ€å…¨é‡éå†ï¼‰
            for idx in range(len(g_list)):
                # 1 é€ä¸ªæå–å½“å‰ç´¢å¼•å¯¹åº”çš„å›¾å’ŒèŠ‚ç‚¹UIDåˆ—è¡¨
                current_graph = g_list[idx]
                current_node_uids = node_uids_list[idx]

                # 2 ä¸ºå½“å‰å›¾åŒ¹é…å¯¹åº”çš„æ ‡ç­¾å’Œsplitä¿¡æ¯ï¼ˆç¡®ä¿ä¸å½“å‰graph/node_uidsä¸¥æ ¼å¯¹é½ï¼‰
                label_name, label_id, is_malicious = self.session_parser.aggregate_session_label(current_node_uids)

                if label_name == "mixed":
                    with self._stats_lock:
                        self.stats["mixed_session_count"] += 1

                        # ä»…åœ¨ verbose æˆ–å‰ N ä¸ªæ‰“å°ï¼Œé¿å…åˆ·å±
                        if verbose and self.stats["mixed_session_count"] <= 10:
                            logger.warning(
                                f"[MIXED_SESSION] session_index={session_row.get('session_index')}, "
                                f"node_uids={current_node_uids}"
                            )

                        if len(self.mixed_sessions) < 10: # åªä¿ç•™å‰ 10 / 100 / 1000 ä¸ªmixed sessionã€‚
                            self.mixed_sessions.append(session_index)

                    continue  # skip this â€œmixedâ€ graph, not whole session

                # 3. appendåˆ°å…¨å±€åˆ—è¡¨ï¼ˆç­‰æ•ˆåŸextendï¼Œä¸”æ›´ä¸¥è°¨ï¼‰
                with self._graph_list_lock:
                    self.graph_list.append(current_graph)  # æ›¿ä»£åŸextendï¼ŒæŒ‰ç´¢å¼•é€ä¸ªæ·»åŠ 
                    self.graph_node_uids_list.append(current_node_uids)  # ä¸graph_listä¸€ä¸€å¯¹åº”
                    self.label_name_list.append(label_name)
                    self.label_id_list.append(label_id)
                    self.is_malicious_list.append(is_malicious)
                    self.graph_split_list_by_session.append(session_split)  # æ¯ä¸ªå›¾ç»‘å®šå¯¹åº”çš„split

            # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
            with self._stats_lock:
                self.stats["total_flows"] += stats["num_flows"]
                self.stats["total_bursts"] += stats["num_bursts"]
                self.stats["total_edges"] += stats["num_edges_avg"] * len(g_list)
                self.stats["total_graphs"] += stats["num_graphs"]
                self.stats["processed_session_count"] += 1

            # è°ƒè¯•æ‰“å°ï¼šåªæ‰“å°å‰5æ¡ï¼ˆåŸé€»è¾‘ä¿ç•™ï¼Œä¼˜åŒ–Graph idè®¡ç®—ï¼Œç¡®ä¿å”¯ä¸€ï¼‰
            if verbose:
                if self.stats["processed_session_count"] < 5:  # åªæ‰“å°å‰5ä¸ªä¼šè¯
                    logger.debug(f" Processing session {session_index}, split: {session_split}")
                    # Graph id = åˆå§‹é•¿åº¦ + å½“å‰ç´¢å¼•ï¼ˆé¿å…åŸlen(self.graph_list)å¯èƒ½çš„è®¡æ•°åå·®ï¼‰
                    logger.info(
                        f"[DEBUG] Graph id: {len(self.graph_list) - 1}, "
                        f"label_name: {label_name}, label_id: {label_id}, "
                        f"session_index: {session_index}, split: {session_split}, "
                        f"å½“å‰sessionå¤„ç†è¿›åº¦: {idx+1}/{len(g_list)}, "
                        f"processed_session_count: {self.stats['processed_session_count']}"
                    )


            # åƒåœ¾å›æ”¶ï¼ˆçº¿ç¨‹å®‰å…¨ï¼Œç”±ä¸»çº¿ç¨‹è®¡æ•°è§¦å‘ï¼‰
            # if self.stats["processed_session_count"] % self.gc_interval == 0:
            #     gc.collect()

        except Exception as e:
            logger.exception(f"Error processing session_index={session_row.get('session_index')}")
            return None

    # ========================================================================
    # ç»Ÿä¸€ä¸‰ç±»è§†å›¾ï¼šFlow / SSL / X509 / DNS
    # æ‰€æœ‰ç‰¹å¾ä» flow_node_builder.global_dims è·å–å›ºå®šç»´åº¦
    # ========================================================================

    def build_flow_relation_graph(self, nodes, edges):
        """
        æ„å»º DGL Graphï¼ˆæœ€æ–°ç‰ˆï¼‰
        ä½¿ç”¨ FlowNodeBuilder.global_dims ç¡®å®šæ‰€æœ‰ç‰¹å¾ç»´åº¦
        """

        # æ•°æ®è´¨é‡éªŒè¯å’Œä¿®å¤
        is_clean, fixed_count = self._validate_and_fix_node_features(nodes)
        
        if not is_clean:
            logger.warning(f"âš ï¸ æ£€æµ‹å¹¶ä¿®å¤äº† {fixed_count} ä¸ªNaN/Infå€¼")

        global_node_feature_dims = self.flow_node_builder.global_node_feature_dims
        enabled_views = self.flow_node_builder.enabled_views

        # -------- å›ºå®šç»´åº¦æŸ¥è¯¢ --------
        def dim(name):
            if "textual" in name:
                raise RuntimeError(f"Textual feature `{name}` should not query dim()")
            return int(global_node_feature_dims.get(name, 1))

        num_nodes = len(nodes)
        graph = dgl.graph([])
        graph.add_nodes(num_nodes)

        # ===================================================================
        # é€šç”¨çŸ©é˜µæ„å»ºå‡½æ•°ï¼ˆæ ¹æ®å…¨å±€ç»´åº¦ padding/truncateï¼‰
        # ===================================================================
        def make_matrix(key, L, dtype):
            mat = np.zeros((num_nodes, L), dtype=dtype)
            for i, node in enumerate(nodes):
                vec = node.get(key, [])
                if not vec:
                    continue
                arr = np.asarray(vec, dtype=dtype)
                L0 = min(L, len(arr))
                mat[i, :L0] = arr[:L0]
            return mat

        # ===================================================================
        # å°†æ‰€æœ‰è§†å›¾æ”¾å…¥ graph.ndata
        # ===================================================================

        def build_textual_ndata(nodes, key):
            for n in nodes:
                if key not in n:
                    raise KeyError(f"Node missing textual feature `{key}`")            
            
            sample = nodes[0][key]
            assert isinstance(sample, dict), f"{key} must be dict"
            assert "input_ids" in sample and "attention_mask" in sample, \
                f"{key} must contain input_ids and attention_mask"

            return {
                "input_ids": torch.cat(
                    [n[key]["input_ids"] for n in nodes], dim=0
                ),
                "attention_mask": torch.cat(
                    [n[key]["attention_mask"] for n in nodes], dim=0
                ),
            }
        # -------- Flow --------
        if enabled_views.get("flow_numeric_features", False):
            flow_num_mat = make_matrix("flow_numeric_features", dim("flow_numeric_features"), dtype=np.float32)
            graph.ndata["flow_numeric_features"] = torch.from_numpy(flow_num_mat).float()

        if enabled_views.get("flow_categorical_features", False):
            flow_cat_mat = make_matrix("flow_categorical_features", dim("flow_categorical_features"), dtype=np.int64)
            graph.ndata["flow_categorical_features"] = torch.from_numpy(flow_cat_mat).long()

        if enabled_views.get("flow_textual_features", False):
            t = build_textual_ndata(nodes, "flow_textual_features")
            graph.ndata["flow_textual_input_ids"] = t["input_ids"]
            graph.ndata["flow_textual_attention_mask"] = t["attention_mask"]

        # -------- SSL --------
        if enabled_views.get("ssl_numeric_features", False):
            ssl_num_mat = make_matrix("ssl_numeric_features", dim("ssl_numeric_features"), dtype=np.float32)            
            graph.ndata["ssl_numeric_features"] = torch.from_numpy(ssl_num_mat).float()

        if enabled_views.get("ssl_categorical_features", False):
            ssl_cat_mat = make_matrix("ssl_categorical_features", dim("ssl_categorical_features"), dtype=np.int64)
            graph.ndata["ssl_categorical_features"] = torch.from_numpy(ssl_cat_mat).long()

        if enabled_views.get("ssl_textual_features", False):
            t = build_textual_ndata(nodes, "ssl_textual_features")
            graph.ndata["ssl_textual_input_ids"] = t["input_ids"]          # [N, L]
            graph.ndata["ssl_textual_attention_mask"] = t["attention_mask"] # [N, L]            

        # -------- X509 --------
        if enabled_views.get("x509_numeric_features", False):
            x509_num_mat = make_matrix("x509_numeric_features", dim("x509_numeric_features"), dtype=np.float32)
            graph.ndata["x509_numeric_features"] = torch.from_numpy(x509_num_mat).float()

        if enabled_views.get("x509_categorical_features", False):
            x509_cat_mat = make_matrix("x509_categorical_features", dim("x509_categorical_features"), dtype=np.int64)            
            graph.ndata["x509_categorical_features"] = torch.from_numpy(x509_cat_mat).long()

        if enabled_views.get("x509_textual_features", False):
            t = build_textual_ndata(nodes, "x509_textual_features")
            graph.ndata["x509_textual_input_ids"] = t["input_ids"]
            graph.ndata["x509_textual_attention_mask"] = t["attention_mask"]

        # -------- DNS --------
        if enabled_views.get("dns_numeric_features", False):
            dns_num_mat = make_matrix("dns_numeric_features", dim("dns_numeric_features"), dtype=np.float32)
            graph.ndata["dns_numeric_features"] = torch.from_numpy(dns_num_mat).float()

        if enabled_views.get("dns_categorical_features", False):
            dns_cat_mat = make_matrix("dns_categorical_features", dim("dns_categorical_features"), dtype=np.int64)
            graph.ndata["dns_categorical_features"] = torch.from_numpy(dns_cat_mat).long()

        if enabled_views.get("dns_textual_features", False):
            t = build_textual_ndata(nodes, "dns_textual_features")
            graph.ndata["dns_textual_input_ids"] = t["input_ids"]
            graph.ndata["dns_textual_attention_mask"] = t["attention_mask"]

        # -------- Packet Sequences --------
        if enabled_views.get("packet_len_seq", False):
            pkt_len_mat = make_matrix("packet_len_seq", dim("packet_len_seq"), dtype=np.float32)
            graph.ndata["packet_len_seq"] = torch.from_numpy(pkt_len_mat).float()

        if enabled_views.get("packet_iat_seq", False):
            pkt_iat_mat = make_matrix("packet_iat_seq", dim("packet_iat_seq"), dtype=np.float32)
            graph.ndata["packet_iat_seq"] = torch.from_numpy(pkt_iat_mat).float()

        if "packet_seq_mask" in nodes[0]:
            graph.ndata["packet_seq_mask"] = torch.tensor(
                [n["packet_seq_mask"] for n in nodes], dtype=torch.bool
            )

        # -------- é€‰é¡¹ï¼šåŸŸåæ¦‚ç‡å‘é‡ --------
        if enabled_views.get("domain_probs", False):
            dom_len = dim("domain_probs")
            dom_mat = make_matrix("domain_probs", dom_len, dtype=np.float32)
            graph.ndata["domain_probs"] = torch.from_numpy(dom_mat).float()

        # ===================================================================
        # è¾…åŠ©å­—æ®µï¼šts / id / burst_id
        # ===================================================================
        graph.ndata["burst_id"] = torch.tensor(
            [n.get("burst_id", 0) for n in nodes], dtype=torch.long
        )
        graph.ndata["ts"] = torch.tensor(
            [n.get("ts", 0.0) for n in nodes], dtype=torch.float32
        )
        graph.ndata["id"] = torch.tensor(
            [n.get("id", 0) for n in nodes], dtype=torch.long
        )

        # ===================================================================
        # æ·»åŠ è¾¹
        # ===================================================================
        if edges:
            if len(edges[0]) == 2:
                src, dst = zip(*edges)
                graph.add_edges(list(src), list(dst))

            elif len(edges[0]) == 3:
                src, dst, etype = zip(*edges)
                graph.add_edges(list(src), list(dst))
                graph.edata["etype"] = torch.tensor(etype, dtype=torch.long)

            else:
                raise ValueError("Edge tuple must be (src, dst) or (src, dst, etype)")

        # å¼ºä¸€è‡´æ€§æ£€æŸ¥ï¼ˆå»ºè®®ä¿ç•™ï¼‰
        for feat_name, feat in graph.ndata.items():
            if isinstance(feat, dict):
                # textual featureï¼šæ£€æŸ¥ input_ids / attention_mask
                for sub_key, sub_tensor in feat.items():
                    if sub_tensor.shape[0] != num_nodes:
                        raise ValueError(
                            f"[FATAL] ç‰¹å¾ {feat_name}.{sub_key} çš„èŠ‚ç‚¹æ•° "
                            f"{sub_tensor.shape[0]} ä¸å›¾èŠ‚ç‚¹æ•° {num_nodes} ä¸ä¸€è‡´ï¼"
                        )
            else:
                # æ™®é€š tensor ç‰¹å¾
                if feat.shape[0] != num_nodes:
                    raise ValueError(
                        f"[FATAL] ç‰¹å¾ {feat_name} çš„èŠ‚ç‚¹æ•° {feat.shape[0]} "
                        f"ä¸å›¾èŠ‚ç‚¹æ•° {num_nodes} ä¸ä¸€è‡´ï¼"
                    )        
        
        # session graph ä¸­å­˜åœ¨å…¥åº¦ä¸º 0 çš„èŠ‚ç‚¹
        # è¿™åœ¨ä½ çš„åœºæ™¯é‡Œæ˜¯å¿…ç„¶çš„ï¼Œæ¯”å¦‚ï¼š
        # session ä¸­çš„ç¬¬ä¸€ä¸ª flowï¼ˆæ²¡æœ‰è¢«ä»»ä½• flow æŒ‡å‘ï¼‰
        # trigger / concurrent è¾¹æ˜¯æœ‰å‘çš„ï¼šetype 0=concurrent, 1=trigger
        # å•èŠ‚ç‚¹ sessionï¼ˆåªæœ‰ 1 ä¸ª flowï¼‰
        # è€Œ GAT / GATv2 çš„æ¶ˆæ¯ä¼ é€’è§„åˆ™æ˜¯ï¼š
        # èŠ‚ç‚¹åªèƒ½ä»ã€Œå…¥è¾¹ã€èšåˆä¿¡æ¯
        # å…¥åº¦ä¸º 0 â‡’ æ²¡æœ‰ä»»ä½•å¯èšåˆçš„ä¿¡æ¯ â‡’ è¾“å‡ºæœªå®šä¹‰
        # æ‰€ä»¥å»ºå›¾é˜¶æ®µåŠ  self-loopï¼Œè¿™æ ·è¯­ä¹‰æ¸…æ™°ï¼šèŠ‚ç‚¹è‡³å°‘èƒ½çœ‹åˆ°è‡ªå·±ï¼ŒGAT / GIN / GraphSAGE å…¨éƒ½å®‰å…¨
        # graph = dgl.add_self_loop(graph)
        # self-loop è¯­ä¹‰ï¼šconcurrent (etype=0)
        graph = self.add_self_loop_with_etype(graph, self_loop_etype=0)

        assert graph.num_edges() == graph.edata["etype"].shape[0], \
            f"Edge count mismatch: {graph.num_edges()} vs {graph.edata['etype'].shape[0]}"
        
        return graph # è¿”å›å›¾å’Œå¯¹åº”çš„èŠ‚ç‚¹uidåˆ—è¡¨

    def _validate_and_fix_node_features(self, nodes):
        """éªŒè¯å¹¶ä¿®å¤èŠ‚ç‚¹ç‰¹å¾ä¸­çš„NaNå€¼"""
        issues_found = False
        fixed_count = 0

        def is_nan_or_inf(x):
            """æ›´å…¨é¢çš„NaN/Infæ£€æŸ¥"""
            if x is None:
                return True
            try:
                # å¤„ç†numpyç±»å‹
                if hasattr(x, 'dtype'):
                    return np.isnan(x) or np.isinf(x)
                # å¤„ç†Pythonæ•°å€¼ç±»å‹
                elif isinstance(x, (int, float, np.number)):
                    return np.isnan(x) or np.isinf(x)
                return False
            except (TypeError, ValueError):
                return False
        
        for i, node in enumerate(nodes):
            numeric_fields = ['flow_numeric_features', 'ssl_numeric_features', 
                            'x509_numeric_features', 'dns_numeric_features']
            
            for field in numeric_fields:
                if field in node and node[field]:
                    features = node[field]
                    new_features = []
                    nan_count = 0
                    
                    for value in features:
                        if is_nan_or_inf(value):
                            new_features.append(0.0)  # ä¿®å¤ä¸º0
                            nan_count += 1
                        else:
                            new_features.append(value)
                    
                    if nan_count > 0:
                        issues_found = True
                        fixed_count += nan_count
                        node[field] = new_features
                        
                        logger.debug(f"ä¿®å¤èŠ‚ç‚¹ {i} çš„ {field}: {nan_count}ä¸ªNaNå€¼")
        
        return not issues_found, fixed_count

    def add_self_loop_with_etype(self, graph, self_loop_etype=0):
        device = graph.device
        num_nodes = graph.num_nodes()

        # self-loop è¾¹
        src = torch.arange(num_nodes, device=device)
        dst = torch.arange(num_nodes, device=device)
        self_loop_etype_tensor = torch.full(
            (num_nodes,),
            fill_value=self_loop_etype,
            dtype=torch.long,
            device=device
        )

        # å…ˆå–åŸ etypeï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if "etype" in graph.edata:
            old_etype = graph.edata["etype"]
        else:
            old_etype = None

        # æ·»åŠ  self-loop è¾¹
        graph.add_edges(src, dst)

        # é‡æ–°è®¾ç½® etypeï¼ˆä¸€æ¬¡æ€§ï¼‰
        if old_etype is not None:
            graph.edata["etype"] = torch.cat(
                [old_etype, self_loop_etype_tensor],
                dim=0
            )
        else:
            graph.edata["etype"] = self_loop_etype_tensor

        return graph


    def build_graphs_from_session(self, session_row):
        """ä¸ºæ¯ä¸ªsessionæ„å»ºæµå…³ç³»å›¾ï¼ˆé‡æ„ç‰ˆï¼‰"""
        # 1. æå–æµè®°å½•
        flows = self._extract_flow_records(session_row)
        if not flows:
            if verbose:
                logger.debug(f"[NO_GRAPH] session={session_row.get('session_index')}, "
                    f"split={session_row.get('split')}, "
                    f"reason=no_flows_or_invalid_uids")            
            return [], [], {"num_flows": 0, "num_bursts": 0, "num_edges_avg": 0, "num_graphs": 0}
        else:
            if verbose:
                logger.debug(f" Session {session_row['session_index']} æå–åˆ° {len(flows)} æ¡Flowè®°å½•")
        
        # 2. æ’åºæµè®°å½•
        flows.sort(key=lambda x: x['ts'])
        
        # 3. æ„å»ºå›¾
        graph_list, node_uids_list, edges_count_list, num_bursts = self.build_graphs_from_flows(flows)
        
        # 6. ç»Ÿè®¡ä¿¡æ¯
        stats = {
            "num_flows": len(flows),
            "num_bursts": num_bursts,
            "num_edges_avg": int(np.mean(edges_count_list)) if edges_count_list else 0,
            "num_graphs": len(graph_list)
        }
        
        return graph_list, node_uids_list, stats


    def _extract_flow_records(self, session_row):
        """ä»sessionè¡Œä¸­æå–æµè®°å½•ï¼ˆå¥å£®è§£æç‰ˆï¼‰"""
        flow_uid_raw = session_row.get('flow_uid_list', None)
        session_index = session_row.get('session_index', 'unknown')
        
        if verbose:
            logger.debug(f" Session {session_index}: flow_uid_raw = {flow_uid_raw}")
        
        # ---------- 1ï¸âƒ£ è§£æ flow_uid_list ----------
        flow_uid_list = []
        if isinstance(flow_uid_raw, str):
            try:
                parsed = ast.literal_eval(flow_uid_raw)  # å°è¯•å°†å­—ç¬¦ä¸²å®‰å…¨åœ°è½¬ä¸º Python å¯¹è±¡
                if isinstance(parsed, list):
                    flow_uid_list = [str(uid).strip() for uid in parsed if str(uid).strip()]
                else:
                    logger.warning(f" éåˆ—è¡¨ç±»å‹ flow_uid_listï¼š{type(parsed)} session={session_row.get('session_index')}")
            except Exception as e:
                logger.error(f" æ— æ³•è§£æ flow_uid_list (session={session_row.get('session_index')}): {e}")
        elif isinstance(flow_uid_raw, list):
            # å·²ç»æ˜¯åˆ—è¡¨ï¼ˆæå°‘æ•°æƒ…å†µï¼‰
            flow_uid_list = flow_uid_raw
        else:
            logger.warning(f" flow_uid_list ç±»å‹å¼‚å¸¸: {type(flow_uid_raw)} session={session_row.get('session_index')}")
        
        if not flow_uid_list:
            if verbose:
                logger.debug(f"[EMPTY_UID] session={session_row.get('session_index')}, split={session_row.get('split')}")
            return []
        else:
            if verbose:
                logger.debug(f" Session {session_index}: è§£æåˆ° {len(flow_uid_list)} ä¸ªflow_uid")
        
        # ---------- 2ï¸âƒ£ æ„å»º flow_record ----------
        flows = []
        missing_uids = 0
        for flow_uid in flow_uid_list:
            flow_record = self.flow_node_builder.get_flow_record(flow_uid)
            if verbose:
                logger.debug(f" Flow UID: {flow_uid}, Record: {flow_record is not None}")
            if flow_record:
                if verbose:
                    logger.debug(f" Flow TS: {flow_record.get('ts')}, Conn TS: {flow_record.get('conn.ts')}")
                flows.append(flow_record)
            else:
                missing_uids += 1

        # ---------- 3ï¸âƒ£ è°ƒè¯•ç»Ÿè®¡ ----------
        if verbose and missing_uids > 0:
            logger.debug(
                f"[LAZY_FILTER] session={session_index}: "
                f"dropped {missing_uids} / {len(flow_uid_list)} flows "
                f"due to flow-level filtering and sampling"
            )

        return flows


    def create_concurrent_edges(self, burst, edges, mode="k_nearest", k=3):
        """
        åœ¨ burst å†…åˆ›å»ºè¾¹ï¼Œæ ¹æ®ä¸åŒç­–ç•¥ï¼š
        - full_connect: å…¨è¿æ¥ O(n^2)
        - chain_connect: é“¾å¼ç›¸è¿ O(n)
        - k_nearest: æ¯ä¸ªèŠ‚ç‚¹ä¸æ—¶é—´ä¸Šæœ€è¿‘çš„ k ä¸ªç›¸è¿ O(kn)
        """
        n = len(burst)
        if n <= 1:
            return
        
        if mode == "full_connect":
            for j in range(n - 1):
                for k in range(j + 1, n):
                    edges.append((burst[j]['id'], burst[k]['id'], 0)) # etype 0=concurrent, 1=trigger

        elif mode == "chain_connect":
            # é¡ºåºé“¾å¼è¿æ¥
            for j in range(n - 1):
                edges.append((burst[j]['id'], burst[j+1]['id'], 0)) # etype 0=concurrent, 1=trigger

        elif mode == "k_nearest":
            # å‡è®¾ burst å†…å·²æŒ‰æ—¶é—´æ’åº
            for j in range(n):
                for d in range(1, k+1):
                    if j + d < n:
                        edges.append((burst[j]['id'], burst[j+d]['id'], 0)) # etype 0=concurrent, 1=trigger
        else:
            raise ValueError(f"Unknown mode: {mode}")
        
    def build_graphs_from_flows(self, flows):
        if not flows or len(flows) == 0:
            return [], [], [], 0
              
        # ==================== 1. Flow Burstèšç±» ====================
        flow_bursts = self._cluster_flows_into_bursts(flows)
        
        # ==================== 2. ä¼šè¯çª—å£èšåˆ ====================
        def calculate_avg_interval(window_flows):
            """è®¡ç®—çª—å£ä¸­æµä¹‹é—´çš„å¹³å‡æ—¶é—´é—´éš”"""            
            if len(window_flows) <= 1: # å¦‚æœwindowsçš„æµæ•°é‡å°‘äº2ï¼Œé‚£ä¹ˆè¿”å›ç¼ºçœçš„æµæ•°é‡
                return self.concurrent_flow_iat_threshold 
            times = [f['ts'] for f in window_flows]
            return (max(times) - min(times)) / (len(window_flows) - 1)
        
        session_windows = []
        current_window = list(flow_bursts[0])
        if verbose:
            logger.debug(f" åˆå§‹çª—å£: {len(current_window)}ä¸ªæµ, å¹³å‡é—´éš”: {calculate_avg_interval(current_window):.2f}s")

        for burst_idx in range(1, len(flow_bursts)):
            flow_burst = flow_bursts[burst_idx]
            avg_before = calculate_avg_interval(current_window)
            avg_after = calculate_avg_interval(current_window + flow_burst)

            if verbose:
                logger.debug(f" Burst {burst_idx}: {len(flow_burst)}ä¸ªæµ")
                logger.debug(f"   åˆå¹¶å‰å¹³å‡é—´éš”: {avg_before:.2f}s, åˆå¹¶åå¹³å‡é—´éš”: {avg_after:.2f}s")
                logger.debug(f"   é˜ˆå€¼æ£€æŸ¥: {avg_after:.2f}s <= {self.sequential_flow_iat_threshold * avg_before:.2f}s ?")
            
            if avg_after <= self.sequential_flow_iat_threshold * avg_before:
                current_window.extend(flow_burst)
                if verbose:
                    logger.debug(f"   âœ… åˆå¹¶åˆ°å½“å‰çª—å£ï¼Œçª—å£å¤§å°: {len(current_window)}")                
            else:
                session_windows.append(current_window)
                if verbose:
                    logger.debug(f"   âŒ åˆ›å»ºæ–°çª—å£ï¼ŒåŸçª—å£å¤§å°: {len(current_window)}")                
                current_window = list(flow_burst)
                    
        if current_window and len(current_window) > 0:
            session_windows.append(current_window)
            if verbose:
                logger.debug(f" æœ€ç»ˆçª—å£å¤§å°: {len(current_window)}")    
        
        # æœ€ç»ˆç»Ÿè®¡
        if verbose:
            logger.debug(f" === ä¼šè¯çª—å£èšåˆå®Œæˆ ===")
            logger.debug(f" æ€»å…±ç”Ÿæˆ {len(session_windows)} ä¸ªä¼šè¯çª—å£")
            for i, window in enumerate(session_windows):
                logger.debug(f" çª—å£{i}: {len(window)}ä¸ªæµ, æ—¶é—´èŒƒå›´: {min([f['ts'] for f in window]):.1f} - {max([f['ts'] for f in window]):.1f}")

        # ==================== 3. æµå…³ç³»å›¾æ„å»º ====================
        graph_list, node_uids_list, edges_count_list = [], [], []
        for session_win in session_windows:
            # èŠ‚ç‚¹ç‰¹å¾æ„å»º
            flow_uids = [flow['uid'] for flow in session_win]
            nodes = self.flow_node_builder.build_node_features(flow_uids)           
            if not nodes or len(nodes) == 0: 
                logger.error("æµå…³ç³»å›¾æ„å»ºï¼šèŠ‚ç‚¹ç‰¹å¾æ„å»ºå¤±è´¥ï¼æ£€æŸ¥build_node_features()")
                continue
            else:
                if verbose:
                    logger.debug(f" æµå…³ç³»å›¾æ„å»ºï¼šèŠ‚ç‚¹åºåˆ—é•¿åº¦: {len(nodes)}")
                        
            # å¹¶è¡Œè¾¹æ„å»º
            for idx, node in enumerate(nodes):
                node['id'] = idx            
            edges = []
            node_bursts = self._create_current_edges_by_node_clustering(nodes, edges) # burstèšç±»é€»è¾‘
            
            # é¡ºåºè¾¹æ„å»º
            for j in range(len(node_bursts) - 1):
                current_node_burst = node_bursts[j]
                next_node_burst = node_bursts[j + 1]
                
                # è°ƒè¯•ä¿¡æ¯
                if len(node_bursts) > 1:
                    if verbose:
                        logger.debug(f" âœ… Burstè¿æ¥ {j}â†’{j+1}: len(current_node_burst)={len(current_node_burst)}, len(next_node_burst)={len(next_node_burst)}, len(node_bursts)={len(node_bursts)}")
                
                if len(current_node_burst) == 1 and len(next_node_burst) == 1:
                    edges.append((current_node_burst[0]['id'], next_node_burst[0]['id'], 1))
                else:
                    edges.append((current_node_burst[-1]['id'], next_node_burst[-1]['id'], 1))
                    edges.append((current_node_burst[0]['id'], next_node_burst[0]['id'], 1))
            
            if nodes and len(nodes) > 0:
                node_uids = [node['uid'] for node in nodes]
                graph = self.build_flow_relation_graph(nodes, edges)
                graph_list.append(graph)
                node_uids_list.append(node_uids)
                edges_count_list.append(len(edges))
                if verbose:
                    logger.debug(f" æ„å»ºåˆ° {len(edges)} æ¡è¾¹")

        if node_bursts and len(node_bursts) > 0:
            if verbose:
                logger.debug(f" ğŸ“Š æ€»node_burstæ•°: {len(node_bursts)}, ç¬¬ä¸€ä¸ªnode_burstå¤§å°: {len(node_bursts[0])}ï¼Œæœ€åä¸€ä¸ªnode_burstå¤§å°: {len(node_bursts[-1])}")
        
        return graph_list, node_uids_list, edges_count_list, len(flow_bursts)

    def _cluster_flows_into_bursts(self, flows):
        if verbose:
            logger.debug(f" Flowæ—¶é—´æˆ³åºåˆ—: {[flow.get('ts', 0) for flow in flows]}")
            logger.debug(f" æ—¶é—´å·®è®¡ç®—: {[abs(flows[i+1]['ts'] - flows[i]['ts']) for i in range(len(flows)-1)]}")

        flows.sort(key=lambda x: x['ts'])
        flow_bursts = []
        current_flow_burst = [flows[0]]
        
        for idx in range(1, len(flows)):
            flow = flows[idx]
            time_diff = abs(flow['ts'] - current_flow_burst[-1]['ts'])
            if time_diff <= self.concurrent_flow_iat_threshold:
                current_flow_burst.append(flow)
                if verbose:
                    logger.debug(f" æ·»åŠ åˆ°å½“å‰flow burst (delta={time_diff:.2f}s)")
            elif current_flow_burst and len(current_flow_burst) > 0: # ç¡®ä¿å½“å‰burstä¸ä¸ºç©º
                flow_bursts.append(current_flow_burst)
                if verbose:
                    logger.debug(f" æ–°å»ºflow burst (delta={time_diff:.2f}s > {self.concurrent_flow_iat_threshold}s)")
                current_flow_burst = [flow]
        
        # å¤„ç†æœ€åä¸€ä¸ªburst
        if current_flow_burst and len(current_flow_burst) > 0:
            flow_bursts.append(current_flow_burst)
        
        # éªŒè¯burstå¤§å°åˆ†å¸ƒ
        burst_sizes = [len(burst) for burst in flow_bursts]
        if burst_sizes and len(flow_bursts) > 0:
            if verbose:
                logger.debug(f" ğŸ“ˆ Flow burstå¤§å°åˆ†å¸ƒ: æ€»æ•°={len(burst_sizes)}, å¹³å‡å¤§å°={np.mean(burst_sizes):.2f}")
                logger.debug(f" ğŸ“Š Flow burstå¤§å°è¯¦æƒ…: {burst_sizes}")
                logger.debug(f" ğŸ” æœ€åä¸€ä¸ªflow burstå¤§å°: {burst_sizes[-1]}")
            
        return flow_bursts
        
    def _create_current_edges_by_node_clustering(self, nodes, edges):        
        """çª—å£å†…burstèšç±»æ–¹æ³•"""
        if not nodes or len(nodes) == 0:
            raise ValueError("èŠ‚ç‚¹åˆ—è¡¨ä¸ºç©ºï¼Œæ— æ³•è¿›è¡Œburstèšç±»")  # æ­£ç¡®çš„å¼‚å¸¸æŠ›å‡º
        
        node_bursts = []
        current_node_burst = [nodes[0]]
        burst_id_counter = 0  # æ–°å¢ï¼šburst_idè®¡æ•°å™¨
        # ç¡®ä¿ç¬¬ä¸€ä¸ªèŠ‚ç‚¹æœ‰æ­£ç¡®çš„burst_id
        nodes[0]['burst_id'] = burst_id_counter
        
        # ä½¿ç”¨æ›´ç²¾ç¡®çš„æ—¶é—´é—´éš”è®¡ç®—
        for idx in range(1, len(nodes)):
            node = nodes[idx]
            # è®¡ç®—ä¸å½“å‰burstæœ€åä¸€ä¸ªèŠ‚ç‚¹çš„æ—¶é—´å·®
            time_diff = abs(node['ts'] - current_node_burst[-1]['ts'])
            if time_diff <= self.concurrent_flow_iat_threshold:
                current_node_burst.append(nodes[idx])
                if verbose:
                    logger.debug(f" æ·»åŠ åˆ°å½“å‰burst (delta={time_diff:.2f}s)")
                # ä¸ºåŒä¸€burstå†…çš„èŠ‚ç‚¹è®¾ç½®ç›¸åŒburst_id
                nodes[idx]['burst_id'] = burst_id_counter                    
            elif current_node_burst and len(current_node_burst) > 0:  # ç¡®ä¿å½“å‰burstä¸ä¸ºç©º
                # å®Œæˆå½“å‰burst
                self.create_concurrent_edges(current_node_burst, edges)
                node_bursts.append(current_node_burst)
                if verbose:
                    logger.debug(f" æ–°å»ºburst (delta={time_diff:.2f}s > {self.concurrent_flow_iat_threshold}s)")

                # å¼€å§‹æ–°burst
                burst_id_counter += 1  # æ–°å¢burst_id
                current_node_burst = [node]
                # ä½¿ç”¨é€’å¢åçš„burst_id
                nodes[idx]['burst_id'] = burst_id_counter
        
        # å¤„ç†æœ€åä¸€ä¸ªburst
        if current_node_burst and len(current_node_burst) > 0:
            self.create_concurrent_edges(current_node_burst, edges)
            node_bursts.append(current_node_burst)
        
        # éªŒè¯burstå¤§å°åˆ†å¸ƒ
        burst_sizes = [len(burst) for burst in node_bursts]
        if burst_sizes and len(node_bursts) > 0:
            if verbose:
                logger.debug(f" ğŸ“ˆ Node Burstå¤§å°åˆ†å¸ƒ: æ€»æ•°={len(burst_sizes)}, å¹³å‡å¤§å°={np.mean(burst_sizes):.2f}")
                logger.debug(f" ğŸ“Š Node Burstå¤§å°è¯¦æƒ…: {burst_sizes}")
                logger.debug(f" ğŸ” æœ€åä¸€ä¸ªnode burstå¤§å°: {burst_sizes[-1]}")
        
        return node_bursts

    def _validate_sequential_edges(self, bursts, edges, sequential_edges_count):
        """éªŒè¯sequential edgesæ•°é‡æ˜¯å¦ç¬¦åˆé¢„æœŸ"""
        if not bursts:
            return
        
        expected_edges = 0
        validation_details = []
        
        # è®¡ç®—é¢„æœŸçš„è¾¹æ•°é‡
        for j in range(len(bursts) - 1):
            current_size = len(bursts[j])
            next_size = len(bursts[j + 1])
            
            if current_size == 1 and next_size == 1:
                expected_edges += 1
                validation_details.append(f"Burst {j}â†’{j+1}: 1æ¡è¾¹ (1â†’1)")
            else:
                expected_edges += 2
                validation_details.append(f"Burst {j}â†’{j+1}: 2æ¡è¾¹ ({current_size}â†’{next_size})")
        
        # éªŒè¯å®é™…è¾¹æ•°é‡
        actual_sequential_edges = len([e for e in edges if len(e) == 3 and e[2] == 1])
        
        # è®°å½•éªŒè¯ç»“æœ
        validation_result = {
            'total_bursts': len(bursts),
            'expected_sequential_edges': expected_edges,
            'actual_sequential_edges': actual_sequential_edges,
            'sequential_edges_count': sequential_edges_count,
            'is_valid': expected_edges == actual_sequential_edges == sequential_edges_count,
            'details': validation_details
        }
        
        # è¾“å‡ºéªŒè¯ä¿¡æ¯
        if not validation_result['is_valid']:
            logger.info(f"âš ï¸ SEQUENTIAL EDGESéªŒè¯å¤±è´¥:")
            logger.info(f"   é¢„æœŸè¾¹æ•°: {expected_edges}")
            logger.info(f"   å®é™…è¾¹æ•°: {actual_sequential_edges}")
            logger.info(f"   è®¡æ•°è¾¹æ•°: {sequential_edges_count}")
            logger.info(f"   Burstæ•°é‡: {len(bursts)}")
            
            for detail in validation_details:
                logger.info(f"   {detail}")
            
            # è°ƒè¯•ä¿¡æ¯ï¼šæ˜¾ç¤ºæ‰€æœ‰sequential edges
            sequential_edges = [e for e in edges if len(e) == 3 and e[2] == 1]
            logger.info(f"   Sequential edgesè¯¦æƒ…:")
            for i, edge in enumerate(sequential_edges):
                logger.info(f"     è¾¹{i}: {edge[0]} â†’ {edge[1]} (ç±»å‹: {edge[2]})")
        elif verbose:
            logger.info(f"âœ… Sequential edgeséªŒè¯é€šè¿‡: å­˜åœ¨{expected_edges}æ¡é¡ºåºè¾¹ï¼Œç¬¦åˆé¢„æœŸ")
        
        # å­˜å‚¨éªŒè¯ç»“æœï¼ˆå¯é€‰ï¼‰
        self.last_validation_result = validation_result
        
        return validation_result

    def split_datasets(self):
        """æ ¹æ®sessionçš„splitåˆ—åˆ’åˆ†æ•°æ®é›† - ä¿®å¤ç‰ˆæœ¬"""
        if not self.graph_list:
            return

        # ç¡®ä¿æ‰€æœ‰åˆ—è¡¨é•¿åº¦ä¸€è‡´
        min_len = min(len(self.graph_list), 
                    len(self.graph_split_list_by_session),
                    len(self.label_name_list),
                    len(self.label_id_list))
        
        if min_len != len(self.graph_list):
            logger.warning(f"åˆ—è¡¨é•¿åº¦ä¸ä¸€è‡´ï¼Œå°†æˆªæ–­åˆ°æœ€å°é•¿åº¦ {min_len}")
            self.graph_list = self.graph_list[:min_len]
            self.graph_split_list_by_session = self.graph_split_list_by_session[:min_len]
            self.label_name_list = self.label_name_list[:min_len]
            self.label_id_list = self.label_id_list[:min_len]
        
        # é‡æ–°åˆå§‹åŒ–ç´¢å¼•åˆ—è¡¨
        self.train_index = []
        self.validate_index = []
        self.test_index = []
        
        # ç»Ÿè®¡splitåˆ†å¸ƒ
        split_counts = {'train': 0, 'validate': 0, 'test': 0, 'other': 0}
        
        if verbose:
            logger.debug(f" å¼€å§‹åˆ’åˆ†æ•°æ®é›†ï¼Œæ€»å›¾æ•°: {len(self.graph_split_list_by_session)}")
        
        for idx, split in enumerate(self.graph_split_list_by_session):
            if split is None or pd.isna(split):
                split_str = 'unknown'  # é»˜è®¤å€¼
            else:
                split_str = str(split).strip().lower()
            
            # æ›´çµæ´»çš„åŒ¹é…é€»è¾‘
            if split_str == 'train':
                self.train_index.append(idx)
                split_counts['train'] += 1
            elif split_str in ['validate', 'valid', 'validation', 'val']:
                self.validate_index.append(idx)
                split_counts['validate'] += 1
            elif split_str in ['test', 'testing']:
                self.test_index.append(idx)
                split_counts['test'] += 1
            else:
                # æœªçŸ¥çš„splitå€¼ï¼Œæ ¹æ®å†…å®¹åˆ¤æ–­
                if 'test' in split_str:
                    self.test_index.append(idx)
                    split_counts['test'] += 1
                elif 'validate' in split_str or 'valid' in split_str:
                    self.validate_index.append(idx)
                    split_counts['validate'] += 1
                else:
                    if verbose and split_counts['other'] < 5:
                        logger.warning(f"Graph {idx} has unknown split '{split_str}', skipping (NOT adding to train).")
                    split_counts['other'] += 1
            
            # è°ƒè¯•ä¿¡æ¯
            if verbose and idx < 5:  # åªæ‰“å°å‰5ä¸ª
                logger.debug(f" å›¾ {idx}: split='{split}' -> åˆ†é…ä¸º '{split_str}'")
        
        # éªŒè¯åˆ’åˆ†ç»“æœ
        self._validate_split_consistency(split_counts)

    def _validate_split_consistency(self, split_counts):
        """éªŒè¯åˆ’åˆ†ä¸€è‡´æ€§"""
        logger.info("\n=== åˆ’åˆ†éªŒè¯ ===")
        
        # æ–¹æ³•1ï¼šé€šè¿‡splitåˆ—è¡¨è®¡ç®—
        calculated_train = []
        calculated_validate = []
        calculated_test = []
        
        for idx, split in enumerate(self.graph_split_list_by_session):
            if split is None or pd.isna(split):
                split_str = 'train'
            else:
                split_str = str(split).strip().lower()
            
            if split_str == 'train':
                calculated_train.append(idx)
            elif split_str in ['validate', 'valid', 'validation', 'val']:
                calculated_validate.append(idx)
            elif split_str == 'test':
                calculated_test.append(idx)
            else:
                calculated_train.append(idx)  # é»˜è®¤åˆ†é…åˆ°è®­ç»ƒé›†
        
        # æ–¹æ³•2ï¼šç›´æ¥ä½¿ç”¨å­˜å‚¨çš„ç´¢å¼•
        stored_train = self.train_index
        stored_validate = self.validate_index
        stored_test = self.test_index
        
        # æ¯”è¾ƒç»“æœ
        train_match = set(calculated_train) == set(stored_train)
        validate_match = set(calculated_validate) == set(stored_validate)
        test_match = set(calculated_test) == set(stored_test)
        
        logger.info(f"è®­ç»ƒé›†åŒ¹é…: {train_match} (è®¡ç®—: {len(calculated_train)}, å­˜å‚¨: {len(stored_train)})")
        logger.info(f"éªŒè¯é›†åŒ¹é…: {validate_match} (è®¡ç®—: {len(calculated_validate)}, å­˜å‚¨: {len(stored_validate)})")
        logger.info(f"æµ‹è¯•é›†åŒ¹é…: {test_match} (è®¡ç®—: {len(calculated_test)}, å­˜å‚¨: {len(stored_test)})")
        
        # å¦‚æœä¸åŒ¹é…ï¼Œæ˜¾ç¤ºå·®å¼‚
        if not validate_match:
            logger.info("\néªŒè¯é›†å·®å¼‚åˆ†æ:")
            calc_only = set(calculated_validate) - set(stored_validate)
            stored_only = set(stored_validate) - set(calculated_validate)
            
            if calc_only:
                logger.info(f"è®¡ç®—æœ‰ä½†å­˜å‚¨æ— : {calc_only}")
                for idx in calc_only:
                    if idx < len(self.graph_split_list_by_session):
                        logger.info(f"  å›¾ {idx}: split='{self.graph_split_list_by_session[idx]}'")
            
            if stored_only:
                logger.info(f"å­˜å‚¨æœ‰ä½†è®¡ç®—æ— : {stored_only}")
                for idx in stored_only:
                    if idx < len(self.graph_split_list_by_session):
                        logger.info(f"  å›¾ {idx}: split='{self.graph_split_list_by_session[idx]}'")
        
        # å¦‚æœéªŒè¯å¤±è´¥ï¼Œä½¿ç”¨è®¡ç®—çš„ç»“æœ
        if not (train_match and validate_match and test_match):
            logger.warning("\nâš ï¸ åˆ’åˆ†ä¸ä¸€è‡´ï¼Œä½¿ç”¨è®¡ç®—çš„ç»“æœ")
            self.train_index = calculated_train
            self.validate_index = calculated_validate
            self.test_index = calculated_test
            
            # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
            split_counts = {
                'train': len(calculated_train),
                'validate': len(calculated_validate), 
                'test': len(calculated_test),
                'other': 0
            }
        
        # æœ€ç»ˆç»Ÿè®¡
        total_graphs = len(self.graph_list)
        if total_graphs > 0:
            logger.info(f"===æœ€ç»ˆæ•°æ®é›†åˆ’åˆ†===")
            logger.info(f"è®­ç»ƒé›†: {len(self.train_index)} å›¾ ({len(self.train_index)/total_graphs:.1%})")
            logger.info(f"éªŒè¯é›†: {len(self.validate_index)} å›¾ ({len(self.validate_index)/total_graphs:.1%})")
            logger.info(f"æµ‹è¯•é›†: {len(self.test_index)} å›¾ ({len(self.test_index)/total_graphs:.1%})")
            

    def extract_train_flow_uids(self):
        """æå–è®­ç»ƒé›†ä¸­æ‰€æœ‰flow UIDçš„é›†åˆï¼ˆå»é‡ï¼‰"""
        train_flow_uids = set()
        
        for idx in self.train_index:
            node_uids = self.graph_node_uids_list[idx]  # å•ä¸ªå›¾çš„èŠ‚ç‚¹UIDåˆ—è¡¨
            
            # å°†å½“å‰å›¾çš„èŠ‚ç‚¹UIDæ·»åŠ åˆ°é›†åˆä¸­ï¼ˆè‡ªåŠ¨å»é‡ï¼‰
            train_flow_uids.update(node_uids)  # æ­£ç¡®ç”¨æ³•ï¼šå°†åˆ—è¡¨å…ƒç´ æ·»åŠ åˆ°set
        
        return train_flow_uids  # è¿”å›setï¼Œå˜é‡åæ”¹ä¸º_uids

    def save_results(self):
        """
        ä¿å­˜å›¾å’Œé™„åŠ ä¿¡æ¯
        è‡ªåŠ¨ç”Ÿæˆ:
          xxx.bin   -> DGL å›¾
          xxx_info.pkl -> æ ‡ç­¾/å…ƒä¿¡æ¯
        """
        if not self.graph_list:
            logger.info("No graphs to save!")
            return
                
        base, _ = os.path.splitext(self.dumpFilename)
        bin_path = base + ".bin"
        info_path = base + "_info.pkl"

        logger.info(f"Saving graphs to {bin_path} ...")
        save_graphs(bin_path, self.graph_list)
        
        # éªŒè¯ä¸€è‡´æ€§
        assert len(self.label_name_list) == len(self.graph_split_list_by_session)
        
        # éªŒè¯ç´¢å¼•ä¸splitåˆ—è¡¨çš„ä¸€è‡´æ€§
        calculated_train = [i for i, s in enumerate(self.graph_split_list_by_session) 
                        if s.lower() == 'train']
        calculated_validate = [i for i, s in enumerate(self.graph_split_list_by_session) 
                        if s.lower() in ['validate', 'valid', 'validation', 'val']]
        calculated_test = [i for i, s in enumerate(self.graph_split_list_by_session) 
                        if s.lower() == 'test']
        
        assert set(calculated_train) == set(self.train_index)
        assert set(calculated_validate) == set(self.validate_index) 
        assert set(calculated_test) == set(self.test_index)
         
        graph_infos = {
            # ===== è¯­ä¹‰ä¿¡æ¯ï¼ˆä¸å‚ä¸è®­ç»ƒï¼Œä»…ç”¨äºåˆ†æ/è°ƒè¯•ï¼‰=====
            'label_name': self.label_name_list,      # æ¯ä¸ªå›¾çš„æ ‡ç­¾å
            'label_id': self.label_id_list,          # æ¯ä¸ªå›¾çš„æ ‡ç­¾ID
            "is_malicious": self.is_malicious_list,  # æ¯ä¸ªå›¾çš„å–„æ„/æ¶æ„æ ‡ç­¾
            'split': self.graph_split_list_by_session,  # æ¯ä¸ªå›¾çš„åˆ’åˆ†ä¿¡æ¯

            # ===== æ‰§è¡Œçº§ç´¢å¼•ï¼ˆè®­ç»ƒ/éªŒè¯/æµ‹è¯•å”¯ä¸€å¯ä¿¡æ¥æºï¼‰=====
            'train_index': self.train_index,        # è®­ç»ƒé›†ç´¢å¼•ï¼ˆå¿«é€Ÿè®¿é—®ï¼‰
            'validate_index': self.validate_index,        # éªŒè¯é›†ç´¢å¼•ï¼ˆå¿«é€Ÿè®¿é—®ï¼‰
            'test_index': self.test_index,           # æµ‹è¯•é›†ç´¢å¼•ï¼ˆå¿«é€Ÿè®¿é—®ï¼‰

            # ===== â­ å„ä¸ªè§†å›¾çš„ç‰¹å¾ç»´åº¦ä¿¡æ¯ =====
            "feature_dims": copy.deepcopy(dict(self.flow_node_builder.global_node_feature_dims)),
            "enabled_views": copy.deepcopy(dict(self.flow_node_builder.enabled_views)),
        }
        
        save_info(info_path, graph_infos)
        
        logger.info("Graph construction completed successfully!")