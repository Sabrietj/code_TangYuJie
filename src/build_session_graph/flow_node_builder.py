import tqdm
import os, sys
import pandas as pd
import numpy as np
import json
import ast
from typing import Union, List
import logging
from collections import Counter
from transformers import BertTokenizer
from types import MappingProxyType
from typing import List, Tuple

# å¯¼å…¥é…ç½®ç®¡ç†æ¨¡å—
try:
    # æ·»åŠ ../utilsç›®å½•åˆ°Pythonæœç´¢è·¯å¾„
    utils_path = os.path.join(os.path.dirname(__file__), '..', 'utils')
    sys.path.insert(0, utils_path)
    from zeek_columns import conn_columns, http_columns
    from zeek_columns import conn_numeric_columns, conn_categorical_columns, conn_textual_columns
    from zeek_columns import flowmeter_numeric_columns, flowmeter_categorical_columns, flowmeter_textual_columns
    from zeek_columns import ssl_numeric_columns, ssl_categorical_columns, ssl_textual_columns
    from zeek_columns import dns_numeric_columns, dns_categorical_columns, dns_textual_columns
    from zeek_columns import x509_numeric_columns, x509_categorical_columns, x509_textual_columns
    from zeek_columns import max_x509_cert_chain_len, dtype_dict_in_flow_csv
    from config_manager import read_session_label_id_map, read_text_encoder_config
    from logging_config import setup_preset_logging
    # ä½¿ç”¨ç»Ÿä¸€çš„æ—¥å¿—é…ç½®
    logger = setup_preset_logging(log_level=logging.DEBUG)    
except ImportError:
    # å¦‚æœå¯¼å…¥å¤±è´¥ï¼Œæä¾›ä¸€ä¸ªé»˜è®¤å®ç°
    def read_session_label_id_map():
        return {'benign': 0, 'background': 1, 'mixed': 2, 'malicious': 3}
    
class FlowNodeBuilder:
    """å¤„ç†å°†ç½‘ç»œæµæ•°æ®æ„é€ æˆå›¾èŠ‚ç‚¹çš„ç±»ï¼Œè´Ÿè´£åŠ è½½å’Œé¢„å¤„ç†æµç‰¹å¾"""
    def __init__(self, flow_csv_path, session_label_id_map, max_packet_sequence_length, text_encoder_name, max_text_length, thread_count=1, enabled_views=None):
        self.mtu_normalize = 1500
        self.max_packet_sequence_length = max_packet_sequence_length
        self.text_encoder_name = text_encoder_name
        self.max_text_length = max_text_length
        self.text_tokenizer, self.max_text_length = load_text_tokenizer(
            model_name=self.text_encoder_name,
            max_text_length=self.max_text_length
        )

        self.thread_count = thread_count
        self.enabled_views = enabled_views or {
            "flow_numeric_features": True,
            "flow_categorical_features": True,
            "flow_textual_features": True,
            "packet_len_seq": True,
            "packet_iat_seq": True,
            "domain_probs": True,
            "ssl_numeric_features": True,
            "ssl_categorical_features": True,
            "ssl_textual_features": True,
            "x509_numeric_features": True,
            "x509_categorical_features": True,
            "x509_textual_features": True,
            "dns_numeric_features": True,
            "dns_categorical_features": True,
            "dns_textual_features": True,
        }

        # è¯»å–ä¼šè¯æ ‡ç­¾æ˜ å°„é…ç½®å¹¶è®¡ç®—ç±»åˆ«æ•°é‡
        self.session_label_id_map = session_label_id_map
        self.num_classes = len(set(self.session_label_id_map.values()))
        logger.info(f"Loaded session label string-to-id mapping: len={self.num_classes}, mapping={self.session_label_id_map}")

        logger.info("Loading flow CSV file as a pandas dataframe...")
        flow_df = read_large_csv_with_progress(flow_csv_path)

        # æ„å»º flow uid -> recordçš„å­—å…¸ç»“æ„ï¼Œæ–¹ä¾¿åç»­ä»session rowåŸºäºflow_uid_listçš„æ£€ç´¢å½’å±äºsessionçš„ç½‘ç»œæµã€‚
        self.flow_dict = {}
        logger.info("Building flow dictionary using the 'uid' fields as keys...")
        for _, row in tqdm.tqdm(flow_df.iterrows(), total=len(flow_df)):
            flow_uid = row['uid']
            flow_record = row.to_dict()

            # ===== åœ¨è¿™é‡Œä¸€æ¬¡æ€§è§„èŒƒåŒ– flow_record =====
            flow_record['uid'] = flow_uid
            try:
                flow_record['ts'] = float(flow_record.get('conn.ts', 0.0)) # æ¯ä¸ªrecordæœ‰æ—¶é—´æˆ³ï¼Œæ–¹ä¾¿åç»­å»ºå›¾
            except Exception:
                flow_record['ts'] = 0.0

            self.flow_dict[flow_uid] = MappingProxyType(flow_record) # æŠŠ flow_record å†»ç»“ä¸ºåªè¯»

        self.categorical_vocabulary_group = self.scan_flow_dict_for_categorical_topk_vocab_group(flow_dict = self.flow_dict)
        self.global_node_feature_dims, self.numeric_feature_stats = self.scan_flow_dict_for_node_feature_dims_and_numeric_stats(
            flow_dict = self.flow_dict,
            enabled_views = self.enabled_views,
            max_text_length = self.max_text_length,
            text_tokenizer = self.text_tokenizer,
            max_packet_sequence_length = self.max_packet_sequence_length,
            categorical_vocabulary_group = self.categorical_vocabulary_group,
            num_classes = self.num_classes,
        )
        self.categorical_vocabulary_group = MappingProxyType(self.categorical_vocabulary_group)        
        self.global_node_feature_dims = MappingProxyType(self.global_node_feature_dims)
        self.numeric_feature_stats = MappingProxyType(self.numeric_feature_stats)

        logger.info("âœ… Global node feature dimension summary (enabled views):")
        for view_name, dim in self.global_node_feature_dims.items():
            enabled = self.enabled_views.get(view_name, False)
            status = "ON " if enabled else "OFF"
            logger.info(f"  - [{status}] {view_name}: {dim}")

        # 4ï¸âƒ£ æ˜¾å¼é‡Šæ”¾ flow_dfï¼ŒèŠ‚çº¦å†…å­˜
        del flow_df
        import gc
        gc.collect()
    
    def get_flow_record(self, flow_uid):
        """è·å–æŒ‡å®šUIDçš„æµè®°å½•"""
        record = self.flow_dict.get(flow_uid)
        if record is None:
            logger.info(f"[DEBUG] Flow UID '{flow_uid}' åœ¨flow_dictä¸­æœªæ‰¾åˆ°")
        return record
    
    def get_all_flow_uids(self):
        """è·å–æ‰€æœ‰æµUID"""
        return list(self.flow_dict.keys())
    
    def get_num_classes(self):
        """è·å–ç±»åˆ«æ•°é‡"""
        return self.num_classes

    @staticmethod
    def scan_flow_dict_for_categorical_topk_vocab_group(flow_dict):
        """
        ä»…åŸºäº flow_dict æ„å»º categorical ç‰¹å¾çš„ vocabularyï¼ˆé«˜æ•ˆç‰ˆï¼Œä»…æ‰«æä¸€æ¬¡ flow_dictï¼‰ã€‚
        flow_dict: { uid -> flow_record(dict) }

        è¿”å›:
            vocab_group = {
                col_name: { token -> index }
            }
        """

        top_k_cat = 500  # å¯è°ƒ
        top_k_map = {
            # ---------------- SSL ----------------
            "ssl.cipher": 50,
            "ssl.curve": 10,
            "ssl.version": 6,
            "ssl.next_protocol": 20,
            "ssl.client_signature_algorithms": 50,
            "ssl.server_signature_algorithms": 50,
            "ssl.client_key_exchange_groups": 20,
            "ssl.server_key_exchange_groups": 20,
            "ssl.client_supported_versions": 10,
            "ssl.server_supported_versions": 10,

            # ---------------- DNS ----------------
            "dns.qtype": 40,
            "dns.qclass": 10,
            "dns.rcode_name": 20,
            "dns.qtype_name": 40,
            "dns.qclass_name": 10,
            "dns.rcode": 10,

            # ---------------- conn ----------------
            "conn.proto": 10,
            "conn.service": 50,
            "conn.conn_state": 20,
            "conn.history": 30,
            "conn.local_orig": 3,
            "conn.local_resp": 3,

            # ---------------- flowmeter ----------------
            # Flowmeter categoricalï¼ˆåªæœ‰ protoï¼‰
            "flowmeter.proto": 10,
        }
        
        # æ¯ç§ç±»å‹çš„åˆ—åï¼ŒåŠ ä¸Šå‰ç¼€ â†’ çœŸå® DataFrame åˆ—å
        conn_cat_cols_prefixed      = [f"conn.{c}"      for c in conn_categorical_columns]
        flowmeter_cat_cols_prefixed = [f"flowmeter.{c}" for c in flowmeter_categorical_columns]
        flow_cat_cols_prefixed = conn_cat_cols_prefixed + flowmeter_cat_cols_prefixed
        ssl_cat_cols_prefixed       = [f"ssl.{c}"       for c in ssl_categorical_columns]
        x509_cat_cols_prefixed = []
        for n in [0, 1, 2]:
            x509_cat_cols_prefixed += [f"x509.cert{n}.{c}" for c in x509_categorical_columns]
        dns_cat_cols_prefixed       = [f"dns.{c}"       for c in dns_categorical_columns]
        categorical_columns = (
            flow_cat_cols_prefixed +
            ssl_cat_cols_prefixed +
            x509_cat_cols_prefixed +
            dns_cat_cols_prefixed
        )

        # Counter åˆå§‹åŒ–
        categorical_vocab_counter = {col: Counter() for col in categorical_columns}

        # ğŸ”¥ åªæ‰«æä¸€æ¬¡ flow_dictï¼ˆé«˜æ•ˆï¼‰
        for flow_uid, flow_record in tqdm.tqdm(flow_dict.items(), 
                                               desc="[1st PASS] Scanning categorical vocab", 
                                               unit="flow"):
            for col in categorical_columns:
                raw = flow_record.get(col)

                if raw is None:
                    token = "<OOV>"
                else:
                    token = str(raw).strip() or "<OOV>"

                categorical_vocab_counter[col][token] += 1

        # æ„å»ºæœ€ç»ˆ vocab_group
        categorical_vocab_group = {}
        for col in categorical_columns:
            counter = categorical_vocab_counter[col]

            if not counter:
                categorical_vocab_group[col] = {"<OOV>": 0}
                continue

            # top-k
            this_top_k = next((v for k, v in top_k_map.items() if col.startswith(k)), top_k_cat)
            most = counter.most_common(this_top_k)

            values = [v for v, _ in most]
            mapping = {v: i+1 for i, v in enumerate(values)}
            mapping["<OOV>"] = 0

            categorical_vocab_group[col] = mapping

        return categorical_vocab_group
    
    @staticmethod
    def scan_flow_dict_for_node_feature_dims_and_numeric_stats(flow_dict, enabled_views, max_text_length, text_tokenizer, 
                                                               max_packet_sequence_length, categorical_vocabulary_group, num_classes):
        """è®¡ç®—å…¨å±€çš„æ•°å€¼å‹+ç±»åˆ«å‹èŠ‚ç‚¹ç‰¹å¾ç»´åº¦"""
        global_node_feature_dims = {
            "flow_numeric_features": 0,
            "flow_categorical_features": 0,
            "packet_len_seq": 0,
            "packet_iat_seq": 0,
            "domain_probs": 0,
            "ssl_numeric_features": 0,
            "ssl_categorical_features": 0,
            "x509_numeric_features": 0,
            "x509_categorical_features": 0,
            "dns_numeric_features": 0,
            "dns_categorical_features": 0,
        }

        numeric_feature_stats = {}
        
        for view_name in [
            "flow_numeric_features",
            "ssl_numeric_features",
            "x509_numeric_features",
            "dns_numeric_features",
        ]:
            if enabled_views.get(view_name, False):
                numeric_feature_stats[view_name] = {
                    "count": 0,
                    "sum": None,
                    "sum_of_squares": None,
                }

        logger.info("Calculating global node feature dimensions and numeric features' statistics from flow_dict...")

        for flow_uid, flow_record in tqdm.tqdm(
            flow_dict.items(),
            total=len(flow_dict),
            desc="[2nd PASS] Calc global node feature dims and numeric features' statistics",
            dynamic_ncols=True,
            leave=True,
            mininterval=0.5            
        ):
            try:
                # æå–ç½‘ç»œæµçš„å„ç±»ç‰¹å¾å‘é‡
                if enabled_views.get("flow_numeric_features", False) \
                    or enabled_views.get("flow_categorical_features", False) \
                    or enabled_views.get("flow_textual_features", False):
                    flow_numeric_features, flow_categorical_features, flow_textual_features = extract_conn_and_flowmeter_features(flow_record, categorical_vocabulary_group, text_tokenizer, max_text_length)

                # è®¡ç®—ç½‘ç»œæµçš„æ•°å€¼å‹+ç±»åˆ«å‹ç‰¹å¾ç»´åº¦
                if enabled_views.get("flow_numeric_features", False):
                    global_node_feature_dims['flow_numeric_features'] = max(
                        global_node_feature_dims['flow_numeric_features'], len(flow_numeric_features) if len(flow_numeric_features) > 0 else 1)
                    
                    vec = np.array(flow_numeric_features, dtype=np.float64)
                    if np.any(np.isnan(vec)) or np.any(np.isinf(vec)):                        
                        vec = np.nan_to_num(vec, nan=0.0, posinf=0.0, neginf=0.0) # ä¿®å¤NaNå’ŒInfå€¼

                    stats = numeric_feature_stats["flow_numeric_features"]
                    if stats["sum"] is None:
                        stats["sum"] = np.zeros_like(vec)
                        stats["sum_of_squares"] = np.zeros_like(vec)

                    stats["sum"] += vec
                    stats["sum_of_squares"] += vec * vec
                    stats["count"] += 1
                    
                if enabled_views.get("flow_categorical_features", False):
                    global_node_feature_dims['flow_categorical_features'] = max(
                        global_node_feature_dims['flow_categorical_features'], len(flow_categorical_features) if len(flow_categorical_features) > 0 else 1)
                    
                # è®¡ç®—æ•°æ®åŒ…æ—¶é—´åºåˆ—ç‰¹å¾å‘é‡ç»´åº¦              
                if enabled_views.get("packet_len_seq", False):
                    global_node_feature_dims["packet_len_seq"] = max_packet_sequence_length
                else:
                    global_node_feature_dims["packet_len_seq"] = 0

                if enabled_views.get("packet_iat_seq", False):
                    global_node_feature_dims["packet_iat_seq"] = max_packet_sequence_length
                else:
                    global_node_feature_dims["packet_iat_seq"] = 0
                    
                # æå–åŸºäºdomain-appå…±ç°æ¦‚ç‡çš„åŸŸååµŒå…¥ç‰¹å¾å‘é‡
                if enabled_views.get("domain_probs", False):
                    domain_probs = extract_domain_name_probabilities(flow_record, num_classes)
                # è®¡ç®—åŸºäºdomain-appå…±ç°æ¦‚ç‡çš„åŸŸååµŒå…¥ç‰¹å¾å‘é‡ç»´åº¦
                if enabled_views.get("domain_probs", False):
                    global_node_feature_dims['domain_probs'] = max(
                        global_node_feature_dims['domain_probs'], len(domain_probs) if len(domain_probs) > 0 else 1)

                # æå–SSLçš„å„ç±»ç‰¹å¾å‘é‡
                if enabled_views.get("ssl_numeric_features", False) \
                    or enabled_views.get("ssl_categorical_features", False) \
                    or enabled_views.get("ssl_textual_features", False):
                    ssl_numeric_features, ssl_categorical_features, ssl_textual_features = extract_ssl_features(flow_record, categorical_vocabulary_group, text_tokenizer, max_text_length)

                # è®¡ç®—SSLçš„æ•°å€¼å‹+ç±»åˆ«å‹ç‰¹å¾å‘é‡çš„ç»´åº¦
                if enabled_views.get("ssl_numeric_features", False):
                    global_node_feature_dims['ssl_numeric_features'] = max(
                        global_node_feature_dims['ssl_numeric_features'], len(ssl_numeric_features) if len(ssl_numeric_features) > 0 else 1)
                    
                    vec = np.array(ssl_numeric_features, dtype=np.float64)
                    if np.any(np.isnan(vec)) or np.any(np.isinf(vec)):                        
                        vec = np.nan_to_num(vec, nan=0.0, posinf=0.0, neginf=0.0) # ä¿®å¤NaNå’ŒInfå€¼
                    
                    stats = numeric_feature_stats["ssl_numeric_features"]
                    if stats["sum"] is None:
                        stats["sum"] = np.zeros_like(vec)
                        stats["sum_of_squares"] = np.zeros_like(vec)

                    stats["sum"] += vec
                    stats["sum_of_squares"] += vec * vec
                    stats["count"] += 1
                
                if enabled_views.get("ssl_categorical_features", False):
                    global_node_feature_dims['ssl_categorical_features'] = max(
                        global_node_feature_dims['ssl_categorical_features'], len(ssl_categorical_features) if len(ssl_categorical_features) > 0 else 1)
                    
                # æå–X509çš„å„ç±»ç‰¹å¾å‘é‡
                if enabled_views.get("x509_numeric_features", False) \
                    or enabled_views.get("x509_categorical_features", False) \
                    or enabled_views.get("x509_textual_features", False):
                    x509_numeric_features, x509_categorical_features, x509_textual_features = extract_x509_features(flow_record, categorical_vocabulary_group, text_tokenizer, max_text_length)

                # è®¡ç®—X509çš„æ•°å€¼å‹+ç±»åˆ«å‹ç‰¹å¾å‘é‡çš„ç»´åº¦
                if enabled_views.get("x509_numeric_features", False):
                    global_node_feature_dims['x509_numeric_features'] = max(
                        global_node_feature_dims['x509_numeric_features'], len(x509_numeric_features) if len(x509_numeric_features) > 0 else 1)
                    
                    vec = np.array(x509_numeric_features, dtype=np.float64)
                    if np.any(np.isnan(vec)) or np.any(np.isinf(vec)):                        
                        vec = np.nan_to_num(vec, nan=0.0, posinf=0.0, neginf=0.0) # ä¿®å¤NaNå’ŒInfå€¼

                    stats = numeric_feature_stats["x509_numeric_features"]
                    if stats["sum"] is None:
                        stats["sum"] = np.zeros_like(vec)
                        stats["sum_of_squares"] = np.zeros_like(vec)

                    stats["sum"] += vec
                    stats["sum_of_squares"] += vec * vec
                    stats["count"] += 1
                
                if enabled_views.get("x509_categorical_features", False):
                    global_node_feature_dims['x509_categorical_features'] = max(
                        global_node_feature_dims['x509_categorical_features'], len(x509_categorical_features) if len(x509_categorical_features) > 0 else 1)

                # æå–DNSçš„å„ç±»ç‰¹å¾å‘é‡
                if enabled_views.get("dns_numeric_features", False) \
                    or enabled_views.get("dns_categorical_features", False) \
                    or enabled_views.get("dns_textual_features", False):
                    dns_numeric_features, dns_categorical_features, dns_textual_features = extract_dns_features(flow_record, categorical_vocabulary_group, text_tokenizer, max_text_length)

                # è®¡ç®—DNSçš„æ•°å€¼å‹+ç±»åˆ«å‹ç‰¹å¾å‘é‡çš„ç»´åº¦                
                if enabled_views.get("dns_numeric_features", False):
                    global_node_feature_dims['dns_numeric_features'] = max(
                        global_node_feature_dims['dns_numeric_features'], len(dns_numeric_features) if len(dns_numeric_features) > 0 else 1)
                    
                    vec = np.array(dns_numeric_features, dtype=np.float64)
                    if np.any(np.isnan(vec)) or np.any(np.isinf(vec)):
                        vec = np.nan_to_num(vec, nan=0.0, posinf=0.0, neginf=0.0) # ä¿®å¤NaNå’ŒInfå€¼

                    stats = numeric_feature_stats["dns_numeric_features"]
                    if stats["sum"] is None:
                        stats["sum"] = np.zeros_like(vec)
                        stats["sum_of_squares"] = np.zeros_like(vec)

                    stats["sum"] += vec
                    stats["sum_of_squares"] += vec * vec
                    stats["count"] += 1
                
                if enabled_views.get("dns_categorical_features", False):
                    global_node_feature_dims['dns_categorical_features'] = max(
                        global_node_feature_dims['dns_categorical_features'], len(dns_categorical_features) if len(dns_categorical_features) > 0 else 1)
                    
            except Exception as e:
                logger.error(f"Flow {flow_uid} ç‰¹å¾æå–é”™è¯¯: {e}")
                continue
        
        logger.info(f"Global feature dimensions: {global_node_feature_dims}, with max_packet_sequence_length = {max_packet_sequence_length}")
        
        # æ·»åŠ è°ƒè¯•ä¿¡æ¯ï¼šæ˜¾ç¤ºFlowã€SSLã€X509ã€å’ŒDNSç‰¹å¾çš„å®é™…ç»´åº¦
        logger.info(f"Flow feature dimension breakdown: numeric={global_node_feature_dims['flow_numeric_features']}, categorical={global_node_feature_dims['flow_categorical_features']}")
        logger.info(f"SSL feature dimension breakdown: numeric={global_node_feature_dims['ssl_numeric_features']}, categorical={global_node_feature_dims['ssl_categorical_features']}")
        logger.info(f"X509 feature dimension breakdown: numeric={global_node_feature_dims['x509_numeric_features']}, categorical={global_node_feature_dims['x509_categorical_features']}")
        logger.info(f"DNS feature dimension breakdown: numeric={global_node_feature_dims['dns_numeric_features']}, categorical={global_node_feature_dims['dns_categorical_features']}")

        for k, stats in numeric_feature_stats.items():
            count = stats["count"]
            # â­ æ ¸å¿ƒé˜²æŠ¤
            if count == 0 or stats["sum"] is None:
                logger.warning(
                    f"scan_flow_dict_for_node_feature_dims_and_numeric_stats(): [NUMERIC-STATS] Skip {k}: count={count}, sum is None"
                )
                stats["mean"] = []
                stats["std"] = []

            else:
                mean = stats["sum"] / count
                var = stats["sum_of_squares"] / count - mean * mean
                std = np.sqrt(np.maximum(var, 1e-12))

                stats["mean"] = mean.tolist()
                stats["std"] = std.tolist()
            
        return global_node_feature_dims, numeric_feature_stats

    def get_global_node_feature_dims(self, key):
        assert hasattr(self, 'global_node_feature_dims'), \
            "global_node_feature_dims must be initialized in __init__"
            
        return self.global_node_feature_dims[key]
    
    def build_node_features(self, flow_uids):
        """ä¸ºæŒ‡å®šçš„æµUIDæ„å»ºèŠ‚ç‚¹ç‰¹å¾"""
        flow_dict = self.flow_dict
        enabled_views = self.enabled_views
        max_text_length = self.max_text_length
        categorical_vocabulary_group = self.categorical_vocabulary_group
        text_tokenizer = self.text_tokenizer
        numeric_feature_stats = self.numeric_feature_stats
        mtu_normalize = self.mtu_normalize
        max_packet_sequence_length = self.max_packet_sequence_length
        num_classes = self.num_classes

        if not flow_uids:
            return

        logger.debug("build_node_features(): begin")
        def is_nan_or_inf(x):
            """æ›´å…¨é¢çš„NaN/Infæ£€æŸ¥"""
            if x is None:
                return True
            try:
                # å¤„ç†numpyç±»å‹
                if hasattr(x, 'dtype'):
                    return np.isnan(x) or np.isinf(x)
                # å¤„ç†Pythonæ•°å€¼ç±»å‹
                elif isinstance(x, (int, float, np.number)):
                    return np.isnan(x) or np.isinf(x)
                return False
            except (TypeError, ValueError):
                return False
            
        nodes = []
        # æå–æ¯ä¸ªèŠ‚ç‚¹çš„ç‰¹å¾
        for flow_uid in flow_uids:
            flow_record = flow_dict.get(flow_uid)
            if flow_record is None:
                continue

            node = {
                'uid': flow_record['uid'],
                'ts': flow_record['ts'],
            }
            nodes.append(node)
        
        if not nodes or len(nodes) == 0:
            return []
        
        logger.debug("build_node_features(): nodes list construction is ok")

        # æ„å»ºèŠ‚ç‚¹ç‰¹å¾
        for n in nodes:
            flow_record = flow_dict.get(n['uid'])
            if flow_record is None:
                continue

            if (enabled_views.get("flow_numeric_features", False)
                or enabled_views.get("flow_categorical_features", False)
                or enabled_views.get("flow_textual_features", False)
            ):
                flow_numeric_features, flow_categorical_features, flow_textual_features = extract_conn_and_flowmeter_features(flow_record, categorical_vocabulary_group, text_tokenizer, max_text_length)

            if enabled_views.get("flow_numeric_features", False):
                n['flow_numeric_features'] = flow_numeric_features                   
                if numeric_feature_stats["flow_numeric_features"]["count"] > 1:
                    vec = n['flow_numeric_features']
                    # æ ‡å‡†åŒ–å‰æ£€æŸ¥NaN
                    if any(is_nan_or_inf(x) for x in vec):
                        # logger.warning(f"Flow {flow_uid} æ ‡å‡†åŒ–å‰å‘ç°NaNå€¼")
                        vec = [0.0 if is_nan_or_inf(x) else x for x in vec]                    
                    mean = numeric_feature_stats["flow_numeric_features"]["mean"]
                    std  = numeric_feature_stats["flow_numeric_features"]["std"]
                    assert len(mean) == len(vec), f"Flow numeric feature length inconsistent: mean={len(mean)}, vec={len(vec)}, uid={flow_uid}"
                    assert len(std) == len(vec), f"Flow numeric feature length inconsistent: mean={len(std)}, vec={len(vec)}, uid={flow_uid}"
                    n['flow_numeric_features'] = [(x - m) / s for x, m, s in zip(vec, mean, std)]

                max_flow_numeric_len = self.get_global_node_feature_dims('flow_numeric_features')
                # å¡«å……æˆ–è£å‰ª flow_numeric_features
                if len(n['flow_numeric_features']) < max_flow_numeric_len:
                    n['flow_numeric_features'] += [0.0] * (max_flow_numeric_len - len(n['flow_numeric_features']))
                else:
                    n['flow_numeric_features'] = n['flow_numeric_features'][:max_flow_numeric_len]

            if enabled_views.get("flow_categorical_features", False):
                n['flow_categorical_features'] = flow_categorical_features
                max_flow_categorical_len = self.get_global_node_feature_dims('flow_categorical_features')
                # å¡«å……æˆ–è£å‰ª flow_categorical_features
                if len(n['flow_categorical_features']) < max_flow_categorical_len:
                    n['flow_categorical_features'] += [0] * (max_flow_categorical_len - len(n['flow_categorical_features']))
                else:
                    n['flow_categorical_features'] = n['flow_categorical_features'][:max_flow_categorical_len]

            if enabled_views.get("flow_textual_features", False):
                # âœ… ç›´æ¥ä¿å­˜ dictï¼Œä¸åšä»»ä½•é•¿åº¦å¤„ç†
                assert isinstance(flow_textual_features, dict)
                assert flow_textual_features["input_ids"].dim() == 2                
                n['flow_textual_features'] = flow_textual_features

            if (enabled_views.get("packet_len_seq", False)
                or enabled_views.get("packet_iat_seq", False)
            ):
                packet_len_seq, packet_iat_seq, packet_seq_mask = extract_flowmeter_packet_level_features(flow_record, mtu_normalize, max_packet_sequence_length)

            if enabled_views.get("packet_len_seq", False):
                n['packet_len_seq'] = packet_len_seq

            if enabled_views.get("packet_iat_seq", False):
                n['packet_iat_seq'] = packet_iat_seq

            n['packet_seq_mask'] = packet_seq_mask

            if enabled_views.get("domain_probs", False):
                domain_probs = extract_domain_name_probabilities(flow_record, num_classes)
                n['domain_probs'] = domain_probs
                max_domain_prob_len = self.get_global_node_feature_dims('domain_probs')
                # å¡«å……æˆ–è£å‰ª domain_probs
                if len(n['domain_probs']) < max_domain_prob_len:
                    n['domain_probs'] += [0.0] * (max_domain_prob_len - len(n['domain_probs']))
                else:
                    n['domain_probs'] = n['domain_probs'][:max_domain_prob_len]

            if (enabled_views.get("ssl_numeric_features", False)
                or enabled_views.get("ssl_categorical_features", False)
                or enabled_views.get("ssl_textual_features", False)
            ):
                ssl_numeric_features, ssl_categorical_features, ssl_textual_features = extract_ssl_features(flow_record, categorical_vocabulary_group, text_tokenizer, max_text_length)

            if enabled_views.get("ssl_numeric_features", False):
                n['ssl_numeric_features'] = ssl_numeric_features
                if numeric_feature_stats["ssl_numeric_features"]["count"] > 1:
                    vec = n['ssl_numeric_features']
                    # æ ‡å‡†åŒ–å‰æ£€æŸ¥NaN
                    if any(is_nan_or_inf(x) for x in vec):
                        # logger.warning(f"Flow {flow_uid} æ ‡å‡†åŒ–å‰å‘ç°NaNå€¼")
                        vec = [0.0 if is_nan_or_inf(x) else x for x in vec]                    
                    mean = numeric_feature_stats["ssl_numeric_features"]["mean"]
                    std  = numeric_feature_stats["ssl_numeric_features"]["std"]
                    assert len(mean) == len(vec), f"SSL numeric feature length inconsistent: mean={len(mean)}, vec={len(vec)}, uid={flow_uid}"
                    assert len(std) == len(vec), f"SSL numeric feature length inconsistent: mean={len(std)}, vec={len(vec)}, uid={flow_uid}"
                    n['ssl_numeric_features'] = [(x - m) / s for x, m, s in zip(vec, mean, std)]

                max_ssl_numeric_len = self.get_global_node_feature_dims('ssl_numeric_features')
                # å¡«å……æˆ–è£å‰ª ssl_numeric_features
                if len(n['ssl_numeric_features']) < max_ssl_numeric_len:
                    n['ssl_numeric_features'] += [0.0] * (max_ssl_numeric_len - len(n['ssl_numeric_features']))
                else:
                    n['ssl_numeric_features'] = n['ssl_numeric_features'][:max_ssl_numeric_len]

            if enabled_views.get("ssl_categorical_features", False):
                n['ssl_categorical_features'] = ssl_categorical_features
                max_ssl_categorical_len = self.get_global_node_feature_dims('ssl_categorical_features')
                if len(n['ssl_categorical_features']) < max_ssl_categorical_len:
                    n['ssl_categorical_features'] += [0] * (max_ssl_categorical_len - len(n['ssl_categorical_features']))
                else:
                    n['ssl_categorical_features'] = n['ssl_categorical_features'][:max_ssl_categorical_len]

            if enabled_views.get("ssl_textual_features", False):
                # âœ… ç›´æ¥ä¿å­˜ dictï¼Œä¸åšä»»ä½•é•¿åº¦å¤„ç†
                assert isinstance(ssl_textual_features, dict)
                assert ssl_textual_features["input_ids"].dim() == 2                
                n['ssl_textual_features'] = ssl_textual_features

            if (enabled_views.get("x509_numeric_features", False)
                or enabled_views.get("x509_categorical_features", False)
                or enabled_views.get("x509_textual_features", False)
            ):
                x509_numeric_features, x509_categorical_features, x509_textual_features = extract_x509_features(flow_record, categorical_vocabulary_group, text_tokenizer, max_text_length)

            if enabled_views.get("x509_numeric_features", False):
                n['x509_numeric_features'] = x509_numeric_features
                if numeric_feature_stats["x509_numeric_features"]["count"] > 1:
                    vec = n['x509_numeric_features']
                    # æ ‡å‡†åŒ–å‰æ£€æŸ¥NaN
                    if any(is_nan_or_inf(x) for x in vec):
                        # logger.warning(f"Flow {flow_uid} æ ‡å‡†åŒ–å‰å‘ç°NaNå€¼")
                        vec = [0.0 if is_nan_or_inf(x) else x for x in vec]                    
                    mean = numeric_feature_stats["x509_numeric_features"]["mean"]
                    std  = numeric_feature_stats["x509_numeric_features"]["std"]
                    assert len(mean) == len(vec), f"X509 numeric feature length inconsistent: mean={len(mean)}, vec={len(vec)}, uid={flow_uid}"
                    assert len(std) == len(vec), f"X509 numeric feature length inconsistent: mean={len(std)}, vec={len(vec)}, uid={flow_uid}"
                    n['x509_numeric_features'] = [(x - m) / s for x, m, s in zip(vec, mean, std)]

                max_x509_numeric_len = self.get_global_node_feature_dims('x509_numeric_features')
                # å¡«å……æˆ–è£å‰ª x509_features
                if len(n['x509_numeric_features']) < max_x509_numeric_len:
                    n['x509_numeric_features'] += [0.0] * (max_x509_numeric_len - len(n['x509_numeric_features']))
                else:
                    n['x509_numeric_features'] = n['x509_numeric_features'][:max_x509_numeric_len]
                
            if enabled_views.get("x509_categorical_features", False):
                n['x509_categorical_features'] = x509_categorical_features
                max_x509_categorical_len = self.get_global_node_feature_dims('x509_categorical_features')
                if len(n['x509_categorical_features']) < max_x509_categorical_len:
                    n['x509_categorical_features'] += [0] * (max_x509_categorical_len - len(n['x509_categorical_features']))
                else:
                    n['x509_categorical_features'] = n['x509_categorical_features'][:max_x509_categorical_len]

            if enabled_views.get("x509_textual_features", False):
                # âœ… ç›´æ¥ä¿å­˜ dictï¼Œä¸åšä»»ä½•é•¿åº¦å¤„ç†
                assert isinstance(x509_textual_features, dict)
                assert x509_textual_features["input_ids"].dim() == 2
                n['x509_textual_features'] = x509_textual_features

            if (
                enabled_views.get("dns_numeric_features", False)
                or enabled_views.get("dns_categorical_features", False)
                or enabled_views.get("dns_textual_features", False)
            ):
                dns_numeric_features, dns_categorical_features, dns_textual_features = extract_dns_features(flow_record, categorical_vocabulary_group, text_tokenizer, max_text_length)

            if enabled_views.get("dns_numeric_features", False):
                n['dns_numeric_features'] = dns_numeric_features
                if numeric_feature_stats["dns_numeric_features"]["count"] > 1:
                    vec = n['dns_numeric_features']
                    # æ ‡å‡†åŒ–å‰æ£€æŸ¥NaN
                    if any(is_nan_or_inf(x) for x in vec):
                        # logger.warning(f"Flow {flow_uid} æ ‡å‡†åŒ–å‰å‘ç°NaNå€¼")
                        vec = [0.0 if is_nan_or_inf(x) else x for x in vec]                    
                    mean = numeric_feature_stats["dns_numeric_features"]["mean"]
                    std  = numeric_feature_stats["dns_numeric_features"]["std"]
                    assert len(mean) == len(vec), f"DNS numeric feature length inconsistent: mean={len(mean)}, vec={len(vec)}, uid={flow_uid}"
                    assert len(std) == len(vec), f"DNS numeric feature length inconsistent: mean={len(std)}, vec={len(vec)}, uid={flow_uid}"
                    n['dns_numeric_features'] = [(x - m) / s for x, m, s in zip(vec, mean, std)]

                max_dns_numeric_len = self.get_global_node_feature_dims('dns_numeric_features')
                # å¡«å……æˆ–è£å‰ª dns_features
                if len(n['dns_numeric_features']) < max_dns_numeric_len:
                    n['dns_numeric_features'] += [0.0] * (max_dns_numeric_len - len(n['dns_numeric_features']))
                else:
                    n['dns_numeric_features'] = n['dns_numeric_features'][:max_dns_numeric_len]
                
            if enabled_views.get("dns_categorical_features", False):
                n['dns_categorical_features'] = dns_categorical_features
                max_dns_categorical_len = self.get_global_node_feature_dims('dns_categorical_features')
                if len(n['dns_categorical_features']) < max_dns_categorical_len:
                    n['dns_categorical_features'] += [0] * (max_dns_categorical_len - len(n['dns_categorical_features']))
                else:
                    n['dns_categorical_features'] = n['dns_categorical_features'][:max_dns_categorical_len]

            if enabled_views.get("dns_textual_features", False):
                # âœ… ç›´æ¥ä¿å­˜ dictï¼Œä¸åšä»»ä½•é•¿åº¦å¤„ç†
                assert isinstance(dns_textual_features, dict)
                assert dns_textual_features["input_ids"].dim() == 2
                n['dns_textual_features'] = dns_textual_features

        logger.debug("build_node_features() ends: node feature extraction is ok, and max feature lengths are determined")

        return nodes


def parse_list_field(field_value):
    """ç»ˆæä¿®æ­£ç‰ˆåˆ—è¡¨è§£æå‡½æ•°"""
    if field_value is None or pd.isna(field_value):
        return []
    
    if isinstance(field_value, (list, np.ndarray)):
        return list(field_value)
    
    if isinstance(field_value, str):
        value = field_value.strip()
        if not value or value.lower() in ['nan', 'none', 'null', '[]', '{}']:
            return []
        
        # å°è¯•è‡ªåŠ¨ä¿®å¤ä¸å®Œæ•´æ‹¬å·
        if value.count('[') != value.count(']'):
            # æƒ…å†µ1ï¼šç¼ºå°‘é—­åˆæ‹¬å·
            if value.startswith('[') and not value.endswith(']'):
                value += ']'  # å°è¯•è‡ªåŠ¨è¡¥å…¨
            # æƒ…å†µ2ï¼šå¤šä½™é—­åˆæ‹¬å·
            elif not value.startswith('[') and value.endswith(']'):
                value = '[' + value
            # å…¶ä»–æƒ…å†µä¿æŒåŸæ ·
        
        # è§£æä¼˜å…ˆçº§ï¼šJSON > Pythonå­—é¢é‡ > é€—å·åˆ†éš”
        for parser in [json.loads, ast.literal_eval]:
            try:
                parsed = parser(value)
                if isinstance(parsed, (list, tuple)):
                    return [int(x) if isinstance(x, float) and x.is_integer() else x 
                           for x in parsed]
                return [parsed]
            except (ValueError, SyntaxError, json.JSONDecodeError):
                continue
        
        # å¤„ç†çº¯é€—å·åˆ†éš”å­—ç¬¦ä¸²ï¼ˆæ— æ‹¬å·ï¼‰
        if ',' in value:
            parts = []
            for part in value.split(','):
                part = part.strip()
                if not part:
                    continue
                try:
                    num = float(part)
                    parts.append(int(num) if num.is_integer() else num)
                except ValueError:
                    parts.append(part)
            return parts
        
        return [value]
    
    return [field_value]


def normalize_packet_direction(d):
    # è¿”å› 1 è¡¨ç¤º å®¢æˆ·ç«¯->æœåŠ¡ç«¯ï¼Œè¿”å› -1 è¡¨ç¤º æœåŠ¡å™¨->å®¢æˆ·ç«¯
    if isinstance(d, (int, float, np.integer, np.floating)):
        try:
            v = int(d)
            return 1 if v == 1 else -1
        except:
            return 1
    if isinstance(d, str):
        ds = d.strip().lower()
        if ds in ('1', 'true', 't', 'c2s', 'client', '->', '>'):
            return 1
        if ds in ('0','false','f','s2c','server','<-','<'):
            return -1
        # å°è¯•è¯†åˆ« -1
        if ds.startswith('-'):
            return -1
        return 1
    if isinstance(d, bool):
        return 1 if d else -1
    return 1


def extract_flowmeter_packet_level_features(
    flow_record,
    mtu_normalize=1500,
    max_packet_sequence_length: int | None = None,
    pad_value: float = 0.0,
) -> Tuple[List[float], List[float], List[int]]:
    """
    æå– packet-level ç‰¹å¾ï¼š
      âœ” æ–¹å‘å¢å¼º + MTUå½’ä¸€åŒ– payload
      âœ” æ–¹å‘å¢å¼º + åˆ†æ®µ log ç¼©æ”¾ IAT
      â— åºåˆ—é•¿åº¦ä¸ä¸€è‡´ â†’ ç›´æ¥æŠ›å¼‚å¸¸ï¼Œé˜»æ–­å›¾æ„å»º
    """
    packet_dir_vector = parse_list_field(
        flow_record.get('flowmeter.packet_direction_vector', [])
    )
    packet_len_vector = parse_list_field(
        flow_record.get('flowmeter.packet_payload_size_vector', [])
    )
    raw_packet_iat_vector = parse_list_field(
        flow_record.get('flowmeter.packet_iat_vector', [])
    )
    packet_iat_vector = [0.0] + raw_packet_iat_vector  # âš  IAT å‰è¡¥ä¸€ä¸ª0

    # -------- ç›´æ¥å¼ºæ ¡éªŒé•¿åº¦ --------
    if not (
        len(packet_dir_vector) ==
        len(packet_len_vector) ==
        len(packet_iat_vector)
    ):
        raise ValueError(
            f"[SeqLenError] packet-levelç‰¹å¾é•¿åº¦ä¸ä¸€è‡´:"
            f" packet_dir_vector={len(packet_dir_vector)},"
            f" packet_len_vector={len(packet_len_vector)},"
            f" packet_iat_vector={len(packet_iat_vector)},"
            f" uid={flow_record.get('uid')}"
        )

    # åç»­é€»è¾‘éƒ½åœ¨ä¿è¯é•¿åº¦ä¸€è‡´å‰æä¸‹æ‰§è¡Œ
    dir_vec_len = len(packet_dir_vector)
    if dir_vec_len == 0:
        if max_packet_sequence_length is None:
            return [], [], []
        else:
            return (
                [pad_value] * max_packet_sequence_length,
                [pad_value] * max_packet_sequence_length,
                [0] * max_packet_sequence_length,
            )

    # -------- IAT åˆ†æ®µæ—¶é—´ç¼©æ”¾å‡½æ•° --------
    def _safe_log_scale_time(time_ms):
        if time_ms == 0: return 0.0
        sign = 1 if time_ms > 0 else -1
        abs_time = abs(time_ms)

        if abs_time < 10:
            return sign * abs_time / 1000.0
        elif abs_time < 1000:
            return sign * (0.01 + np.log1p(abs_time) / 10.0)
        elif abs_time < 60000:
            return sign * (0.1 + np.log1p(abs_time / 1000.0) / 5.0)
        elif abs_time < 3600000:
            return sign * (0.5 + np.log1p(abs_time / 60000.0) / 3.0)
        else:
            return sign * (1.0 + np.log1p(abs_time / 3600000.0) / 2.0)

    packet_len_seq = []
    packet_iat_seq = []

    for dir_vec, len_vec, iat_vec in zip(packet_dir_vector, packet_len_vector, packet_iat_vector):
        sign_vec = normalize_packet_direction(dir_vec)

        # Payload å½’ä¸€åŒ–
        norm_payload = float(len_vec) / float(mtu_normalize)
        norm_payload = max(-1.0, min(1.0, norm_payload))  # clip
        packet_len_seq.append(sign_vec * norm_payload)

        # IAT ç¼©æ”¾
        scaled_iat_seq = _safe_log_scale_time(float(iat_vec))
        packet_iat_seq.append(sign_vec * scaled_iat_seq)

    # ===== truncate + padï¼ˆå¦‚æœé…ç½®äº† max_packet_sequence_lengthï¼‰ =====
    if max_packet_sequence_length is not None:
        orig_len = len(packet_len_seq)

        # truncate
        packet_len_seq = packet_len_seq[:max_packet_sequence_length]
        packet_iat_seq = packet_iat_seq[:max_packet_sequence_length]

        # pad
        if orig_len < max_packet_sequence_length:
            pad_len = max_packet_sequence_length - orig_len
            packet_len_seq.extend([pad_value] * pad_len)
            packet_iat_seq.extend([pad_value] * pad_len)

    # âœ… æ„é€  maskï¼šçœŸå®ä½ç½®ä¸º 1ï¼Œpadding ä¸º 0
    if max_packet_sequence_length is None:
        # ä¸æˆªæ–­ã€ä¸ padding
        packet_seq_mask = [1] * len(packet_len_seq)
    else:
        valid_len = min(orig_len, max_packet_sequence_length)
        packet_seq_mask = [1] * valid_len
        if orig_len < max_packet_sequence_length:
            packet_seq_mask.extend([0] * (max_packet_sequence_length - orig_len))

    return packet_len_seq, packet_iat_seq, packet_seq_mask


def extract_domain_name_probabilities(flow_record, num_classes, num_domain_name_hierarchy_levels = 5):
    """ä»DNSå’ŒTLSåŸŸååµŒå…¥ç‰¹å¾ä¸­æå–å¤šå±‚çº§åµŒå…¥å‘é‡ï¼Œä¸¥æ ¼æ ¡éªŒç»´åº¦"""
    domain_probs = []    
    for level in range(num_domain_name_hierarchy_levels): # é»˜è®¤å±‚çº§æ•°é‡ï¼š0~4
        for proto in ['ssl', 'dns']:
            if proto == 'ssl':
                embed_col = f'{proto}.server_name{level}_freq'
            elif proto == 'dns':
                embed_col = f'{proto}.query{level}_freq'
            else:
                raise ValueError(f"extract_domain_name_probabilities(): unsupported protocol or domain name hierarchical level.")
            
            embed_value = flow_record.get(embed_col, None)

            # ğŸ”¹å¦‚æœåˆ—ä¸å­˜åœ¨æˆ–å€¼ä¸ºç©º â†’ å¡«å……å…¨é›¶
            if embed_value is None or pd.isna(embed_value) or embed_value == "":
                domain_probs.extend([0.0] * num_classes)
                continue

            try:
                # ğŸ”¹ç¡®ä¿è½¬æ¢ä¸º Python list æˆ– numpy array
                if isinstance(embed_value, str):
                    embed_vector = ast.literal_eval(embed_value)
                elif isinstance(embed_value, (list, np.ndarray)):
                    embed_vector = list(embed_value)
                else:
                    raise TypeError(f"Unsupported type for {embed_col}: {type(embed_value)}")

                # ğŸ”¹ç¡®ä¿æ˜¯å¯è¿­ä»£çš„æµ®ç‚¹å‘é‡
                embed_vector = [float(x) for x in embed_vector]

                # ä¸¥æ ¼ç»´åº¦æ ¡éªŒ
                if len(embed_vector) != num_classes:
                    raise ValueError(
                        f"extract_domain_name_probabilities(): [DimError] {embed_col} ç»´åº¦é”™è¯¯: "
                        f"expected={num_classes}, got={len(embed_vector)}, value={embed_vector}"
                    )

                domain_probs.extend(embed_vector)

            except Exception as e:
                # âŒä»»ä½•è§£æå¤±è´¥ â†’ æŠ›å‡ºæ˜ç¡®å¼‚å¸¸ï¼Œç”¨äºå®šä½æ•°æ®é—®é¢˜
                raise ValueError(
                    f"[ParseError] åŸŸååµŒå…¥è§£æå¤±è´¥: {embed_col}={embed_value}, error={e}"
                )

    return domain_probs


def to_str_safe(val):
    """æŠŠ val å®‰å…¨åœ°å˜ä¸º str å¹¶ stripï¼›å¯¹äº None / NaN è¿”å›ç©ºå­—ç¬¦ä¸²ã€‚"""
    if val is None:
        return ''
    try:
        # pandas çš„ NaN / None æ£€æµ‹
        if pd.isna(val):
            return ''
    except Exception:
        pass
    if isinstance(val, str):
        return val.strip()
    # å…¶ä»–ç±»å‹ï¼ˆfloat/int/list/..ï¼‰éƒ½è½¬æˆå­—ç¬¦ä¸²å¹¶ strip
    return str(val).strip()

def encode_text(text: str, text_tokenizer, max_text_length):
    """
    å°†ä»»æ„ textual å­—æ®µç¼–ç ä¸ºé•¿åº¦å›ºå®š max_text_length çš„ token åºåˆ—ï¼ˆLongTensorï¼‰
    """
    if not isinstance(text, str):
        text = ""  # éå­—ç¬¦ä¸²ç»Ÿä¸€å¤„ç†ä¸ºç©ºå­—ç¬¦ä¸²

    encoded = text_tokenizer(
        text,
        padding="max_length",
        truncation=True,
        max_length=max_text_length,
        return_tensors="pt",
    )

    # âœ… Sanity checkï¼ˆå¼ºçƒˆæ¨èï¼‰
    assert encoded["input_ids"].dim() == 2, \
        f"encode_text expects input_ids to be 2D [1, L], got shape {encoded['input_ids'].shape}"
    assert encoded["attention_mask"].dim() == 2, \
        f"encode_text expects attention_mask to be 2D [1, L], got shape {encoded['attention_mask'].shape}"

    return {
        "input_ids": encoded["input_ids"],
        "attention_mask": encoded["attention_mask"],
    }

def extract_conn_and_flowmeter_features(flow_record, categorical_vocab_group, text_tokenizer, max_text_length):
    """ä»flowmeterè®°å½•ä¸­æå–ç»Ÿè®¡ç‰¹å¾"""
    numeric = []
    categorical = []
    textual_fields = []

    # ---------- æ•°å€¼ç‰¹å¾ ----------
    for col in conn_numeric_columns:
        full = f"conn.{col}"
        value = flow_record.get(full, None)
        try:
            numeric.append(float(value) if value not in (None, "") else 0.0)
        except:
            logger.error(
                f"[DEBUG] BAD NUMERIC in conn_numeric_columns: "
                f"col={col}, value='{value}'"
            )            
            numeric.append(0.0)

    for col in flowmeter_numeric_columns:
        full = f"flowmeter.{col}"
        value = flow_record.get(full, None)
        try:
            numeric.append(float(value) if value not in (None, "") else 0.0)
        except:
            logger.error(
                f"[DEBUG] BAD NUMERIC in flowmeter_numeric_columns: "
                f"col={col}, value='{value}'"
            )            
            numeric.append(0.0)

    # ---------- ç±»åˆ«ç‰¹å¾ ----------
    for col in conn_categorical_columns:
        full = f"conn.{col}"
        value = flow_record.get(full, "")
        vocab = categorical_vocab_group.get(full)
        if vocab is None:
            categorical.append(0)
        else:
            categorical.append(vocab.get(value, 0))        

    for col in flowmeter_categorical_columns:
        full = f"flowmeter.{col}"
        value = flow_record.get(full, "")
        vocab = categorical_vocab_group.get(full)
        if vocab is None:
            categorical.append(0)
        else:
            categorical.append(vocab.get(value, 0))

    # ---------- æ–‡æœ¬ç‰¹å¾ ----------
    for col in conn_textual_columns:
        full = f"conn.{col}"
        value = flow_record.get(full, "")
        textual_fields.append(to_str_safe(value))

    for col in flowmeter_textual_columns:
        full = f"flowmeter.{col}"
        value = flow_record.get(full, "")
        textual_fields.append(to_str_safe(value))

    # åˆå¹¶æˆä¸€ä¸ªå­—ç¬¦ä¸²ï¼Œå¯é€‰ï¼Œä¹Ÿå¯åˆ†å¤šå­—æ®µç¼–ç 
    combined_text = " ".join([str(x) for x in textual_fields if isinstance(x, str)])
    encoded_text = encode_text(combined_text, text_tokenizer, max_text_length)

    return numeric, categorical, encoded_text

def extract_ssl_features(flow_record, categorical_vocab_group, text_tokenizer, max_text_length):
    """æå– SSL çš„ numeric / categorical / textual ç‰¹å¾ï¼ˆä¸¥æ ¼ä½¿ç”¨ zeek_columns å®šä¹‰ï¼‰"""
    numeric = []
    categorical = []
    textual_fields = []

    # ---------- æ•°å€¼ç‰¹å¾ ----------
    for col in ssl_numeric_columns:
        full = f"ssl.{col}"
        value = flow_record.get(full, None)
        try:
            numeric.append(float(value) if value not in (None, "") else 0.0)
        except:
            logger.error(
                f"[DEBUG] BAD NUMERIC in ssl_numeric_columns: "
                f"col={col}, value='{value}'"
            )            
            numeric.append(0.0)

    # ---------- ç±»åˆ«ç‰¹å¾ ----------
    for col in ssl_categorical_columns:
        full = f"ssl.{col}"
        value = flow_record.get(full, "")
        vocab = categorical_vocab_group.get(full)
        if vocab is None:
            categorical.append(0)
        else:
            categorical.append(vocab.get(value, 0))

    # ---------- æ–‡æœ¬ç‰¹å¾ ----------
    for col in ssl_textual_columns:
        full = f"ssl.{col}"
        value = flow_record.get(full, "")
        textual_fields.append(to_str_safe(value))

    # åˆå¹¶æˆä¸€ä¸ªå­—ç¬¦ä¸²ï¼Œå¯é€‰ï¼Œä¹Ÿå¯åˆ†å¤šå­—æ®µç¼–ç 
    combined_text = " ".join([str(x) for x in textual_fields if isinstance(x, str)])
    encoded_text = encode_text(combined_text, text_tokenizer, max_text_length)

    return numeric, categorical, encoded_text

def extract_x509_features(flow_record, categorical_vocab_group, text_tokenizer, max_text_length):
    numeric = []
    categorical = []
    textual_fields = []

    for idx in range(max_x509_cert_chain_len):
        prefix = f"x509.cert{idx}"

        # å¦‚æœè¯¥è¯ä¹¦ä¸å­˜åœ¨ â†’ å¡«é›¶å ä½ï¼ˆä¿æŒå¯¹é½ï¼‰
        exists = any(k.startswith(prefix) for k in flow_record.keys())

        # ---------- numeric ----------
        for col in x509_numeric_columns:
            full = f"{prefix}.{col}"
            value = flow_record.get(full, None)
            if not exists:
                numeric.append(0.0)
                continue
            try:
                numeric.append(float(value) if value not in (None, "") else 0.0)
            except:
                numeric.append(0.0)

        # ---------- categorical ----------
        for col in x509_categorical_columns:
            full = f"{prefix}.{col}"
            value = flow_record.get(full, "")
            vocab = categorical_vocab_group.get(full)
            if vocab is None:
                categorical.append(0)
            else:
                categorical.append(vocab.get(value, 0))

        # ---------- textual ----------
        for col in x509_textual_columns:
            full = f"{prefix}.{col}"
            value = flow_record.get(full, "")
            textual_fields.append(to_str_safe(value) if exists else "")

    # åˆå¹¶æˆä¸€ä¸ªå­—ç¬¦ä¸²ï¼Œå¯é€‰ï¼Œä¹Ÿå¯åˆ†å¤šå­—æ®µç¼–ç 
    combined_text = " ".join([str(x) for x in textual_fields if isinstance(x, str)])
    encoded_text = encode_text(combined_text, text_tokenizer, max_text_length)

    return numeric, categorical, encoded_text


def extract_dns_features(flow_record, categorical_vocab_group, text_tokenizer, max_text_length):
    numeric = []
    categorical = []
    textual_fields = []

    # ---------- numeric ----------
    for col in dns_numeric_columns:
        full = f"dns.{col}"
        value = flow_record.get(full, None)
        try:
            numeric.append(float(value) if value not in (None, "") else 0.0)
        except:
            logger.error(
                f"[DEBUG] BAD NUMERIC in dns_numeric_columns: "
                f"col={col}, value='{value}'"
            )            
            numeric.append(0.0)

    # ---------- categorical ----------
    for col in dns_categorical_columns:
        full = f"dns.{col}"
        value = flow_record.get(full, "")
        vocab = categorical_vocab_group.get(full)
        if vocab is None:
            categorical.append(0)
        else:
            categorical.append(vocab.get(value, 0))

    # ---------- textual ----------
    for col in dns_textual_columns:
        full = f"dns.{col}"
        value = flow_record.get(full, "")
        textual_fields.append(to_str_safe(value))

    # åˆå¹¶æˆä¸€ä¸ªå­—ç¬¦ä¸²ï¼Œå¯é€‰ï¼Œä¹Ÿå¯åˆ†å¤šå­—æ®µç¼–ç 
    combined_text = " ".join([str(x) for x in textual_fields if isinstance(x, str)])
    encoded_text = encode_text(combined_text, text_tokenizer, max_text_length)

    return numeric, categorical, encoded_text


def get_project_root(start_path: str = None):
    import os, subprocess

    if start_path is None:
        start_path = os.path.abspath(os.path.dirname(__file__))

    # â‘  å°è¯•é€šè¿‡ Git
    try:
        root = subprocess.check_output(
            ["git", "rev-parse", "--show-toplevel"],
            stderr=subprocess.DEVNULL
        ).strip().decode("utf-8")
        return root
    except Exception:
        pass

    # â‘¡ å°è¯•æŸ¥æ‰¾å…³é”®æ–‡ä»¶
    markers = ("pyproject.toml", "setup.py", "requirements.txt", ".git")
    cur = start_path
    while True:
        if any(os.path.exists(os.path.join(cur, m)) for m in markers):
            return cur
        parent = os.path.abspath(os.path.join(cur, os.pardir))
        if parent == cur:
            break
        cur = parent

    # â‘¢ fallbackï¼šä½¿ç”¨ VSCode å·¥ä½œè·¯å¾„
    return os.environ.get("PWD", os.getcwd())

def load_text_tokenizer(model_name="bert-base-uncased", max_text_length=64):
    """
    åŠ è½½ BERT tokenizerã€‚
    æ”¯æŒï¼š
      âœ” å…ˆå°è¯•ä»æœ¬åœ° models_hub æ±‚è§£
      âœ” æ‰¾ä¸åˆ°åˆ™è‡ªåŠ¨åœ¨çº¿åŠ è½½
    è¿”å›ï¼š
      tokenizerï¼ˆBertTokenizerï¼‰
      max_text_lenï¼ˆintï¼‰
    """
    current_file_dir = os.path.dirname(os.path.abspath(__file__))

    # 2. ç¡¬ç¼–ç å›é€€ä¸¤å±‚æ‰¾åˆ°é¡¹ç›®æ ¹ç›®å½•
    # ç»“æ„: [ROOT]/src/build_session_graph/flow_node_builder.py
    # å›é€€: ../../
    project_root = os.path.abspath(os.path.join(current_file_dir, "..", ".."))

    model_path = os.path.join(project_root, 'models_hub', model_name)

    try:
        logger.info(f"å°è¯•ä»æœ¬åœ°ç¼“å­˜åŠ è½½ BERT tokenizer: {model_path}")
        tokenizer = BertTokenizer.from_pretrained(model_path)
    except Exception as e:
        logger.warning(f"æœ¬åœ° tokenizer ä¸å­˜åœ¨: {e}")
        logger.warning("å°è¯•ä» HuggingFace åœ¨çº¿ä¸‹è½½...")
        tokenizer = BertTokenizer.from_pretrained(model_name)

    logger.info(f"BERT tokenizer åŠ è½½æˆåŠŸï¼Œmax_text_len={max_text_length}")
    return tokenizer, max_text_length

def read_large_csv_with_progress(filepath, description="è¯»å–æ•°æ®åˆ°pandas dataframe", verbose=True):
    """å¸¦è¿›åº¦æ¡çš„å¤§å‹CSVæ–‡ä»¶è¯»å–å‡½æ•°"""
    if verbose:
        logger.info(f"{description}ï¼Œä»è·¯å¾„ {filepath}...")
        file_size = os.path.getsize(filepath) / (1024 * 1024 * 1024)  # GB
        logger.info(f"æ–‡ä»¶å¤§å°: {file_size:.2f}GB")
    
    # å…ˆè¯»å–å‰å‡ è¡Œè·å–åˆ—ä¿¡æ¯
    sample_df = pd.read_csv(filepath, nrows=5)
    columns = sample_df.columns.tolist()
    
    # åˆ†å—è¯»å–
    chunks = []
    chunk_size = 100000  # æ¯æ¬¡è¯»å–10ä¸‡è¡Œ

    if verbose:
        logger.info(f"æ£€æµ‹åˆ° {len(columns)} åˆ—ï¼Œå¼€å§‹æ¯{chunk_size}è¡Œåˆ†å—è¯»å–...")
    
    # è·å–æ€»è¡Œæ•°ï¼ˆä¸è¯»å–å…¨éƒ¨å†…å®¹ï¼‰
    with open(filepath, 'r') as f:
        total_rows = sum(1 for _ in f) - 1  # å‡å»æ ‡é¢˜è¡Œ
    
    if verbose:
        # ä½¿ç”¨position=0ç¡®ä¿è¿›åº¦æ¡åœ¨åŒä¸€è¡Œæ›´æ–°
        pbar = tqdm.tqdm(total=total_rows, desc=description, position=0, leave=True)
    
    for chunk in pd.read_csv(filepath, chunksize=chunk_size, low_memory=False):
        chunks.append(chunk)
        if verbose:
            pbar.update(len(chunk))
    
    if verbose:
        pbar.close()
    
    # åˆå¹¶æ‰€æœ‰å—
    df = pd.concat(chunks, ignore_index=True)
    
    if verbose:
        logger.info(f"{description}å®Œæˆ! æ•°æ®å½¢çŠ¶: {df.shape}")
    
    return df

def main():
    """æµ‹è¯•å‡½æ•°ï¼šéªŒè¯flowå’Œsessionæ•°æ®è¯»å–åŠç‰¹å¾æå–ï¼Œå¹¶è®¡ç®—å…¨å±€ç‰¹å¾ç»´åº¦"""
    # æµ‹è¯•æ•°æ®è·¯å¾„
    flow_csv_path = "processed_data/CIC-AndMal2017/SMSMalware/jifake-flow.csv"
    session_csv_path = "processed_data/CIC-AndMal2017/SMSMalware/jifake-session.csv"
    
    try:
        # 1. è¯»å–flowæ•°æ®å¹¶æ„å»ºflow_dictï¼ˆuidåˆ°flowè®°å½•çš„æ˜ å°„ï¼‰
        logger.info(f"è¯»å–flowæ•°æ®: {flow_csv_path}")
        flow_df = pd.read_csv(
            flow_csv_path,
            dtype=dtype_dict_in_flow_csv,
            parse_dates=False  # é¿å…è‡ªåŠ¨è§£ææ—¥æœŸå¯¼è‡´æ ¼å¼é—®é¢˜
        )
        flow_dict = {row['uid']: row.to_dict() for _, row in flow_df.iterrows()}
        logger.info(f"æˆåŠŸåŠ è½½ {len(flow_dict)} æ¡flowè®°å½•")

        # 2. è¯»å–sessionæ•°æ®
        logger.info(f"è¯»å–sessionæ•°æ®: {session_csv_path}")
        session_df = pd.read_csv(session_csv_path)
        logger.info(f"æˆåŠŸåŠ è½½ {len(session_df)} æ¡sessionè®°å½•")

        # 3. åˆå§‹åŒ–å…¨å±€ç‰¹å¾ç»´åº¦ç»Ÿè®¡
        global_node_feature_dims = {
            "flow_numeric_features": 0,
            "flow_categorical_features": 0,
            "flow_textual_features": 0,
            "packet_len_seq": 0,
            "packet_iat_seq": 0,
            "domain_probs": 0,
            "ssl_numeric_features": 0,
            "ssl_categorical_features": 0,
            "ssl_textual_features": 0,
            "x509_numeric_features": 0,
            "x509_categorical_features": 0,
            "x509_textual_features": 0,
            "dns_numeric_features": 0,
            "dns_categorical_features": 0,
            "dns_textual_features": 0,
        }

        # è·å–é»˜è®¤çš„ç±»åˆ«æ•°é‡ï¼ˆç”¨äºæµ‹è¯•ï¼‰
        num_classes = len(set(read_session_label_id_map().values()))
        logger.info(f"ç±»åˆ«æ•°é‡: {num_classes}")

        categorical_vocabulary_group = FlowNodeBuilder.scan_flow_dict_for_categorical_topk_vocab_group(flow_dict)

        text_encoder_name, max_text_length = read_text_encoder_config()
        text_tokenizer, max_text_length = load_text_tokenizer(
            model_name=text_encoder_name,
            max_text_length=max_text_length
        )
            
        # 4. éå†æ‰€æœ‰ä¼šè¯å’Œæµï¼Œè®¡ç®—å…¨å±€ç‰¹å¾ç»´åº¦
        logger.info("è®¡ç®—å…¨å±€ç‰¹å¾ç»´åº¦...")
        for _, session_row in tqdm.tqdm(session_df.iterrows(), total=len(session_df), desc="å¤„ç†ä¼šè¯"):
            # è§£æsessionä¸­çš„flowåˆ—è¡¨
            if 'flow_uid_list' not in session_row:
                continue

            flow_uid_list = ast.literal_eval(session_row['flow_uid_list'])

            # éå†æ¯ä¸ªflow
            for flow_uid in flow_uid_list:
                if flow_uid not in flow_dict:
                    continue

                flow_record = flow_dict[flow_uid]

                try:
                    # æå–åŠ å¯†æµé‡åŸºæœ¬ç‰¹å¾
                    flow_numeric_features, flow_categorical_features, flow_textual_features = extract_conn_and_flowmeter_features(flow_record, categorical_vocabulary_group, text_tokenizer, max_text_length)
                    mtu_normalize = 1500
                    max_packet_sequence_length = 512
                    packet_len_seq, packet_iat_seq, packet_seq_mask = extract_flowmeter_packet_level_features(flow_record, mtu_normalize, max_packet_sequence_length)
                    logger.debug("!!! flow_uid = {flow_uid}, with flow_numeric_features = " + str(flow_numeric_features)
                                 + ", flow_categorical_features = " + str(flow_categorical_features)
                                 + ", flow_textual_features = " + str(flow_textual_features)
                                 + ", packet_len_seq = " + str(packet_len_seq)
                                 + ", packet_iat_seq = " + str(packet_iat_seq)
                                 + ", packet_seq_mask = " + str(packet_seq_mask)
                            )

                    # æå–æ˜æ–‡éƒ¨åˆ†çš„è½½è·ç‰¹å¾
                    domain_probs = extract_domain_name_probabilities(flow_record, num_classes)
                    ssl_numeric_features, ssl_categorical_features, ssl_textual_features = extract_ssl_features(flow_record, categorical_vocabulary_group, text_tokenizer, max_text_length)
                    x509_numeric_features, x509_categorical_features, x509_textual_features = extract_x509_features(flow_record, categorical_vocabulary_group, text_tokenizer, max_text_length)
                    dns_numeric_features, dns_categorical_features, dns_textual_features = extract_dns_features(flow_record, categorical_vocabulary_group, text_tokenizer, max_text_length)
                    logger.debug("!!! flow_uid = {flow_uid}, with domain_probs = " + str(domain_probs) 
                                 + ", ssl_numeric_features = " + str(ssl_numeric_features) 
                                 + ", ssl_categorical_features = " + str(ssl_categorical_features)
                                 + ", ssl_textual_features = " + str(ssl_textual_features)
                                 + ", x509_numeric_features = " + str(x509_numeric_features) 
                                 + ", x509_categorical_features = " + str(x509_categorical_features)
                                 + ", x509_textual_features = " + str(x509_textual_features)
                                 + ", dns_numeric_features = " + str(dns_numeric_features) 
                                 + ", dns_categorical_features = " + str(dns_categorical_features)
                                 + ", dns_textual_features = " + str(dns_textual_features)
                            )

                    # æ›´æ–°å…¨å±€ç»´åº¦ç»Ÿè®¡
                    global_node_feature_dims['flow_numeric_features'] = max(
                        global_node_feature_dims['flow_numeric_features'], 
                        len(flow_numeric_features) if len(flow_numeric_features) > 0 else 1
                    )
                    global_node_feature_dims['flow_categorical_features'] = max(
                        global_node_feature_dims['flow_categorical_features'], 
                        len(flow_categorical_features) if len(flow_categorical_features) > 0 else 1
                    )
                    global_node_feature_dims['flow_textual_features'] = max(
                        global_node_feature_dims['flow_textual_features'], 
                        len(flow_textual_features) if len(flow_textual_features) > 0 else 1
                    )                    
                    global_node_feature_dims['packet_len_seq'] = max_packet_sequence_length
                    global_node_feature_dims['packet_iat_seq'] = max_packet_sequence_length
                    global_node_feature_dims['domain_probs'] = max(
                        global_node_feature_dims.get('domain_probs', 0), 
                        len(domain_probs) if len(domain_probs) > 0 else 1
                    )

                    # è®¡ç®—SSLçš„å„ç±»ç‰¹å¾å‘é‡çš„ç»´åº¦
                    global_node_feature_dims['ssl_numeric_features'] = max(
                        global_node_feature_dims['ssl_numeric_features'], 
                        len(ssl_numeric_features) if len(ssl_numeric_features) > 0 else 1)
                    global_node_feature_dims['ssl_categorical_features'] = max(
                        global_node_feature_dims['ssl_categorical_features'], 
                        len(ssl_categorical_features) if len(ssl_categorical_features) > 0 else 1)
                    global_node_feature_dims['ssl_textual_features'] = max(
                        global_node_feature_dims['ssl_textual_features'], 
                        len(ssl_textual_features) if len(ssl_textual_features) > 0 else 1)

                    # è®¡ç®—X509çš„å„ç±»ç‰¹å¾å‘é‡çš„ç»´åº¦
                    global_node_feature_dims['x509_numeric_features'] = max(
                        global_node_feature_dims['x509_numeric_features'], 
                        len(x509_numeric_features) if len(x509_numeric_features) > 0 else 1)
                    global_node_feature_dims['x509_categorical_features'] = max(
                        global_node_feature_dims['x509_categorical_features'], 
                        len(x509_categorical_features) if len(x509_categorical_features) > 0 else 1)
                    global_node_feature_dims['x509_textual_features'] = max(
                        global_node_feature_dims['x509_textual_features'], 
                        len(x509_textual_features) if len(x509_textual_features) > 0 else 1)

                    # è®¡ç®—DNSçš„å„ç±»ç‰¹å¾å‘é‡çš„ç»´åº¦                
                    global_node_feature_dims['dns_numeric_features'] = max(
                        global_node_feature_dims['dns_numeric_features'], 
                        len(dns_numeric_features) if len(dns_numeric_features) > 0 else 1)
                    global_node_feature_dims['dns_categorical_features'] = max(
                        global_node_feature_dims['dns_categorical_features'], 
                        len(dns_categorical_features) if len(dns_categorical_features) > 0 else 1)
                    global_node_feature_dims['dns_textual_features'] = max(
                        global_node_feature_dims['dns_textual_features'], 
                        len(dns_textual_features) if len(dns_textual_features) > 0 else 1)

                except Exception as e:
                    logger.error(f"Flow {flow_uid} ç‰¹å¾æå–é”™è¯¯: {str(e)}")
                    continue

        # 5. ç¡®ä¿æœ€å°ç»´åº¦ä¸º1
        for key in global_node_feature_dims:
            global_node_feature_dims[key] = max(1, global_node_feature_dims[key])

        # 6. è¾“å‡ºå…¨å±€ç‰¹å¾ç»´åº¦
        logger.info("å…¨å±€ç‰¹å¾ç»´åº¦ç»Ÿè®¡:")
        for key, dim in global_node_feature_dims.items():
            logger.info(f"  {key}: {dim}")

        logger.info("æµ‹è¯•å®Œæˆ")

    except FileNotFoundError as e:
        logger.error(f"é”™è¯¯: æœªæ‰¾åˆ°æµ‹è¯•æ–‡ä»¶ - {str(e)}")
    except KeyError as e:
        logger.error(f"é”™è¯¯: æ•°æ®ä¸­ç¼ºå°‘å¿…è¦å­—æ®µ - {str(e)}")
    except Exception as e:
        logger.error(f"æµ‹è¯•è¿‡ç¨‹å‡ºé”™: {str(e)}")


if __name__ == "__main__":
    main()
    